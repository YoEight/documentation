{
  "server/ports-and-networking/index.html": {
    "href": "server/ports-and-networking/index.html",
    "title": "Ports and networking | Event Store",
    "keywords": "Ports and networking Single Node If you run Event Store as a single node, it will only use two ports. The first port is the external HTTP port. Event Store uses this port for both the client HTTP APIs and for the management HTTP interface. The default external HTTP port is 2113. The second port used is the TCP interface for clients connecting over the client API, and the default for the port is 1113. Event Store in Windows will try to add access via http.sys automatically for the 2113 port. You should ensure that these ports are open and allowed via a firewall if you are using a firewall on the machine. Cluster Node When running in cluster mode the networking for Event Store is more complex. Cluster mode requires 4 ports to run. The ports are for internal HTTP, internal TCP, external HTTP, and external TCP. The internal and external interfaces allow for separation of traffic. The internal network is where all cluster communications runs while the external interfaces is where all client communications runs. This allows for interesting setups such as putting internal communications on a different network than external client communications. You may wish to do this for many reasons including putting communications on separate NICs, locking down internal communications from a security perspective, or for security purposes. A good example of this might be when allowing clients over HTTP to talk directly to Event Store, you can move internal communications to a separate network to ensure the management interface and internal operations are not accessible from the outside. The external TCP and HTTP are similar to the HTTP and TCP ports of a single node deploy. Event Store runs Client requests over the HTTP API through the external HTTP port. You can run without the management API on the external interface (internal only). The external and the internal interfaces support the gossip protocol. You can control whether the admin interface is available on the external HTTP interface using the admin-on-ext option . You can control whether gossip is enable on external with the gossip-on-ext option (though you normally want it). You configure the internal TCP and HTTP ports in the same way as the external. All internal communications for the cluster happen over these interfaces. Elections and internal gossip happen over HTTP. Replication and forwarding of client requests happens over the TCP channel. When setting up a cluster the nodes must be able to reach each other over both the internal HTTP channel and the internal TCP channel. You should ensure that these ports are open on firewalls on the machines and between the machines. Heartbeat Timeouts Event Store uses heartbeats over all TCP connections to attempt to discover dead clients and nodes. Setting heartbeat timeouts is hard, set them too short and you will get false positives, set them too long and discovery of dead clients and nodes becomes slower. Each heartbeat has two points of configuration. The first is the 'interval', this represents how often the system should consider a heartbeat. There will not be a heartbeat sent for every interval. Heartbeats work by saying \"if I have not received something from this node within the last interval send a heartbeat request\". As such on a busy system you will not send any heartbeats. The second point of configuration is the 'timeout', when sending a heartbeat, how long do you wait for the client or node to respond to the heartbeat request. Varying environments want drastically different values for these settings. While low numbers work well on a LAN they tend to not work well in the cloud. The defaults are likely fine on a LAN, in the cloud consider a setting of interval 5000ms and timeout 1000ms which should be fine for most installations. Tip If in question err on the side of higher numbers, it will add a small period of time to discover a dead client or node and is better than the alternative, which is false positives. Advertise As There are times when due to NAT or other reasons a node may not be bound to the address it is reachable from other nodes as. As an example the machine has an IP address of 192.168.1.13 but the node is visible to other nodes as 10.114.12.112. The option advertise-as allows you to tell the node that even though it is bound to a given address it should not gossip that address as its address. Instead it will use the address that you tell it to use. In the example above you would configure --ext-ip 192.168.1.13 --advertise-as 10.114.12.112 Or use the equivalent configuration via environment variables or a configuration file."
  },
  "http-api/optional-http-headers/eventtype/index.html": {
    "href": "http-api/optional-http-headers/eventtype/index.html",
    "title": "Optional HTTP Headers: EventType | Event Store",
    "keywords": "Optional HTTP Headers: EventType Note This event is only available in version 3.0.0 or higher of Event Store. When writing to a stream and not using the application/vnd.eventstore.events+json/+xml media type it is necessary that you specify an event type with the event that you are posting. This is not required with the custom media type as it is also specified within the format itself. You can use the ES-EventType header as follows. Request Response curl -i -d @event.json \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -H \"ES-EventType: SomeEvent\" -H \"ES-EventId: C322E299-CB73-4B47-97C5-5054F920746E\" HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream/1 Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:26:48 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 If you now view the event in the UI or through cURL it will have the EventType of SomeEvent associated with it. Request Reponse curl http://127.0.0.1:2113/streams/newstream/1 HTTP/1.1 200 OK Access-Control-Allow-Methods: GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=31536000, public Vary: Accept Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:29:27 GMT Content-Length: 585 Keep-Alive: timeout=15,max=100 { \"title\": \"1@newstream\", \"id\": \"http://127.0.0.1:2113/streams/newstream/1\", \"updated\": \"2014-04-21T18:26:48.731192Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"SomeEvent\", \"content\": { \"eventStreamId\": \"newstream\", \"eventNumber\": 1, \"eventType\": \"SomeEvent\", \"data\": { \"something\": \"has data\" }, \"metadata\": \"\" }, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1\", \"relation\": \"alternate\" } ] }"
  },
  "http-api/description-document/index.html": {
    "href": "http-api/description-document/index.html",
    "title": "Description Document | Event Store",
    "keywords": "Description Document What is it? With the addition of Competing Consumers which is essentially another way of reading streams, the need arose to expose these different methods to consumers. The introduction of the description document has some benefits Clients can rely on the keys (streams, streamSubscription) in the description document to remain unchanged across versions of Event Store and can be relied on as a lookup for the particular method of reading a stream. Allows the restructuring of URIs underneath without breaking clients. e.g. /streams/newstream -> /streams/newstream/atom How do I get the description document? There are 3 ways in which the description document will be returned. Attempting to read a stream with an unsupported media type. Attempting to read a stream with no accept header. Requesting the description document explicitly. The client is able to request the description document by passing application/vnd.eventstore.streamdesc+json in the accept header e.g. Request Response curl -i http://localhost:2113/streams/newstream -H \"accept:application/vnd.eventstore.streamdesc+json\" HTTP/1.1 200 Description Document Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-Forwarded-Host, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Content-Type: application/vnd.eventstore.streamdesc+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Wed, 02 Dec 2015 07:05:37 GMT Content-Length: 590 Keep-Alive: timeout=15,max=100 { \"title\": \"description document for newstream\", \"description\": \"The description document will be presented when the following is true for the Accept Header. (No Accept Header, Unsupported Response Type, Description Document Requested)\", \"_links\": { \"self\": { \"href\": \"/streams/newstream\", \"supportedContentTypes\": [ \"application/vnd.eventstore.streamdesc+json\" ] }, \"stream\": { \"href\": \"/streams/newstream\", \"supportedContentTypes\": [ \"application/atom+xml\", \"application/vnd.eventstore.atom+json\" ] }, \"streamSubscription\": [ { \"href\": \"/subscriptions/newstream/competing_consumers_group1\", \"supportedContentTypes\": [ \"application/vnd.eventstore.competingatom+xml\", \"application/vnd.eventstore.competingatom+json\" ] }, { \"href\": \"/subscriptions/newstream/competing_consumers_group2\", \"supportedContentTypes\": [ \"application/vnd.eventstore.competingatom+xml\", \"application/vnd.eventstore.competingatom+json\" ] } ] } } In the example above, the client has requested the description document for the stream called newstream which has a set of links which describes the supported methods and content types for reading. The document also includes additional methods if they are available such as the streamSubscription (Which is HTTP Competing Consumers). What is displayed in the example above is that there are 2 subscriptions for the newstream , namely competing_consumers_group1 and competing_consumers_group2 . If there are no subscriptions to the newstream , the streamSubscription key will be absent."
  },
  "dotnet-api/writing-to-a-stream/index.html": {
    "href": "dotnet-api/writing-to-a-stream/index.html",
    "title": "Writing to a Stream | Event Store",
    "keywords": "Writing to a Stream You can use the client API to write one or more events to a stream atomically. You do this by appending the events to the stream in one operation, or by starting a transaction on the stream, writing events in one or more operations in that transaction, and then committing the transaction. You can make an optimistic concurrency check during the write by specifying the version at which you expect the stream to be. Identical write operations are idempotent if the optimistic concurrency check is not disabled. You can find more information on optimistic concurrency and idempotence here . Methods Appending to a stream in a single write Task<WriteResult> AppendToStreamAsync(string stream, long expectedVersion, IEnumerable<EventData> events) Task<WriteResult> AppendToStreamAsync(string stream, long expectedVersion, params EventData[] events) Using a transaction to append to a stream across multiple writes On EventStoreConnection Task<EventStoreTransaction> StartTransactionAsync(string stream, long expectedVersion) EventStoreTransaction ContinueTransaction(long transactionId) On EventStoreTransaction Task WriteAsync(IEnumerable<EventData> events) Task WriteAsync(params EventData[] events) Task CommitAsync() void Rollback() EventData The writing methods all use a type named EventData to represent an event to be stored. Instances of EventData are immutable. Event Store does not have any built-in serialisation, so the body and metadata for each event are represented in EventData as a byte[] . The members on EventData are: Member Description Guid EventId A unique identifier representing this event. Event Store uses this for idempotency if you write the same event twice you should use the same identifier both times. string Type The name of the event type. You can use this for pattern matching in projections, so should be a \"friendly\" name rather than a CLR type name, for example. bool IsJson If the data and metadata fields are serialized as JSON, you should set this to true . Setting this to true will cause the projections framework to attempt to deserialize the data and metadata later. byte[] Data The serialized data representing the event to be stored. byte[] Metadata The serialized data representing metadata about the event to be stored. Append to a stream in a single write The AppendToStreamAsync method writes events atomically to the end of a stream, working in an asynchronous manner. The parameters are: Parameter Description string stream The name of the stream to which to append. long expectedVersion The version at which you expect the stream to be in order that an optimistic concurrency check can be performed. This should either be a positive integer, or one of the constants ExpectedVersion.NoStream , ExpectedVersion.EmptyStream , or to disable the check, ExpectedVersion.Any . See here for a broader discussion of this. IEnumerable<EventData> events The events to append. There is also an overload of each method which takes the events as a params array."
  },
  "dotnet-api/index.html": {
    "href": "dotnet-api/index.html",
    "title": "Overview | Event Store",
    "keywords": "Overview The .NET Client API communicates with Event Store over TCP, using length-prefixed serialised protocol buffers. The API allows for reading and writing operations, as well as for subscriptions to individual event streams or all events written. EventStoreConnection The EventStoreConnection class maintains a full-duplex connection between the client and the Event Store server. EventStoreConnection is thread-safe, and we recommend that you create one instance per application. All operations are handled fully asynchronously, returning either a Task or a Task<T> . If you need to execute synchronously, call .Wait() on the asynchronous version. Note To get maximum performance from the connection, we recommend you use it asynchronously. Quick Start The code below shows how to connect to an Event Store server, write to a stream, and read back the events. For more detailed information, read the full pages for Connecting to a Server , Reading Events and Writing to a Stream var connection = EventStoreConnection.Create(new IPEndPoint(IPAddress.Loopback, 1113)); // Don't forget to tell the connection to connect! connection.ConnectAsync().Wait(); var myEvent = new EventData(Guid.NewGuid(), \"testEvent\", false, Encoding.UTF8.GetBytes(\"some data\"), Encoding.UTF8.GetBytes(\"some metadata\")); connection.AppendToStreamAsync(\"test-stream\", ExpectedVersion.Any, myEvent).Wait(); var streamEvents = connection.ReadStreamEventsForwardAsync(\"test-stream\", 0, 1, false).Result; var returnedEvent = streamEvents.Events[0].Event; Console.WriteLine(\"Read event with data: {0}, metadata: {1}\", Encoding.UTF8.GetString(returnedEvent.Data), Encoding.UTF8.GetString(returnedEvent.Metadata));"
  },
  "server/caching/index.html": {
    "href": "server/caching/index.html",
    "title": "Caching | Event Store",
    "keywords": "Caching As there are large differences between running Event Store in development and production, it's important to understand how caching works. Most of the URIs that Event Store emits are immutable (including the UI, Atom Feeds, etc). You can see an example of this in an Atom feed. An Atom feed has a URI that represents an event, e.g. /streams/foo/0 , representing 'event 0'. The data for event 0 will never change. If this stream is open to public reads then the URI will be set to be 'cachable' for long periods of time. You can see a similar example in reading a feed, if a stream has 50 events in it the feed page 20/forward/10 will never change, it will always be events 20-30. Internally Event Store controls serving the right URIs by using rel links with feeds (for example prev / next ). This caching behaviour is great for performance in a production environment and is highly recommended. In a developer environment it can be confusing. For example, what would happen if you started a database, wrote /streams/foo/0 and performed a GET request. This GET request is cachable and now in your cache. Since this is a development environment, you shutdown and delete the database. You then restart it and write a different event to /streams/foo/0 . You open your browser and inspect the /streams/foo/0 stream, and you will see the event written before you deleted the database. To avoid this it's best to run Event Store with the --disable-http-caching command line option. This will disable all caching and solve the issue."
  },
  "http-api/index.html": {
    "href": "http-api/index.html",
    "title": "Overview | Event Store",
    "keywords": "Overview Event Store provides a native interface of AtomPub over HTTP. AtomPub is a RESTful protocol that can reuse many existing components, for example reverse proxies and a client’s native HTTP caching. Since events stored in Event Store are entirely immutable, cache expiration can be infinite. Event Store leverages content type negotiation and you can access appropriately serialised events can as JSON or XML according to the request headers. Compatibility with AtomPub Event Store is fully compatible with the 1.0 version of the Atom Protocol . Event Store adds extensions to the protocol, such as headers for control and custom rel links. Existing Implementations Many environments have already implemented the AtomPub protocol, that simplifies the process. Library Description NET (BCL) System.ServiceModel.SyndicationServices JVM http://java-source.net/open-source/rss-rdf-tools PHP http://simplepie.org/ or https://github.com/fguillot/picoFeed Ruby http://simple-rss.rubyforge.org Clojure https://github.com/scsibug/feedparser-clj Go https://github.com/jteeuwen/go-pkg-rss Python http://code.google.com/p/feedparser/ node.js https://github.com/danmactough/node-feedparser Objective-C https://geekli.st/darvin/repos/MWFeedParser Note The above list are not officially supported by Event Store, if you know of any others then please let us know . Content Types The preferred way of determining which content type responses Event Store serves is to set the Accept header on the request. As some clients do not deal well with HTTP headers when caching, appending a format parameter to the URL is also supported, e.g. ?format=xml . The accepted content types for POST requests are: application/xml application/vnd.eventstore.events+xml application/json application/vnd.eventstore.events+json text/xml The accepted content types for GET requests are: application/xml application/atom+xml application/json application/vnd.eventstore.atom+json text/xml text/html There will be additions in the future for protobufs and bson."
  },
  "http-api/errors/index.html": {
    "href": "http-api/errors/index.html",
    "title": "Errors | Event Store",
    "keywords": "Errors <!-- TODO: Is this really all errors? --> <!-- TODO: Replace by swagger? --> There are many error conditions that can return from the writing or reading of a stream. You can identify these by their status codes and should be easy to diagnose. Stream Never Created Get curl -i -H \"Accept:application/json\" \"http://127.0.0.1:2113/streams/anewstream\" HTTP/1.1 404 Not Found Access-Control-Allow-Methods: DELETE, GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Content-Type: ; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Tue, 02 Apr 2013 14:41:29 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 Write to Stream with Invalid Content for Content Type cat ~/simpleevent.txt [ { \"eventId\": \"fbf4b1a1-b4a3-4dfe-a01f-ec52c34e16e4\", \"eventType\": \"event-type\", \"data\": { \"a\": \"1\" } }, { \"eventId\": \"0f9fad5b-d9cb-469f-a165-70867728951e\", \"eventType\": \"event-type\", \"data\": { \"a\": \"1\" } } ] curl -i -d @simpleevent.txt \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:text/xml\" HTTP/1.1 400 Write request body invalid Access-Control-Allow-Methods: DELETE, GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Content-Type: Server: Mono-HTTPAPI/1.0 Date: Tue, 02 Apr 2013 14:48:27 GMT Content-Length: 0 Connection: close Security Denied curl -i \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -u admin:foo HTTP/1.1 401 Unauthorized Access-Control-Allow-Methods: Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * WWW-Authenticate: Basic realm=\"ES\" Content-Type: Server: Mono-HTTPAPI/1.0 Date: Fri, 28 Jun 2013 12:45:30 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100"
  },
  "event-sourcing-basics/index.html": {
    "href": "event-sourcing-basics/index.html",
    "title": "Event Sourcing Basics | Event Store",
    "keywords": "Event Sourcing Basics What is an Event Store? Event Store is a database that supports the concept of Event Sourcing, which is an old idea that has recently become popular again Note If you are familiar with functional programming you may wish to jump to the Event Store viewed as a functional database section . Production systems often rely on storing their current state to process transactions, but this has not always been the case. Before the general acceptance of relational database management systems (RDBMS) as the center of system architectures many did not store the current state. This was especially true in high performance, mission critical, and highly secure systems. If you look at the inner workings of an RDBMS you will find that most do not themselves work by managing current state. What is an Event? An event is something that happened in the past, and so you should represent events as verbs in the past tense such as CustomerRelocated , CargoShipped , or InventoryLossageRecorded . If you are taking a domain driven design approach, it's imperative that events are verbs in the past tense, as they are part of the Ubiquitous Language. Consider the differences with ubiquitous language when relocating a customer. An event makes the concept explicit where previously the changes would occur within an aggregate or between multiple aggregates and were left as an implicit concept that you needed to explore and define. In most applications, a developer discovers that a side effect occurred with a tool such as Hibernate or Entity Framework. If there is a change to the side effects of a use case, it is an implicit concept. The introduction of the event makes the concept explicit and part of the Ubiquitous Language. Relocating a customer does not change something, relocating a customer produces a CustomerRelocatedEvent event which is explicitly defined within the language. In code an event is a data holding structure such as the following: public class InventoryItemDeactivated { public readonly Guid InventoryItemId; public readonly string Reason; public InventoryItemDeactivated(inventoryItemId, reason) { InventoryItemId = inventoryItemId; Reason = reason; } } Other Definitions of Domain Events There is a concept related to a Domain Event defined in Streamlined Object Modeling (SOM). The term “Domain Event” is often used in SOM when discussing “The Event Principle”. Model the event of people interacting at a place with a thing with a transaction object. Model a point-in-time interaction as a transaction with a single timestamp; model a time-interval interaction as a transaction with multiple timestamps. Jill Nicola, 2002ll, p. 23 Although people use the term Domain Event to describe this concept the term does not have the same definition as a Domain Event in the context of this document. SOM uses another term for the concept that better describes what the object is, a transaction. The concept of a transaction object is an important one in a domain and deserves a name. An example of such a transaction might be a player swinging a bat. This is an action that occurred at a given point and you should model as such in the domain, but this is not the same as a Domain Event. This also differs from Martin Fowler’s example of what a Domain Event is: Example: I go to Babur’s for a meal on Tuesday, and pay by credit card. This might be modeled as an event, whose type is “Make Purchase”, whose subject is my credit card, and whose occurred date is Tuesday. If Babur’s uses an old manual system and doesn’t transmit the transaction until Friday, then the noticed date would be Friday. Fowler Furthermore By funneling inputs of a system into streams of Domain Events you can keep a record of all the inputs to a system. This helps you to organize your processing logic, and also allows you to keep an audit log of the system. Fowler What Martin is actually describing here is a \"command\". The language “Make Purchase” is wrong if we are to consider this as an event. A purchase was made, therefore it makes more sense to introduce a PurchaseMade event. Martin did make a purchase at the location, they did charge his credit card, and he ate and enjoyed his food. All these events are in the past tense. They have already happened and cannot be undone. An example such as the sales example given tends to lead towards a secondary problem when built within a system. The problem is that the domain may be responsible for filling in parts of the event. Consider a system where the domain processes the sale itself. How much is the sales tax? Often the domain would calculate this. This leads to a dual definition of the event. There is the event as sent from the client without the sales tax, and then the domain would receive it and add in the sales tax. It causes the event to have multiple definitions, as well as forcing mutability on some attributes. Dual events can sidestep this issue (one for the client with just what it provides and another for the domain including what it enriched the event from the client with) but this is the command event model, and the linguistic problems still exist. You can see a further example of the linguistic problems involved in error conditions. How should the domain handle the fact that a client told it to do something that it cannot? This condition can exist for many reasons but let’s imagine a simple one of the client not having enough information to source the event in a known correct way. Linguistically the command/event separation makes more sense here as the command arrives in the imperative “Place Sale” while the event is in the past tense “Sale Completed”. It's natural for the domain to reject a client attempting to “Place a sale”, it's not natural for the domain to tell the client that something in the past tense no longer happened. Consider the discussion with a domain expert. Does the domain have a time machine? Parallel realities are far too complex and costly to model in most business systems. These are the problems that led to the separation of the concepts of Commands and Events. This separation makes the language much clearer and although subtle, it tends to lead developers towards a clearer understanding of context based solely on the language used. Dual definitions of a concept force the developer to recognize and distinguish context. This weight can translate into both ramp up time for new developers on a project and another thing a member of the team needs to remember. Anytime a team member needs to remember something to distinguish context there is a higher probability that they will overlook it or mistaken for another context. Being explicit in the language and avoiding dual definitions helps make things clearer both for domain experts, the developers, and anyone who may be consuming the API. Further Reading DDD CQRS Video Event Sourcing Getting Started"
  },
  "event-sourcing-basics/impendence-missmatch/index.html": {
    "href": "event-sourcing-basics/impendence-missmatch/index.html",
    "title": "Impedance Mismatch | Event Store",
    "keywords": "Impedance Mismatch Using events as a storage mechanism offers different properties when compared to a typical relational model, as the impedance mismatch that exists between a typical relational model and the object oriented domain model is analyzed. Scott Ambler describes the problem in an essay on agiledata.org as: Why does this impedance mismatch exist? The object-oriented paradigm is based on proven software engineering principles. The relational paradigm, however, is based on proven mathematical principles. Because the underlying paradigms are different the two technologies do not work together seamlessly. The impedance mismatch becomes apparent when you look at the preferred approach to access: With the object paradigm you traverse objects via their relationships whereas with the relational paradigm you join the data rows of tables. This fundamental difference results in a non-ideal combination of object and relational technologies, although when have you ever used two different things together without a few hitches? Ambler The impedance mismatch between the domain model and the relational database has a large cost associated with it. There are many tools that aim to help minimize the effects of the impedance mismatch such as Object Relational Mappers (ORMs). They tend to work well in most situations but there is a large cost associated to the impedance mismatch even when using tools such as ORMs. The cost is that a developer needs to be intimately familiar with both the relational model and the object oriented model. They also need to be familiar with the subtle differences between the two models. Scott identifies this with: To succeed using objects and relational databases together you need to understand both paradigms, and their differences, and then make intelligent tradeoffs based on that knowledge. Ambler You can find some of these subtle differences on Wikipedia under the \" Object-Relational Impedance Mismatch \" page but to include some of the major differences: Declarative vs. imperative interfaces Relational thinking tends to use data as interfaces, not behaviour as interfaces. It thus has a declarative tilt in design philosophy in contrast to Object-oriented programming’s behavioural tilt. (Some relational proponents propose using triggers, stored procedures, etc. to provide complex behaviour, but this is not a common viewpoint.) Object-Relational Impedance Mismatch Structure vs. behaviour - Object-oriented programming primarily focuses on ensuring that the structure of the program is reasonable (maintainable, understandable, extensible, reusable, safe), whereas relational systems focus on what kind of behaviour the resulting run-time system has (efficiency, adaptability, fault-tolerance, liveness, logical integrity, etc.). Object-oriented methods generally assume that the primary user of the object-oriented code and its interfaces are the application developers. In relational systems, the end-user’s view of the behaviour of the system is sometimes considered to be more important. However, relational queries and \"views\" are common techniques to re-represent information in application- or task-specific configurations. Further, relational does not prohibit local or application-specific structures or tables from being created, although many common development tools do not directly provide such a feature, assuming objects will be used instead. This makes it difficult to know whether the stated non-developer perspective of relational is inherent to relational, or merely a product of current practice and tObject-oriented programmingl implementation assumptions. Object-Relational Impedance Mismatch Set vs. graph relationships The relationship between different items (objects or records) tend to be handled differently between the paradigms. Relational relationships are usually based on idioms taken from set theory, while object relationships lean toward idioms adopted from graph theory (including trees). While each can represent the same information as the other, the approaches they provide to access and manage information differ. Object-Relational Impedance Mismatch There are many other subtle differences such as data types, identity, and how transactions work. The object-relational impedance mismatch can be quite a pain to deal with and it requires a very large amount of knowledge to deal with effectively. There is not an impedance mismatch between events and the domain model. The events are themselves a domain concept. The idea of replaying events to reach a given state is also a domain concept. The entire system becomes defined in domain terms. Defining everything in domain terms not only lowers the amount of knowledge that developers need to have but it also limits the number of representations of the model needed as the events are directly tied to the domain model itself."
  },
  "server/setting_up_ssl_linux/index.html": {
    "href": "server/setting_up_ssl_linux/index.html",
    "title": "Setting up SSL on Ubuntu 16.04 | Event Store",
    "keywords": "Setting up SSL on Ubuntu 16.04 The steps to set up SSL on Ubuntu 16.04 are as follows: First, create a private key and self-signed certificate request (This is only for testing purposes) openssl req \\ -x509 -sha256 -nodes -days 365 -subj \"/CN=eventstore.org\" \\ -newkey rsa:2048 -keyout eventstore.pem -out eventstore.csr Export a p12 file from the certificate request. You will use this when starting Event Store : openssl pkcs12 -export -inkey eventstore.pem -in eventstore.csr -out eventstore.p12 You need to add the certificate to Ubuntu's trusted certificates. Copy the cert to the ca-certificates folder and update the certificates : sudo cp eventstore.csr /usr/local/share/ca-certificates/eventstore.crt sudo update-ca-certificates The mono framework has its own separate certificate store which you need to sync with the changes you made to Ubuntu's certificates. You will first need to install mono-devel : sudo apt-get install mono-devel This installs cert-sync , which you can use to update mono's certificate store with the new certificate : sudo cert-sync eventstore.csr Start Event Store with the following configuration: <!-- TODO: How? --> CertificateFile: eventstore.p12 ExtSecureTcpPort: 1115 Connect to Event Store using the Event Store .NET Client. var settings = ConnectionSettings.Create().UseSslConnection(\"eventstore.org\", true); using (var conn = EventStoreConnection.Create(settings, new IPEndPoint(IPAddress.Loopback, 1115))) { conn.ConnectAsync().Wait(); }"
  },
  "server/index.html": {
    "href": "server/index.html",
    "title": "Running Event Store | Event Store",
    "keywords": "Running Event Store Event Store runs as a server, that clients can connect either over HTTP or using one of the client APIs. You can run both the open source and commercial versions, as either a single node, or a highly available cluster of nodes. The open source version of Event Store is distributed as a console application. There are separate distributions for Windows on .NET and Linux/macOS on Mono. Running the Open Source version Warning Unless passed a database option, Event Store will write to a new database created in the system’s temporary files path each time it is started. For more information on Command Line Arguments read this guide . On Windows and .NET <!-- TODO: Duplication, turn other into partial and reuse --> A typical command line for running Event Store server on Windows is: EventStore.ClusterNode.Exe --db .\\ESData Setting up HTTP Permissions Event Store has an HTTP interface and the identity which you want to run Event Store with must have permission to listen to incoming HTTP requests, as detailed here . To configure an account with permission to listen for incoming HTTP requests, you execute the following in PowerShell, or the Command Prompt, running as administrator (replace DOMAIN\\username with the actual account details, and the port number if you are not using the default port). netsh http add urlacl url=http://+:2113/ user=DOMAIN\\username <!-- TODO: Is this still true and can it be handled better? --> If you receive 503 errors from the web UI There is a known issue with the .NET HTTPListener class (which Event Store uses) and bad URL ACL registrations which can cause servers to return 503 errors for every request. If you see this, you can issue the following commands: netsh http show urlacl Look for an entry on the port you’re trying to use ( 2113 unless you’ve specified a custom port). It will probably look something like: http://+:2113 . Then issue: netsh http delete urlacl <the entry you just found> For example: netsh http delete urlacl http://+:2113 This should resolve the issue. On Linux/macOS A typical command line for running Event Store server on Linux/macOS is: ./run-node.sh --db ./ESData Although you can run Event Store binary directly, a run-node we provide a shell script which exports the environment variable LD_LIBRARY_PATH to include the installation path of Event Store. This is necessary if you are planning to use projections. Event Store builds for both Linux and macOS have the Mono runtime bundled in, this means that you do not need Mono installed locally to run Event Store. Shutting down an Event Store node Event Store is designed to be safe by default and it is expected that it will be shut down using kill -9 . However, it is also possible to initiate a shutdown via the web UI, by clicking on the Shutdown Server button located on the Admin page. This may be useful if you do not have console access to the node, or need to script initiating a shutdown."
  },
  "server/default-directories/index.html": {
    "href": "server/default-directories/index.html",
    "title": "Default Directories | Event Store",
    "keywords": "Default Directories The default directories used by Event Store vary by platform to best fit with the expectations of users in each case. Warning Paths beginning with \".\" are relative to the directory in which eventstored or EventStore.ClusterNode.exe are located. Absolute paths are as written. Linux Application: /usr/bin (when installed via Debian package) Content: /usr/share/eventstore Configuration: /etc/eventstore/ Data: /var/lib/eventstore Application Logs: /var/log/eventstore Test Client Logs: ./testclientlog (not included in Debian package) Web Content: ./clusternode-web then {Content}/clusternode-web Projections: ./projections then {Content}/projections Prelude: ./Prelude then {Content}/Prelude All other OSes (Includes Windows/macOS) Content: ./ Configuration: ./ Data: ./data Application Logs: ./logs Test Client Log: ./testclientlogs Web Content: ./clusternode-web then {Content}/clusternode-web Projections: ./projections then {Content}/projections Prelude: ./Prelude then {Content}/Prelude"
  },
  "server/64-bit-index/index.html": {
    "href": "server/64-bit-index/index.html",
    "title": "Rebuilding Indexes | Event Store",
    "keywords": "Rebuilding Indexes As of version 3.9.0 all future indexes will use 64-bit hashes instead of 32-bit hashes. The system will automatically transition from 32-bit to 64-bit by writing all new indexes as 64-bit indexes during the merge process. If you prefer to use only 64-bit indexes immediately you can force this. For a small database, delete the index <!-- TODO: Which is where?--> folder and let it rebuild (this might take a while) If you have a large database, remote storage, etc and cannot lose downtime, you can do this operation offline on another node with the following steps: Take a back up. Restore the backup to fast local disks. Delete the index folder from back up. Run Event Store with a cluster size 3 to prevent other writes. It will rebuild the index. Restore the index back to a node ( index folder). Let Event Store catch up from master. Repeat the restore for other nodes. For other indices, you index will eventually become 64 bit due to the merging process that occurs over time."
  },
  "http-api/competing-consumers/index.html": {
    "href": "http-api/competing-consumers/index.html",
    "title": "Competing Consumers | Event Store",
    "keywords": "Competing Consumers This document explains how to use HTTP API for setting up and consuming competing consumer subscription groups. For an overview on competing consumers and how they relate to other subscription types please see the overview document . Note The Administration UI includes a Competing Consumers section where a user is able to create, update, delete and view subscriptions and their statuses. Creating a Persistent Subscription Before interacting with a subscription group, you need to create one. You will receive an error if you attempt to create a subscription group more than once. This requires admin permissions . URI Supported Content Types Method /subscriptions/{stream}/{subscription_name} application/json PUT Query Parameters Parameter Description stream The stream the persistent subscription is on. subscription_name The name of the subscription group. Body Parameter Description resolveLinktos Tells the subscription to resolve link events. startFrom Start the subscription from the position-of the event in the stream. extraStatistics Tells the backend to measure timings on the clients so statistics will contain histograms of them. checkPointAfterMilliseconds The amount of time the system should try to checkpoint after. liveBufferSize The size of the live buffer (in memory) before resorting to paging. readBatchSize The size of the read batch when in paging mode. bufferSize The number of messages that should be buffered when in paging mode. maxCheckPointCount The maximum number of messages not checkpointed before forcing a checkpoint. maxRetryCount Sets the number of times a message should be retried before considered a bad message. maxSubscriberCount Sets the maximum number of allowed subscribers. messageTimeoutMilliseconds Sets the timeout for a client before the message will be retried. minCheckPointCount The minimum number of messages to write a checkpoint for. namedConsumerStrategy RoundRobin/DispatchToSingle/Pinned Updating a Persistent Subscription You can edit the settings of an existing subscription while it is running. This will drop the current subscribers and will reset the subscription internally. This requires admin permissions. URI Supported Content Types Method /subscriptions/{stream}/{subscription_name} application/json POST Query Parameters Parameter Description stream The stream to the persistent subscription is on. subscription_name The name of the subscription group. Body Same parameters as \"Creating a Persistent Subscription\" Deleting a Persistent Subscription URI Supported Content Types Method /subscriptions/{stream}/{subscription_name} application/json DELETE Query Parameters Parameter Description stream The stream to the persistent subscription is on. subscription_name The name of the subscription group. Reading a stream via a Persistent Subscription By default, reading a stream via a persistent subscription will return a single event per request and will not embed the event properties as part of the response. URI Supported Content Types Method /subscriptions/{stream}/{subscription_name} /subscriptions/{stream}/{subscription_name}?embed={embed} /subscriptions/{stream}/{subscription}/{count}?embed={embed} application/vnd.eventstore.competingatom+xml application/vnd.eventstore.competingatom+json GET Query Parameters Parameter Description stream The stream the persistent subscription is on. subscription_name The name of the subscription group. count How many events to return for the request. embed None , Content , Rich , Body , PrettyBody , TryHarder Read Reading Streams for information on the different embed levels. Response { \"title\": \"All Events Persistent Subscription\", \"id\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1\", \"updated\": \"2015-12-02T09:17:48.556545Z\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": false, \"links\": [{ \"uri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/ack%3Fids=c322e299-cb73-4b47-97c5-5054f920746f\", \"relation\": \"ackAll\" }, { \"uri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/nack%3Fids=c322e299-cb73-4b47-97c5-5054f920746f\", \"relation\": \"nackAll\" }, { \"uri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/1%3Fembed=None\", \"relation\": \"previous\" }, { \"uri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1\", \"relation\": \"self\" }], \"entries\": [{ \"title\": \"1@newstream\", \"id\": \"http://localhost:2113/streams/newstream/1\", \"updated\": \"2015-12-02T09:17:48.556545Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"SomeEvent\", \"links\": [{ \"uri\": \"http://localhost:2113/streams/newstream/1\", \"relation\": \"edit\" }, { \"uri\": \"http://localhost:2113/streams/newstream/1\", \"relation\": \"alternate\" }, { \"uri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/ack/c322e299-cb73-4b47-97c5-5054f920746f\", \"relation\": \"ack\" }, { \"uri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/nack/c322e299-cb73-4b47-97c5-5054f920746f\", \"relation\": \"nack\" }] }] } Acknowledgements Clients must acknowledge (or not acknowledge) messages in the competing consumer model. If the client fails to respond in the given timeout period, the message will be retried. You should use the rel links in the feed for acknowledgements not bookmark URIs as they are subject to change in future versions. For example: { \"uri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/ack/c322e299-cb73-4b47-97c5-5054f920746f\", \"relation\": \"ack\" } Ack multiple messages URI Supported Content Types Method /subscriptions/{stream}/{subscription_name}/ack?ids={messageids} application/json POST Query Parameters Parameter Description stream The stream the persistent subscription is on. subscription_name The name of the subscription group. messageids The ids of the messages that needs to be acked Ack a single message URI Supported Content Types Method /subscriptions/{stream}/{subscription_name}/ack/{messageid} application/json POST Query Parameters Parameter Description stream The stream to the persistent subscription is on. subscription_name The name of the subscription group. messageid The id of the message that needs to be acked <!-- Has this been explained? --> Nack multiple messages URI Supported Content Types Method /subscriptions/{stream}/{subscription_name}/nack?ids={messageids}?action={action} application/json POST Query Parameters Parameter Description stream The stream to the persistent subscription is on. subscription_name The name of the subscription group. action Park : Don't retry the message, park it until a request is sent to reply the parked messages Retry : Retry the message Skip : Discard the message Stop : Stop the subscription messageid The id of the message that needs to be acked Nack a single message URI Supported Content Types Method /subscriptions/{stream}/{subscription_name}/nack/{messageid}?action={action} application/json POST Replaying parked messages URI Supported Content Types Method /subscriptions/{stream}/{subscription_name}/replayParked application/json POST Getting information for all subscriptions URI Method /subscriptions GET Response [ { \"links\": [ { \"href\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/info\", \"rel\": \"detail\" } ], \"eventStreamId\": \"newstream\", \"groupName\": \"competing_consumers_group1\", \"parkedMessageUri\": \"http://localhost:2113/streams/$persistentsubscription-newstream::competing_consumers_group1-parked\", \"getMessagesUri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/1\", \"status\": \"Live\", \"averageItemsPerSecond\": 0.0, \"totalItemsProcessed\": 0, \"lastProcessedEventNumber\": -1, \"lastKnownEventNumber\": 5, \"connectionCount\": 0, \"totalInFlightMessages\": 0 }, { \"links\": [ { \"href\": \"http://localhost:2113/subscriptions/another_newstream/competing_consumers_group1/info\", \"rel\": \"detail\" } ], \"eventStreamId\": \"another_newstream\", \"groupName\": \"competing_consumers_group1\", \"parkedMessageUri\": \"http://localhost:2113/streams/$persistentsubscription-another_newstream::competing_consumers_group1-parked\", \"getMessagesUri\": \"http://localhost:2113/subscriptions/another_newstream/competing_consumers_group1/1\", \"status\": \"Live\", \"averageItemsPerSecond\": 0.0, \"totalItemsProcessed\": 0, \"lastProcessedEventNumber\": -1, \"lastKnownEventNumber\": -1, \"connectionCount\": 0, \"totalInFlightMessages\": 0 } ] Getting information about the subscriptions for a stream URI Supported Content Types Method /subscriptions/{stream} application/json GET Response [ { \"links\": [ { \"href\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/info\", \"rel\": \"detail\" } ], \"eventStreamId\": \"newstream\", \"groupName\": \"competing_consumers_group1\", \"parkedMessageUri\": \"http://localhost:2113/streams/$persistentsubscription-newstream::competing_consumers_group1-parked\", \"getMessagesUri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/1\", \"status\": \"Live\", \"averageItemsPerSecond\": 0.0, \"totalItemsProcessed\": 0, \"lastProcessedEventNumber\": -1, \"lastKnownEventNumber\": 5, \"connectionCount\": 0, \"totalInFlightMessages\": 0 } ] Getting information about a specific subscription URI Supported Content Types Method /subscriptions/{stream}/{subscription_name}/info application/json GET Response { \"links\": [{ \"href\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/info\", \"rel\": \"detail\" }, { \"href\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/replayParked\", \"rel\": \"replayParked\" } ], \"config\": { \"resolveLinktos\": false, \"startFrom\": 0, \"messageTimeoutMilliseconds\": 10000, \"extraStatistics\": false, \"maxRetryCount\": 10, \"liveBufferSize\": 500, \"bufferSize\": 500, \"readBatchSize\": 20, \"preferRoundRobin\": true, \"checkPointAfterMilliseconds\": 1000, \"minCheckPointCount\": 10, \"maxCheckPointCount\": 500, \"maxSubscriberCount\": 10, \"namedConsumerStrategy\": \"RoundRobin\" }, \"eventStreamId\": \"newstream\", \"groupName\": \"competing_consumers_group1\", \"status\": \"Live\", \"averageItemsPerSecond\": 0.0, \"parkedMessageUri\": \"http://localhost:2113/streams/$persistentsubscription-newstream::competing_consumers_group1-parked\", \"getMessagesUri\": \"http://localhost:2113/subscriptions/newstream/competing_consumers_group1/1\", \"totalItemsProcessed\": 0, \"countSinceLastMeasurement\": 0, \"lastProcessedEventNumber\": -1, \"lastKnownEventNumber\": 5, \"readBufferCount\": 6, \"liveBufferCount\": 5, \"retryBufferCount\": 0, \"totalInFlightMessages\": 0, \"connections\": [] }"
  },
  "event-sourcing-basics/events-as-a-storage-mechanism/index.html": {
    "href": "event-sourcing-basics/events-as-a-storage-mechanism/index.html",
    "title": "Events as a storage mechanism | Event Store",
    "keywords": "Events as a storage mechanism When most people consider storage for an object they tend to think about it in a structural sense. That is when considering how to store the sale discussed above they think of it as a \"Sale\" that has \"Line Items\" and perhaps \"Shipping Information\" associated with it. This is not the only way to conceptualize the problem and other solutions offer different and often interesting architectural properties. Consider the creation of a small \"Order\" object for a web-based sale system. Most developers would envision something similar to the image below. An \"Order\" has \"n Line Items\" and \"Shipping Information\". <!-- ![A simplified structural model of an order][1] --> This is not the only way to view this data. Earlier we discussed the concept of a transaction. Developers deal with the concept of transactions regularly, you can view them as representing the change between a point and the next subsequent point, and they are often referred to as “deltas”. You can define the delta between two static states, but frequently this is an implicit concept, relegated to a framework such as Hibernate in the Java world or Entity Framework in the Microsoft world. These frameworks save the original state and then calculate the differences with the new state and update the backing data model accordingly. Making these deltas explicit can be valuable for technical and business benefits. You can see the usage of deltas in many mature business models. The canonical example of delta usage is in the field of accounting. When looking at a ledger such as below, each transaction or delta is recorded. Next to it is a denormalized total of the state of the account at the end of that delta. To calculate this value, the current delta is applied to the last known value. The last known value can be trusted because at any given point you could re-run the transactions from the \"beginning of time\" for that account to reconcile the validity of that value. A verifiable audit log always exists. Date Comment Change Current Balance 1/Jan/2000 Deposit from 1372 +10,000.00 10,000.00 3/Jan/2000 Check 1 -4,000.00 6,000.00 4/Jan/2000 Purchase Coffee -3.00 5,997.00 4/Jan/2000 Purchase Internet -5.00 5,992.00 8/Jan/2000 Deposit from 1373 +1,000.00 6,992.00 Because the transactions or deltas associated with the account exist, you can step through them, verifying the result at each stage. You can derive the “Current Balance” at any point by looking at the “Current Balance” or by adding up all the “Changes” since the beginning of time for the account. The second property is valuable in a domain such as accounting as accountants are dealing with money and the ability to check that calculations were performed correctly is invaluable. It was more valuable before computers when it was common place to have an exhausted accountant make a mistake in a calculation at 3am when they should be sleeping. There are other interesting properties to this mechanism of representing state. As an example, it's possible to go back and look at what a state was at a given. Consider for example, that the account has reached a balance below zero and there is a rule that says it's not supposed to. It is relatively easy to view the state the account was in prior to processing the transaction that put it into the invalid state. This makes it far easier to reproduce what often end up as heisenbugs in other circumstances. These types of benefits are not only limited to naturally transaction based domains. Every domain is a naturally transaction-based domain when Domain Driven Design is applied. When applying Domain Driven Design there is a heavy focus on behaviours normally coinciding with use cases, Domain Driven Design is interested in how users use the system. Returning to the order example from earlier, you could represent the same order in the form of a transactional model as below: <!-- ![Transactional view of an order][2] --> You can apply this to any type of object. By replaying through the events, you can return the object to the last known state. It is mathematically equivalent to storing the end of the equation or the equation that represents it. There is a structural representation of the object but it exists only by replaying previous transactions to return the structure to its last known state. Data is not persisted in a structure but as a series of transactions. One interesting possibility is that unlike when storing current state in a structural way, there is no coupling between the representation of current state in the domain and in storage. The representation of current state in the domain can vary without thought of the persistence mechanism. It is important to note the language in Figure 3. All of the verbs are in the past tense. These are Domain Events. Consider what would happen if the language were in the imperative tense, \"Add 2 socks item 137\", \"Create Cart\". What if there were behaviours associated with adding an item, such as reserving it from an inventory system via a webservice call? Should you include these behaviours when reconstituting an object? What if logic has changed so that this item could no longer be added given the context? This is one of many examples where dual contexts between commands and events are required, there is a contextual difference between returning to a given state and attempting to transition to a new one. There is no Delete A common question that arises is how to delete information. It is not possible, as previously, to jump into the time machine and say that an event never happened (e.g. delete a previous event). As such, it is necessary to model a delete explicitly as a new transaction as shown below. Further discussion on the business value of handling deletes in this mechanism can be found in \" Business Value of the Event Log \". <!-- ![Transactional view of an order with delete][3] --> In the event stream above, the two pairs of socks were added then later removed. The end state is equivalent to not having added the two pairs of socks. However, the data was not deleted, new data was added to bring the object to the state as if the first event had not happened, this process is known as a \"Reversal Transaction\". By placing a reversal transaction in the event stream, not only is the object returned to the state as if the item had not been added, the reversal leaves a trail that shows that the object had been in that state at a given point in time."
  },
  "event-sourcing-basics/event-store-as-a-functional-database/index.html": {
    "href": "event-sourcing-basics/event-store-as-a-functional-database/index.html",
    "title": "Event Store as a functional database | Event Store",
    "keywords": "Event Store as a functional database Much of what we have discussed can be looked at through a functional programming perspective as well. For developers in functional languages such as Scala or Haskell this should feel natural to you, for C# developers it should feel familiar, and for Java developers I hear that Scala is a nice language. All kidding aside… When we “replay” an event stream we are returning a series of events. An event is essentially a serialized method call. We left fold something that redefines what those methods mean to us today in order to get our current state. This can be seen explicitly when looking at how the projections work in JavaScript. We define a function as: when([SomePatternMatch], function(state, event) { return new state; }); These functions are then chained over the event stream resulting at the end with a state object. The state is passed from one function to the next allowing each function to transform it. Said differently, Current State is a [left fold][6] of previous facts. We can further continue this to say that a snapshot is nothing but a memoization of the left fold. When looked at from this perspective one could state that an Event Store is actually a functional database."
  },
  "dotnet-api/reading-events/index.html": {
    "href": "dotnet-api/reading-events/index.html",
    "title": "Reading Events | Event Store",
    "keywords": "Reading Events You can use the client API to read events from a stream starting from either end of the stream. There is a method for each direction and one for reading a single event. Methods <!-- TODO: Do the same apply here as in HTTP API --> Read a single event Task<EventReadResult> ReadEventAsync(string stream, long eventNumber, bool resolveLinkTos); Read a specific stream forwards Task<StreamEventsSlice> ReadStreamEventsForwardAsync(string stream, long start, int count, bool resolveLinkTos) Read a specific stream backwards Task<StreamEventsSlice> ReadStreamEventsBackwardAsync(string stream, long start, int count, bool resolveLinkTos) Read all events forwards Task<AllEventsSlice> ReadAllEventsForwardAsync(Position position, int maxCount, bool resolveLinkTos); Read all events backwards Task<AllEventsSlice> ReadAllEventsBackwardAsync(Position position, int maxCount, bool resolveLinkTos); Note These methods also have an optional parameter which allows you to specify the UserCredentials to use for the request. If you don't supply any, the default credentials for the EventStoreConnection are used ( See Connecting to a Server - User Credentials ). StreamEventsSlice The reading methods for individual streams each return a StreamEventsSlice , which is immutable. The available members on StreamEventsSlice are: Member Description string Stream The name of the stream for the slice ReadDirection ReadDirection Either ReadDirection.Forward or ReadDirection.Backward depending on which method was used to read long FromEventNumber The sequence number of the first event in the stream long LastEventNumber The sequence number of the last event in the stream long NextEventNumber The sequence number from which the next read should be performed to continue reading the stream bool IsEndOfStream Whether this slice contained the end of the stream at the time it was created ResolvedEvent[] Events An array of the events read. See the description of how to interpret a Resolved Events below for more information on this ResolvedEvent When you read events from a stream (or received over a subscription) you receive an instance of the RecordedEvent class packaged inside a ResolvedEvent . Event Store supports a special event type called 'Link Events'. Think of these events as pointers to an event in another stream. In situations where the event you read is a link event, ResolvedEvent allows you to access both the link event itself, as well as the event it points to. The members of this class are as follows: Member Description RecordedEvent Event The event, or the resolved link event if this ResolvedEvent is a link event RecordedEvent Link The link event if this ResolvedEvent is a link event RecordedEvent OriginalEvent Returns the event read or which triggered the subscription. If this ResolvedEvent represents a link event, the link will be the OriginalEvent , otherwise it will be the event bool IsResolved Indicates whether this ResolvedEvent is a resolved link event Position? OriginalPosition The logical position of the OriginalEvent string OriginalStreamId The stream name of the OriginalEvent long OriginalEventNumber The event number in the stream of the OriginalEvent Note To ensure that the Event Store server follows link events when reading, ensure you set the ResolveLinkTos parameter to true when calling read methods. RecordedEvent RecordedEvent contains all the data about a specific event. Instances of this class are immutable, and expose the following members: Member Description string EventStreamId The Event Stream this event belongs to Guid EventId The Unique Identifier representing this event long EventNumber The number of this event in the stream string EventType The event type (supplied when writing) byte[] Data A byte array representing the data of this event byte[] Metadata A byte array representing the metadata associated with this event bool IsJson Indicates whether the content was internally marked as json DateTime Created A datetime representing when this event was created. long CreatedEpoch A long representing the milliseconds since the epoch when the was created. Read a single event The ReadEventAsync method reads a single event from a stream at a specified position. This is the simplest case of reading events, but is still useful for situations such as reading the last event in the stream used as a starting point for a subscription. This function accepts three parameters: Parameter Description string stream The stream to read from long eventNumber The event number to read (use StreamPosition.End to read the last event in the stream) bool resolveLinkTos Determines whether any link events encountered in the stream will be resolved. See the discussion on Resolved Events for more information on this This method returns an instance of EventReadResult which indicates if the read was successful, and if so the ResolvedEvent that was read. Reading a stream forwards The ReadStreamEventsForwardAsync method reads the requested number of events in the order in which they were originally written to the stream from a nominated starting point in the stream. The parameters are: Parameter Description string Stream The name of the stream to read long start The earliest event to read (inclusive). For the special case of the start of the stream, you should use the constant StreamPosition.Start . int count The maximum number of events to read in this request (assuming that many exist between the start specified and the end of the stream) bool resolveLinkTos Determines whether any link events encountered in the stream will be resolved. See the discussion on Resolved Events for more information on this Example: Read an entire stream forwards from start to end This example uses the ReadStreamEventsForwardAsync method in a loop to page through all events in a stream, reading 200 events at a time to build a list of all the events in the stream. var streamEvents = new List<ResolvedEvent>(); StreamEventsSlice currentSlice; var nextSliceStart = StreamPosition.Start; do { currentSlice = _eventStoreConnection.ReadStreamEventsForward(\"myStream\", nextSliceStart, 200, false) .Result; nextSliceStart = currentSlice.NextEventNumber; streamEvents.AddRange(currentSlice.Events); } while (!currentSlice.IsEndOfStream); Note It's unlikely that client code would need to build a list in this manner. It's far more likely that you would pass events into a left fold to derive the state of some object as of a given event. Read a stream backwards The ReadStreamEventsBackwardAsync method reads the requested number of events in the reverse order from that in which they were originally written to the stream from a specified starting point. The parameters are: Parameter Description string Stream The name of the stream to read long start The latest event to read (inclusive). For the end of the stream use the constant StreamPosition.End int count The maximum number of events to read in this request (assuming that many exist between the start specified and the start of the stream) bool resolveLinkTos Determines whether any link events encountered in the stream will be resolved. See the discussion on Resolved Events for more information on this Read all events Event Store allows you to read events across all streams using the ReadAllEventsForwardAsync and ReadAllEventsBackwardsAsync methods. These work in the same way as the regular read methods, but use an instance of the global log file Position to reference events rather than the simple integer stream position described previously. They also return an AllEventsSlice rather than a StreamEventsSlice which is the same except it uses global Position s rather than stream positions. Example: Read all events forward from start to end var allEvents = new List<ResolvedEvent>(); AllEventsSlice currentSlice; var nextSliceStart = Position.Start; do { currentSlice = connection.ReadAllEventsForwardAsync(nextSliceStart, 200, false).Result; nextSliceStart = currentSlice.NextPosition; allEvents.AddRange(currentSlice.Events); } while (!currentSlice.IsEndOfStream);"
  },
  "server/database-backup/index.html": {
    "href": "server/database-backup/index.html",
    "title": "Database Backup | Event Store",
    "keywords": "Database Backup Backing up an Event Store database is straightforward, but relies on you carrying out the steps below in the correct order. Backing up a database Copy all *.chk files to the backup location. Copy the remaining files and directories to the backup location. Example: rsync -a /data/eventstore/db/*.chk /backup/eventstore/db/ rsync -a /data/eventstore/db/index /backup/eventstore/db/ rsync -a /data/eventstore/db/*.0* /backup/eventstore/db/ Restoring a database Create a copy of chaser.chk and call it truncate.chk . Copy all files to the desired location. Note Many people do not rely on hot backups in a highly available cluster but instead increase their node counts to keep further copies of data. Differential backup Eventstore stores most data in chunk files , named chunkX.Y , where X is the chunk number, and Y is the version of that chunk file. As Event Store scavenges, it creates new versions of scavenged chunks which are interchangeable with older versions (but for the removed data). Consequently, it is only necessary to keep the file whose name has the highest Y for each X , as well as the checkpoint files and the index directory (to avoid expensive index rebuilding). Other Options There are many other options available for backing up an Event Store database. For example it is possible to set up a durable subscription that would write all the events to another storage mechanism such as a key/value or column store. These methods would require a manual set up for restoring back to a cluster group. You can expand upon this option to use a second Event Store node/cluster as a back up. This is commonly known as a primary/secondary back up scheme. The primary cluster runs and asynchronously pushes data to a second cluster as described above. The second cluster/node is available in case of disaster on the primary cluster. If you are using this strategy then we recommend you only support manual failover from Primary to Secondary as automated strategies risk causing a split brain problem."
  },
  "server/cluster-without-manager-nodes/index.html": {
    "href": "server/cluster-without-manager-nodes/index.html",
    "title": "Setting up a Cluster using only Database Nodes (OSS) | Event Store",
    "keywords": "Setting up a Cluster using only Database Nodes (OSS) The clustering code for Event Store is open source under the BSD-3 license as the rest of the code. This guide will look at how you can setup a highly available cluster using the open source components. When setting up a cluster, you generally want an odd number of nodes as Event Store uses a quorum based algorithm to handle high availability. Running on the Same Machine To start, you will set up three nodes running on a single machine. Run each of the commands below in its own console window, remember that you either need admin privileges or have setup ACLs with IIS if running under Windows (Unix-like operating systems need no configuration). Replace \"127.0.0.1\" with whatever IP address you want to run on. Warning This must be an interface present on the machine. start EventStore.ClusterNode.exe --mem-db --log .\\logs\\log1 --int-ip 127.0.0.1 --ext-ip 127.0.0.1 --int-tcp-port=1111 --ext-tcp-port=1112 --int-http-port=1113 --ext-http-port=1114 --cluster-size=3 --discover-via-dns=false --gossip-seed=127.0.0.1:2113,127.0.0.1:3113 start EventStore.ClusterNode.exe --mem-db --log .\\logs\\log2 --int-ip 127.0.0.1 --ext-ip 127.0.0.1 --int-tcp-port=2111 --ext-tcp-port=2112 --int-http-port=2113 --ext-http-port=2114 --cluster-size=3 --discover-via-dns=false --gossip-seed=127.0.0.1:1113,127.0.0.1:3113 start EventStore.ClusterNode.exe --mem-db --log .\\logs\\log3 --int-ip 127.0.0.1 --ext-ip 127.0.0.1 --int-tcp-port=3111 --ext-tcp-port=3112 --int-http-port=3113 --ext-http-port=3114 --cluster-size=3 --discover-via-dns=false --gossip-seed=127.0.0.1:1113,127.0.0.1:2113 You should now have three nodes running together in a cluster. If you kill one of the nodes, it will continue running. This binds to the loopback interface. To access Event Store from outside your machine, specify a different IP address for the --ext-ip parameter. Running on Separate Machines Most importantly is understanding the \"gossip seeds\". You are instructing seed locations for when the node first starts and needs to begin gossiping. Any node can be a seed. By giving each node the other nodes you ensure that there will always be another node to gossip with if a quorum can be built. If you wanted to move this to run on three machines you would change the IPs on the command line to something such as: EventStore.ClusterNode.exe --mem-db --log c:\\dbs\\cluster\\log1 --int-ip 192.168.0.1 --ext-ip 192.168.0.1 --int-tcp-port=1111 --ext-tcp-port=1112 --int-http-port=2113 --ext-http-port=2114 --cluster-size=3 --discover-via-dns=false --gossip-seed=192.168.0.2:2113,192.168.0.3:2113 EventStore.ClusterNode.exe --mem-db --log c:\\dbs\\cluster\\log2 --int-ip 192.168.0.2 --ext-ip 192.168.0.2 --int-tcp-port=1111 --ext-tcp-port=1112 --int-http-port=2113 --ext-http-port=2114 --cluster-size=3 --discover-via-dns=false --gossip-seed=192.168.0.1:2113,192.168.0.3:2113 EventStore.ClusterNode.exe --mem-db --log c:\\dbs\\cluster\\log3 --int-ip 192.168.0.3 --ext-ip 192.168.0.3 --int-tcp-port=1111 --ext-tcp-port=1112 --int-http-port=2113 --ext-http-port=2114 --cluster-size=3 --discover-via-dns=false --gossip-seed=192.168.0.1:2113,192.168.0.2:2113 Using DNS Entering the commands above into each node is tedious and error-prone (especially as the replica set counts go up). Another configuration option is to create a DNS entry that points to all the nodes in the cluster and then specifying that DNS entry on the command line along with the appropriate port EventStore.ClusterNode.exe --log c:\\dbs\\cluster\\log1 --int-ip 192.168.0.1 --ext-ip 192.168.0.1 --int-tcp-port=1111 --ext-tcp-port=1112 --int-http-port=2113 --ext-http-port=2114 --cluster-size=3 --cluster-dns mydomain.com --cluster-gossip-port=2113 EventStore.ClusterNode.exe --mem-db --log c:\\dbs\\cluster\\log2 --int-ip 192.168.0.2 --ext-ip 192.168.0.2 --int-tcp-port=1111 --ext-tcp-port=1112 --int-http-port=2113 --ext-http-port=2114 --cluster-size=3 --cluster-dns mydomain.com --cluster-gossip-port=2113 EventStore.ClusterNode.exe --mem-db --log c:\\dbs\\cluster\\log3 --int-ip 192.168.0.3 --ext-ip 192.168.0.3 --int-tcp-port=1111 --ext-tcp-port=1112 --int-http-port=2113 --ext-http-port=2114 --cluster-size=3 --cluster-dns mydomain.com --cluster-gossip-port=2113 Warning This method is also good for HTTP clients as you can avoid using a load balancer and fall back to round robin DNS for many deployments. Internal vs. External All communications in Event Store are optionally segregated to different networks. Internal networks for tasks like replication and external for talking to clients. You can place these communications on segregated networks if you wish and it is often a good idea to do so both for performance and security purposes. To setup an internal network all the command line parameters provided above have int- options. All communications channels also support the enabling of SSL for the connections. HTTP Clients If you want to use the HTTP API, then you would add a load balancer in front of the three nodes. It does not matter which node receives a request as the requests the node will forward the request internally. With this setup, you can lose any one machine with an easy setup. <!-- TODO: What does this mean? Better wording --> Native TCP Clients You can also connect to the cluster using the native TCP interface. The client APIs support switching between nodes internally. As such if you have a master failover the connection will automatically failover and handle retries to another node. To set up a connection as above, in the command line you provide gossip seeds to the connection. The client will use the gossip seeds to begin gossiping information about the cluster. EventStoreConnection.Create( ConnectionSettings.Create().KeepReconnecting(), ClusterSettings.Create() .WithGossipTimeoutOf(TimeSpan.FromMilliseconds(500)) .WithGossipSeeds(new [] { new IPEndPoint(IPAddress.Parse(\"192.168.0.1\"), 2113), new IPEndPoint(IPAddress.Parse(\"192.168.0.2\"), 2113), new IPEndPoint(IPAddress.Parse(\"192.168.0.3\"), 2113) })); As in the example above, you can also use DNS to avoid manually specifying the seeds. You add the nodes to a DNS record and then specify that DNS entry to the connection to locate nodes. EventStoreConnection.Create(ConnectionSettings.Create() .KeepReconnecting(), ClusterSettings.Create() .SetClusterDns(\"mycluster.com\")) .SetGossipPort(2113) Tip For those using the closed source version GossipPort is an alias for ManagerPort as the closed source version includes a node manager on each physical node. This allows for controlling many virtual nodes on a machine with easy configuration. The manager also acts as a supervisor for the nodes. As mentioned the connection will automatically reconnect during node failures. You can control this behaviour with options on the ConnectionSettings such as limiting retry attempts or frequency. The connection and durable subscription will even manage a subscription during node failures, you will not even receive duplicated messages over your durable subscription."
  },
  "projections/debugging/index.html": {
    "href": "projections/debugging/index.html",
    "title": "Debugging | Event Store",
    "keywords": "Debugging User projections you create in JavaScript have a bonus that debugging is easier via any browser that ships with debugging capabilities. The following screenshots show the use of Chrome, but we have tested debugging with all major browsers which includes Firefox, Microsoft Edge and Safari. Logging from within a Projection For debugging purposes, projections includes a log method which will, when called, send messages to the configured Event Store logger (the default is NLog and to a file as well as stdout ). You might find printing out the structure of the event body for inspection useful. For example: fromStream('$stats-127.0.0.1:2113') .when({ $any: function(s,e){ log(JSON.stringify(e)); } }) Starting and Configuring Event Store for Projections The following configuration starts Event Store with all the projection modes enabled (user and system defined) and uses an in memory database which is suitable for development purposes. EventStore.ClusterNode.exe --run-projections=all --mem-db Creating a sample projection for debugging purposes Filename: stats-counter.json Contents: fromStream('$stats-127.0.0.1:2113') .when({ $init: function(){ return { count: 0 } }, $any: function(s,e){ s.count += 1; } }) You create the projection by making a call to the API and providing it with the definition of the projection. curl -i -d@stats-counter.json http://localhost:2113/projections/continuous?name=stats-counter%26type=js%26enabled=true%26emit=true%26trackemittedstreams=true -u admin:changeit <!-- TODO: Where are these images? --> Debugging your first projection Once the projection is running, open your browser and enable the developer tools. Once you have the developer tools open, visit your projection URL and you should see a button labelled Debug . After clicking the projection \"Debug\" button, you will see the debugging interface with the definition of the projection and information about the events the projection is processing on the right hand side. At the top there are couple of buttons to take note of, specifically the Run Step and Update buttons. You use Run Step to step through the event waiting in the queue, placing you in projection debugging mode. The Update button provides you with a way to update the projection definition without having to go back to the projection itself and leave the context of the debugger. If the Run Step button is not greyed out and you click it, you will see that you browser has hit a breakpoint. You are now able to step through the projection, the important method to step into so that you can debug your projection is to step into the handler(state, eventEnvelope) method."
  },
  "getting-started/index.html": {
    "href": "getting-started/index.html",
    "title": "Step 1 - Install, run, and write your first event | Event Store",
    "keywords": "Step 1 - Install, run, and write your first event This getting started guide shows you how to get started with Event Store using the Atom publishing protocol as the primary interface. This first step covers installation and running Event Store, and writing your first event. Note The described is for development and evaluation of Event Store. It does not describe a production setup. Windows Linux Docker The prerequisites for Installing on Windows are: NET Framework 4.0+ Windows platform SDK with compilers (v7.1) or Visual C++ installed (Only required for a full build) Event Store has Chocolatey packages available that you can install with the following command: choco install eventstore-oss You can also download a binary, unzip the archive and run from the folder location with an administrator console: EventStore.ClusterNode.exe --db ./db --log ./logs This will start Event Store with the database stored at the path ./db and the logs in ./logs . You can view further command line arguments in the server docs . Event Store runs in an administration context because it starts an HTTP server through http.sys . For permanent or production instances you need to provide an ACL such as: netsh http add urlacl url=http://+:2113/ user=DOMAIN\\username The prerequisites for Installing on Linux are: We recommend Mono 4.6.2 , but later versions may also work. Event Store has pre-built packages available for Debian-based distributions , manual instructions for distributions that use RPM , or you can build from source . If you installed from a pre-built package, start Event Store with: sudo systemctl start eventstore When you install the Event Store package, the service is not started by default. This is to allow you to change configuration, located at /etc/eventstore/eventstore.conf and to prevent creating a default database. In all other cases you can run the Event Store binary or use our run-node.sh shell script which exports the environment variable LD_LIBRARY_PATH to include the installation path of Event Store, which is necessary if you are planning to use projections. ./run-node.sh --db ./ESData Note We recommend that when using Linux you set the 'open file limit' to a high number. The precise value depends on your usecase, but at minimum, between 30,000 and 60,000 . Event Store has a Docker image available for any platform that supports Docker: docker run --name eventstore-node -it -p 2113:2113 -p 1113:1113 eventstore/eventstore Event Store should now be running, and you can open http://127.0.0.1:2113/ to see the admin web interface. The console ask for a username and password. The defaults are admin:changeit . Writing Events to an Event Stream Event Store operates on a concept of Event Streams, and the first operation we look at in this guide is how to write to a stream. If you are Event Sourcing a domain model, a stream equates to an aggregate function. Event Store can handle hundreds of millions of streams, so create as many as you need. To begin, open a text editor, copy and paste the following event definition, and save it as event.json . [ { \"eventId\": \"fbf4a1a1-b4a3-4dfe-a01f-ec52c34e16e4\", \"eventType\": \"event-type\", \"data\": { \"a\": \"1\" } } ] To write the event to a stream, issue the following cURL command. Request Response curl -i -d \"@event.json\" \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/vnd.eventstore.events+json\" HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Location: http://127.0.0.1:2113/streams/newstream/0 Content-Type: text/plain; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 28 Jun 2013 12:17:59 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 Note You can also post events as XML, by changing the Content-Type header to XML . Click the Stream Browser tab in the admin web interface after running this command and you see the stream you created. If you post to a stream that doesn’t exist, Event Store creates it. Click a stream to see an HTML representation of it. Next Step In this first part of our getting started guide you learned how to install and run Event Store and write your first event. The next part covers reading events from a stream. Step 2 - Read events from a stream and subscribe to changes"
  },
  "http-api/optional-http-headers/expected-version/index.html": {
    "href": "http-api/optional-http-headers/expected-version/index.html",
    "title": "Optional HTTP Headers: Expected Version | Event Store",
    "keywords": "Optional HTTP Headers: Expected Version When writing to a stream you often want to use Expected Version to allow for optimistic concurrency with a stream. i.e. my write can succeed if I have seen everyone else's writes. You most commonly use this for a domain object projection, You can set ExpectedVersion as ES-ExpectedVersion: # . By default the ES-ExpectedVersion is -2 (append). You can set an actual version number as well or -1 to say that the stream should not exist when processing (e.g. you expect to be creating it), or -4 to say that the stream should exist with any number of events in it. If the ExpectedVersion does not match the version of the stream, Event Store will return an HTTP 400 Wrong expected EventNumber response. This response contains the current version of the stream in an ES-CurrentVersion header. In the following cURL command ExpectedVersion is not set (and it will append or create/append to the stream). Request Response curl -i -d @event.js \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Location: http://127.0.0.1:2113/streams/newstream/0 Content-Type: text/plain; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 27 Jun 2013 14:26:14 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 The stream 'newstream' now has one event. If appending with an expected version of '3' this will not work. Request Response curl -i -d @event.js \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -H \"ES-ExpectedVersion: 3\" HTTP/1.1 400 Wrong expected EventNumber Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * ES-CurrentVersion: 0 Content-Type: text/plain; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 27 Jun 2013 14:27:24 GMT Content-Length: 0 Connection: close You can see from the ES-CurrentVersion header above that the stream is at version zero. Appending with an expected version of zero will work. The expected version is always the version of the last event you know of in the stream. Request Response curl -i -d @event.js \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -H \"ES-ExpectedVersion: 0\" HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Location: http://127.0.0.1:2113/streams/newstream/1 Content-Type: text/plain; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 27 Jun 2013 14:47:10 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100"
  },
  "http-api/stream-metadata/index.html": {
    "href": "http-api/stream-metadata/index.html",
    "title": "Stream Metadata | Event Store",
    "keywords": "Stream Metadata <!-- TODO: Break up to write / read? And maybe transclude--> Every stream in Event Store has metadata associated with it. Internally, the metadata includes information such as the ACL of the stream and the maximum count and age for the events in the stream. Client code can also add information into stream metadata for use with projections or the client API. A common use case for information you may want to store in metadata is information associated with an event that is not part of the event. Examples of this might be: Which user wrote the event. Which application server were they talking to. What IP address did the request come from? Stream metadata is stored internally as JSON, and you can access it over the HTTP APIs. Reading Stream Metadata To read the metadata, issue a GET request to the attached metadata resource, which is of the form: http://{eventstore}/streams/{stream-name}/metadata You should not access metadata by constructing this URL yourself, as the right to change the resource address is reserved. Instead, you should follow the link for from the stream itself, which will enable your client to tolerate future changes to the addressing structure without breaking. Request Response curl -i http://127.0.0.1:2113/streams/$users --user admin:changeit HTTP/1.1 200 OK Cache-Control: max-age=0, no-cache, must-revalidate Content-Length: 1267 Content-Type: application/vnd.eventstore.atom+json; charset: utf-8 ETag: \"0;-2060438500\" Vary: Accept Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Date: Sun, 16 Jun 2013 15:08:49 GMT { \"title\": \"Event stream '$users'\", \"id\": \"<http://127.0.0.1:2113/streams/%24users\">, \"updated\": \"2013-06-16T15:08:47.5245306Z\", \"author\": { \"name\": \"EventStore\" }, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/%24users\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24users/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24users/0/forward/20\", \"relation\": \"last\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24users/1/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24users/metadata\", \"relation\": \"metadata\" } ], ... Once you have the URI of the metadata stream, a GET request will retrieve the metadata: Request Response curl -i http://127.0.0.1:2113/streams/$users/metadata --user admin:changeit HTTP/1.1 200 OK Cache-Control: max-age=31536000, public Content-Length: 652 Content-Type: application/vnd.eventstore.atom+json; charset: utf-8 Vary: Accept Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: GET, POST, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Date: Sun, 16 Jun 2013 13:18:29 GMT { \"title\": \"0@$$$users\", \"id\": \"<http://127.0.0.1:2113/streams/%24%24%24users/0\">, \"updated\": \"2013-06-16T12:25:13.8428624Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"$metadata\", \"content\": { \"eventStreamId\": \"$$$users\", \"eventNumber\": 0, \"eventType\": \"$metadata\", \"data\": { \"readRole\": \"$all\", \"metaReadRole\": \"$all\" }, \"metadata\": \"\" }, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/%24%24%24users/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24%24%24users/0\", \"relation\": \"alternate\" } ] } Reading metadata may require that you pass credentials if you have security enabled, as in the examples above. If you do not pass credentials and they are required you will receive a '401 Unauthorized' response. Request Response curl -i http://127.0.0.1:2113/streams/$users/metadata HTTP/1.1 401 Unauthorized Content-Length: 0 Content-Type: text/plain; charset: utf-8 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: GET, POST, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * WWW-Authenticate: Basic realm=\"ES\" Date: Sun, 16 Jun 2013 13:20:22 GMT Writing Metadata To update the metadata for a stream, issue a POST request to the metadata resource. This will replace the current metadata with the information posted. Inside a file named metadata.txt : [ { \"eventId\": \"7c314750-05e1-439f-b2eb-f5b0e019be72\", \"eventType\": \"$user-updated\", \"data\": { \"readRole\": \"$all\", \"metaReadRole\": \"$all\" } } ] You can also add user-specified metadata here. Some examples of good uses of user-specified metadata: which adapter is responsible for populating a stream. which projection caused a stream to be created. a correlation ID of some business process. This information is then posted to the stream: Request Response curl -i -d @metadata.txt http://127.0.0.1:2113/streams/$users/metadata --user admin:changeit -H \"Content-Type: application/vnd.eventstore.events+json\" HTTP/1.1 201 Created Content-Length: 0 Content-Type: text/plain; charset: utf-8 Location: http://127.0.0.1:2113/streams/%24%24%24users/1 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: GET, POST, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Date: Sun, 16 Jun 2013 14:50:21 GMT If the specified user does not have permissions to write to the stream metadata, you will receive a '401 Unauthorized' response: Request Response curl -i -d @metadata.txt http://127.0.0.1:2113/streams/$users/metadata --user invaliduser:invalidpass -H \"Content-Type: application/vnd.eventstore.events+json\" HTTP/1.1 401 Unauthorized Content-Length: 0 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * WWW-Authenticate: Basic realm=\"ES\" Date: Sun, 16 Jun 2013 14:51:37 GMT"
  },
  "event-sourcing-basics/performance-and-scaling/index.html": {
    "href": "event-sourcing-basics/performance-and-scaling/index.html",
    "title": "Performance and Scalability | Event Store",
    "keywords": "Performance and Scalability There are also architectural benefits to not deleting data. The storage system becomes an append-only architecture. Append-only architectures typically distribute and scale more easily than updating architectures because there are fewer locks to deal with. A common performance optimization is the use of \"horizontal partitioning\" (or \"sharding\"), where the same schema will exist in many places and a key within the data will determine in which place the data exists. One problem with horizontal partitioning with an RDMS is that it is necessary to define the key with which the partitioning should operate. This problem goes away when using events. Aggregate IDs are the only partition point in the system. No matter how many aggregates exist or how they may change structures, the aggregate ID associated with events is the only partition point in the system. Horizontally partitioning an Event Store is a simpler process. Saving Objects When dealing with a stereotypical system utilizing an RDMS it can be complex to figure out what has changed within the aggregate. Many tools have been built to help alleviate the pain that arises from this task but is the need for a tool a sign of a bigger problem? Most Object-relational mapping (ORM) tools figure out the changes that occurred within a graph <!-- TODO: Again, is it clear what this is? --> . They do this by maintaining two copies of a given graph. The first held in memory and the second for applications to interact with. When it's time to save, the database logic traverses the graph that the code has interacted with and uses the copy of the original graph to determine what has changed while the graph was in use by the code. These changes are then saved to the data storage system. In a system that is Domain Event centric, the aggregates are themselves tracking strong events as to what has changed within them. There is no complex process for comparing to another copy of a graph. Instead simply ask the aggregate for its changes. The operation to ask for changes is far more efficient than having to figure out what has changed. Loading Objects A similar issue exists when loading objects. Consider the work that involved with loading a graph of objects in a stereotypical relational database backed system. Often there are many queries that must be issued to build the aggregate. To help minimize the latency cost of these queries many ORMs have introduced a heuristic of \"Lazy Loading\" also known as \"Delayed Loading\", where a proxy is given in lieu of the real object. The data is only loaded when some code attempts to use that particular object. Lazy loading is useful because often a given behavior will only use a certain portion of data out of the aggregate and it prevents the developer from having to explicitly represent which data that is while reducing the cost of the loading of the aggregate. It is this need for reducing cost that shows a problem. Aggregates are considered as a whole represented by the Aggregate Root. Conceptually an Aggregate is loaded and saved in its entirety. Evans, 2001 Conceptually it is much easier to deal with the concept of an aggregate loaded and saved in its entirety. The concept of lazy loading is not a trivial one and especially when optimizing use cases. The heuristic is needed because loading full aggregates from a relational database is operationally too slow. When dealing with events as a storage mechanism things are different. There is one thing stored, events. Load all the events for an aggregate and replay them. There can only ever be a single query on the system. There is no need to attempt to implement things like lazy loading."
  },
  "http-api/swagger/Projections.html": {
    "href": "http-api/swagger/Projections.html",
    "title": "Projections | Event Store",
    "keywords": "Projections Endpoints for Projection operations"
  },
  "http-api/swagger/Stats.html": {
    "href": "http-api/swagger/Stats.html",
    "title": "Stats | Event Store",
    "keywords": "Stats Endpoints for Statistics operations"
  },
  "http-api/swagger/Streams/Read a stream.html": {
    "href": "http-api/swagger/Streams/Read a stream.html",
    "title": "Read a stream | Event Store",
    "keywords": "Read a stream Reads a stream Read a stream, receiving a standard AtomFeed document as a response. | Improve this Doc View Source Read a stream Request GET /streams/{stream}[?embed] Parameters Name Type Value Notes *stream string The stream ID embed string Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Streams/Create a stream.html": {
    "href": "http-api/swagger/Streams/Create a stream.html",
    "title": "Create a stream | Event Store",
    "keywords": "Create a stream Create a stream Create a new stream. | Improve this Doc View Source Create a stream Request POST /streams/{stream} Parameters Name Type Value Notes *stream string The name of the new stream streamItem Stream to create Responses Status Code Description Samples 201 New stream created 307 Temporary Redirect"
  },
  "http-api/swagger/Streams/Delete a stream.html": {
    "href": "http-api/swagger/Streams/Delete a stream.html",
    "title": "Delete a stream | Event Store",
    "keywords": "Delete a stream Deletes a stream Delete specified stream | Improve this Doc View Source Delete a stream Request DELETE /streams/{stream} Parameters Name Type Value Notes *stream string The stream ID to delete Responses Status Code Description Samples 204 Stream deleted"
  },
  "http-api/swagger/Streams/Alternative stream URL.html": {
    "href": "http-api/swagger/Streams/Alternative stream URL.html",
    "title": "Alternative stream URL | Event Store",
    "keywords": "Alternative stream URL An alternative URL to post events to A URL generated by Event Store if you don't supply an ID when creating a stream. You then use this URL to post events to. | Improve this Doc View Source Alternative stream URL Request POST /streams/{stream}/incoming/{guid} Parameters Name Type Value Notes *stream string The name of the stream *guid string Autogenerated UUID Responses Status Code Description Samples 200 New event created 400 Bad request"
  },
  "http-api/swagger/Streams/Read stream event.html": {
    "href": "http-api/swagger/Streams/Read stream event.html",
    "title": "Read stream event | Event Store",
    "keywords": "Read stream event Read a stream event Reads a single event from a stream. | Improve this Doc View Source Read stream event Request GET /streams/{stream}/{event}[?embed] Parameters Name Type Value Notes *stream string The stream ID *event string The event ID embed string Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Streams/Get {n} events.html": {
    "href": "http-api/swagger/Streams/Get {n} events.html",
    "title": "Get {n} events | Event Store",
    "keywords": "Get {n} events Paginate backwards through stream events Paginate backwards though stream events by a specified amount. | Improve this Doc View Source Get {n} events Request GET /streams/{stream}/{event}/{count}[?embed] Parameters Name Type Value Notes *stream string The stream ID *event string The event ID *count integer How many events to skip backwards from in the request. embed string Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Streams/Page back through events.html": {
    "href": "http-api/swagger/Streams/Page back through events.html",
    "title": "Page back through events | Event Store",
    "keywords": "Page back through events Paginate backwards through stream events Paginate backwards though stream events by a specified amount. | Improve this Doc View Source Page back through events Request GET /streams/{stream}/{event}/backward/{count}[?embed] Parameters Name Type Value Notes *stream string The stream ID *event string The event ID *count integer How many events to skip backwards from in the request. embed string Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Streams/Page forward through events.html": {
    "href": "http-api/swagger/Streams/Page forward through events.html",
    "title": "Page forward through events | Event Store",
    "keywords": "Page forward through events Paginate forwards through stream events Paginate forwards though stream events by a specified amount. | Improve this Doc View Source Page forward through events Request GET /streams/{stream}/{event}/forward/{count}[?embed] Parameters Name Type Value Notes *stream string The stream ID *event string The event ID *count integer How many events to skip forwards in the request. embed string Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Streams/Read stream metadata.html": {
    "href": "http-api/swagger/Streams/Read stream metadata.html",
    "title": "Read stream metadata | Event Store",
    "keywords": "Read stream metadata Reads the metadata of a stream Returns metadata of a stream, typically information associated with an event that is not part of the event. | Improve this Doc View Source Read stream metadata Request GET /streams/{stream}/metadata[?embed] Parameters Name Type Value Notes *stream string The stream ID embed string Responses Status Code Description Samples 200 OK"
  },
  "http-api/swagger/Streams/Update stream metadata.html": {
    "href": "http-api/swagger/Streams/Update stream metadata.html",
    "title": "Update stream metadata | Event Store",
    "keywords": "Update stream metadata Update stream metadata Update the metadata of a stream. | Improve this Doc View Source Update stream metadata Request POST /streams/{stream}/metadata Parameters Name Type Value Notes *stream string The name of the stream streamMetadataItem Metadata object Responses Status Code Description Samples 201 New stream created 400 Bad request"
  },
  "http-api/swagger/Streams/Get all events.html": {
    "href": "http-api/swagger/Streams/Get all events.html",
    "title": "Get all events | Event Store",
    "keywords": "Get all events Returns all events from all streams Returns all events from all streams, you must provide user details. | Improve this Doc View Source Get all events Request GET /streams/$all[?embed] Parameters Name Type Value Notes embed string Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Subscriptions/Get all subscriptions.html": {
    "href": "http-api/swagger/Subscriptions/Get all subscriptions.html",
    "title": "Get all subscriptions | Event Store",
    "keywords": "Get all subscriptions Get information for all subscriptions Returns all subscriptions from all streams. | Improve this Doc View Source Get all subscriptions Request GET /subscriptions Responses Status Code Description Samples 200 New persistant subscription 400 bad input parameter"
  },
  "http-api/swagger/Subscriptions/Get stream information.html": {
    "href": "http-api/swagger/Subscriptions/Get stream information.html",
    "title": "Get stream information | Event Store",
    "keywords": "Get stream information Returns stream information Needed | Improve this Doc View Source Get stream information Request GET /subscriptions/{stream} Parameters Name Type Value Notes *stream string The stream name Responses Status Code Description Samples 200 OK"
  },
  "http-api/optional-http-headers/eventid/index.html": {
    "href": "http-api/optional-http-headers/eventid/index.html",
    "title": "Optional HTTP Headers: EventID | Event Store",
    "keywords": "Optional HTTP Headers: EventID Note This event is only available in version 3.0.0 or higher of Event Store. When writing to a stream and not using the application/vnd.eventstore.events+json/+xml media type it is necessary that you specify an event ID with the event that you are posting. This is not required with the custom media type as it is also specified within the format itself (there is EventId on each entry in the format). EventId is used for idempotency within Event Store. You can include an event ID on an event by specifying this header. Request Response curl -i -d @event.json \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -H \"ES-EventType: SomeEvent\" -H \"ES-EventId: C322E299-CB73-4B47-97C5-5054F920746E\" HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream/1 Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:43:03 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 If you do not put an ES-EventId header on an append where the body is considered the actual event (e.g. not using application/vnd.eventstore.events+json/+xml ) the server will generate a unique identifier for you and redirect you to an idempotent URI where you can post your event. It is generally recommended if you can create a UUID that you should not use this facility but it is useful when you are in an environment that cannot create a UUID. Request Response curl -i -d @event.json \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -H \"ES-EventType: SomeEvent\" HTTP/1.1 301 FOUND Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream/incoming/0a61c093-2fe5-4f9f-8c4e-8589099e917 Content-Type: ; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:47:46 GMT Content-Length: 28 Keep-Alive: timeout=15,max=100 As you can see the server returned a '301 redirect' with a location header that points to http://127.0.0.1:2113/streams/newstream/incoming/0a61c093-2fe5-4f9f-8c4e-8589099e917c this generated URI is idempotent for purposes of retrying."
  },
  "http-api/reading-streams/index.html": {
    "href": "http-api/reading-streams/index.html",
    "title": "Reading Streams and Events | Event Store",
    "keywords": "Reading Streams and Events Reading a stream Event Store exposes streams as a resource located at http(s)://{yourdomain.com}:{port}/streams/{stream} . If you issue a simple GET request to this resource you will receive a standard AtomFeed document as a response. Request Response curl -i -H \"Accept:application/vnd.eventstore.atom+json\" \"http://127.0.0.1:2113/streams/newstream\" HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"0;248368668\" Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 12:15:08 GMT Content-Length: 1272 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'newstream'\", \"id\": \"<http://127.0.0.1:2113/streams/newstream\">, \"updated\": \"2017-11-09T13:10:16.111657Z\", \"streamId\": \"newstream\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": true, \"selfUrl\": \"<http://127.0.0.1:2113/streams/newstream\">, \"eTag\": \"1;-2060438500\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/2/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/metadata\", \"relation\": \"metadata\" } ], \"entries\": [ { \"title\": \"1@newstream\", \"id\": \"<http://127.0.0.1:2113/streams/newstream/1\">, \"updated\": \"2017-11-09T13:10:16.111657Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"SomeEvent\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1\", \"relation\": \"alternate\" } ] }, { \"title\": \"0@newstream\", \"id\": \"<http://127.0.0.1:2113/streams/newstream/0\">, \"updated\": \"2017-11-09T12:04:15.494635Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"event-type\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"alternate\" } ] } ] } Reading an event from a stream There some important aspects to notice here. The feed has one item in it, if there are more than one, then items are sorted from newest to oldest. For each entry, there are a series of links to the actual events, embedding data into a stream is covered later . To get an event, follow the alternate link and set your Accept headers to the mime type you would like the event in. The accepted content types for GET requests are: application/xml application/atom+xml application/json application/vnd.eventstore.atom+json text/xml text/html Request Response curl -i http://127.0.0.1:2113/streams/newstream/1 -H \"Accept: application/json\" HTTP/1.1 200 OK Access-Control-Allow-Methods: GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=31536000, public Vary: Accept Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 29 Jan 2015 15:45:45 GMT Content-Length: 572 Keep-Alive: timeout=15,max=100 { \"title\": \"1@newstream\", \"id\": \"http://127.0.0.1:2113/streams/newstream/1\", \"updated\": \"2017-11-09T13:10:16.111657Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"event-type\", \"content\": { \"eventStreamId\": \"newstream\", \"eventNumber\": 0, \"eventType\": \"event-type\", \"data\": { \"a\": \"1\" }, \"metadata\": \"\" }, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"alternate\" } ] } The atom version of the event contains extra details about the event. Request Response curl -i http://127.0.0.1:2113/streams/newstream/1 -H \"Accept: application/vnd.eventstore.atom+json\" HTTP/1.1 200 OK Access-Control-Allow-Methods: GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-Forwarded-Host, X-Forwarded-Prefix, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTos Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position, ES-CurrentVersion Cache-Control: max-age=31536000, public Vary: Accept Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 09 Nov 2017 13:35:43 GMT Content-Length: 640 Keep-Alive: timeout=15,max=100 { \"title\": \"1@newstream\", \"id\": \"<http://127.0.0.1:2113/streams/newstream/1\">, \"updated\": \"2017-11-09T13:10:16.111657Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"SomeEvent\", \"content\": { \"eventStreamId\": \"newstream\", \"eventNumber\": 1, \"eventType\": \"SomeEvent\", \"eventId\": \"c322e299-cb73-4b47-97c5-5054f920746e\", \"data\": { \"something\": \"has data\" }, \"metadata\": \"\" }, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1\", \"relation\": \"alternate\" } ] } Feed Paging The next step in understanding how to read a stream is the first / last / previous / next links within a stream. Event Store supplies these links so you can read through a stream, and they follow the pattern defined in RFC 5005 . In the example above the server returned the following links as part of its result: \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/metadata\", \"relation\": \"metadata\" } ] This shows that there is not a next URL (all the information is in this request), and that the URL requested is the first link. When dealing with these URL there are two ways of reading the data in the stream. You can either go to the last link and then move backward following previous or you can go to the first link and follow the next links, the final item will not have a next link. If you want to follow a live stream then you would keep following the previous links. When you reach the end of a stream you will receive an empty document with no entries or previous link. You then continue polling this URI (in the future a document will appear here). You can see this by trying the previous link from the above feed. Request Response curl -i http://127.0.0.1:2113/streams/newstream/1/forward/20 -H \"Accept:application/vnd.eventstore.atom+json\" HTTP/1.1 200 OK Access-Control-Allow-Methods: GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"0;248368668\" Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 14:04:47 GMT Content-Length: 795 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'newstream'\", \"id\": \"<http://127.0.0.1:2113/streams/newstream\">, \"updated\": \"0001-01-01T00:00:00Z\", \"streamId\": \"newstream\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": false, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0/forward/20\", \"relation\": \"last\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0/backward/20\", \"relation\": \"next\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/metadata\", \"relation\": \"metadata\" } ], \"entries\": \\[] } When parsing an atom subscription the IDs of events will always stay the same. This is important for figuring out if you are referring to the same event. <!-- TODO: Make this example more generic and covering different installations --> Let’s now try an example with more than a single page. You can use the testclient that comes with Event Store and use the VERIFY command to create fake banking data. After running this command you should find many streams such as http://127.0.0.1:2113/streams/account-28 in the system. Opening the link http://127.0.0.1:2113/streams/account-28 will return: Request Response curl -i http://127.0.0.1:2113/streams/account-28 -H \"Accept:application/vnd.eventstore.atom+json\" HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"180;248368668\" Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 16:08:06 GMT Content-Length: 11095 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'account-28'\", \"id\": \"<http://127.0.0.1:2113/streams/account-28\">, \"updated\": \"2015-03-13T16:06:18.47214Z\", \"streamId\": \"account-28\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": true, \"selfUrl\": \"<http://127.0.0.1:2113/streams/account-28\">, \"eTag\": \"180;248368668\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/account-28\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/account-28/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/account-28/0/forward/20\", \"relation\": \"last\" }, { \"uri\": \"http://127.0.0.1:2113/streams/account-28/160/backward/20\", \"relation\": \"next\" }, { \"uri\": \"http://127.0.0.1:2113/streams/account-28/181/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/account-28/metadata\", \"relation\": \"metadata\" } ], \"entries\": <SNIP> Using the links in this response you can traverse through all the events in the stream by going to the last URL and walking \"previous\" or by walking \"next\" from the “first” link. If you go to the “last” link you will receive: Request Response curl -i http://127.0.0.1:2113/streams/account-28/0/forward/20 -H \"Accept:application/vnd.eventstore.atom+json\" HTTP/1.1 200 OK Access-Control-Allow-Methods: GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=31536000, public Vary: Accept Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 16:16:05 GMT Content-Length: 10673 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'account-28'\", \"id\": \"<http://127.0.0.1:2113/streams/account-28\">, \"updated\": \"2015-03-13T16:05:24.262029Z\", \"streamId\": \"account-28\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": false, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/account-28\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/account-28/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/account-28/20/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/account-28/metadata\", \"relation\": \"metadata\" } ], \"entries\": <SNIP> You then follow its \"previous\" link until you got back to the head of the document. This is the general way of reading back a stream. Once at the end you can continue reading events as they happen by polling the previous link and you will get events in near real time as they happen. Note All links except the head link are fully cachable as seen in the HTTP header Cache-Control: max-age=31536000, public . This is important when discussing intermediaries and performance as you commonly replay a stream from storage. Note You should never bookmark links aside from the head of the stream resource. You should always follow links. We may in the future change how internal links work. If you bookmark link other than the head you will break in these scenarios. Reading All Events There is a special paged feed for all events that named $all . You can use the same paged form of reading described above to read all events for the entire node by pointing the stream at /streams/$all . As it's a stream like any other, you can perform all other operations with it except posting to it. Note To access the $all stream, you must provide user details, find more information on the security page. Request Response curl -i http://127.0.0.1:2113/streams/%24all -H \"Accept:application/vnd.eventstore.atom+json\" -u admin:changeit HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"25159393;248368668\" Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 16:19:09 GMT Content-Length: 12157 Keep-Alive: timeout=15,max=100 { \"title\": \"All events\", \"id\": \"<http://127.0.0.1:2113/streams/%24all\">, \"updated\": \"2015-03-13T16:19:06.548415Z\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": false, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/%24all\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24all/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24all/00000000000000000000000000000000/forward/20\", \"relation\": \"last\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24all/00000000017BC0D000000000017BC0D0/backward/20\", \"relation\": \"next\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24all/0000000001801EBF0000000001801EBF/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24all/metadata\", \"relation\": \"metadata\" } ], \"entries\": <SNIP> Conditional Gets The head link supports conditional GET s with the use of ETAGS , a well known HTTP construct described here . You can include the ETAG of your last request and issue a conditional GET to the server. If nothing has changed it will not return the full feed. For example: Request Response curl -i http://127.0.01:2113/streams/account-28l -H \"Accept:application/vnd.eventstore.atom+json\" HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"180;248368668\" Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 16:23:52 GMT Content-Length: 11095 Keep-Alive: timeout=15,max=100 … The server responded in the headers that the ETag for this content is ETag: \"180;248368668\" . You can use this in your next request when polling the stream for changes by putting it in the header If-None-Match . This tells the server to check if the response will be the one you already know. Request Response curl -i http://127.0.0.1:2113/streams/account-28 -H \"Accept:application/vnd.eventstore.atom+json\" -H \"If-None-Match:180;248368668\" HTTP/1.1 304 Not Modified Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 16:27:53 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 When you use the conditional GET the server will return a '304 not modified' response. If the tags have changed, the server will return a '200 OK' response. You can use this method to optimize your application by not sending large streams if there are not changes. Note Etags are created using the version of the stream and the media type you are reading the stream in. You can NOT take an etag from a stream in one media type and use it with another media type. Embedding Data into Stream So far in this guide, the feeds returned have contained links that refer to the actual event data. This is normally a preferable mechanism for a few reasons. They can be in a different media type than the feed and can you can negotiate them separately from the feed itself (e.g. the feed in JSON, the event in XML). You can cache the event data separately from the feed and you can point it to different feeds (if you use a linkTo() in projections this is what happens in your atom feeds). If you are using JSON, you can embed the events into the atom feeds events. This can help cut down on the number of requests in some situations but the messages will be larger. Though these are mostly used by the StreamUI component in the Web API there are ways of embedding events and/or further metadata into your stream controlled by the embed= parameter. Rich The rich embed mode will return more properties about the event ( eventtype , streamid , position , etc) as you can see in the following request. Request Response curl -i -H \"Accept:application/vnd.eventstore.atom+json\" \"http://127.0.0.1:2113/streams/newstream?embed=rich\" HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"0;248368668\" Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 16:30:57 GMT Content-Length: 1570 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'newstream'\", \"id\": \"<http://127.0.0.1:2113/streams/newstream\">, \"updated\": \"2015-03-13T12:13:42.492473Z\", \"streamId\": \"newstream\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": true, \"selfUrl\": \"<http://127.0.0.1:2113/streams/newstream\">, \"eTag\": \"0;248368668\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/metadata\", \"relation\": \"metadata\" } ], \"entries\": \\[ { \"eventId\": \"fbf4a1a1-b4a3-4dfe-a01f-ec52c34e16e4\", \"eventType\": \"event-type\", \"eventNumber\": 0, \"streamId\": \"newstream\", \"isJson\": true, \"isMetaData\": false, \"isLinkMetaData\": false, \"positionEventNumber\": 0, \"positionStreamId\": \"newstream\", \"title\": \"0@newstream\", \"id\": \"<http://127.0.0.1:2113/streams/newstream/0\">, \"updated\": \"2015-03-13T12:13:42.492473Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"event-type\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"alternate\" } ] } ] } Body The body embed mode will return the JSON/XML body of the events into the feed as well, depending on the type of the feed. You can see this in the request below: Request Response curl -i -H \"Accept:application/vnd.eventstore.atom+json\" \"http://127.0.0.1:2113/streams/newstream?embed=body\" HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"0;248368668\" Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 16:32:06 GMT Content-Length: 1608 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'newstream'\", \"id\": \"<http://127.0.0.1:2113/streams/newstream\">, \"updated\": \"2015-03-13T12:13:42.492473Z\", \"streamId\": \"newstream\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": true, \"selfUrl\": \"<http://127.0.0.1:2113/streams/newstream\">, \"eTag\": \"0;248368668\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/metadata\", \"relation\": \"metadata\" } ], \"entries\": \\[ { \"eventId\": \"fbf4a1a1-b4a3-4dfe-a01f-ec52c34e16e4\", \"eventType\": \"event-type\", \"eventNumber\": 0, \"data\": \"{\\\\n \"a\": \"1\"\\\\n}\", \"streamId\": \"newstream\", \"isJson\": true, \"isMetaData\": false, \"isLinkMetaData\": false, \"positionEventNumber\": 0, \"positionStreamId\": \"newstream\", \"title\": \"0@newstream\", \"id\": \"<http://127.0.0.1:2113/streams/newstream/0\">, \"updated\": \"2015-03-13T12:13:42.492473Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"event-type\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"alternate\" } ] } ] } If you use XML format then no additional data is embedded, as embedding is only supported with JSON. Request Response curl -i -H \"Accept:application/atom+xml\" \"http://127.0.0.1:2113/streams/newstream?embed=body\" HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"0;-1296467268\" Content-Type: application/atom+xml; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 13 Mar 2015 16:32:56 GMT Content-Length: 929 Keep-Alive: timeout=15,max=100 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <feed xmlns=\"http://www.w3.org/2005/Atom\"> <title>Event stream 'newstream'</title> <id>http://127.0.0.1:2113/streams/newstream</id> <updated>2013-06-29T15:12:53.570125Z</updated> <author> <name>EventStore</name> </author> <link href=\"http://127.0.0.1:2113/streams/newstream\" rel=\"self\" /> <link href=\"http://127.0.0.1:2113/streams/newstream/head/backward/20\" rel=\"first\" /> <link href=\"http://127.0.0.1:2113/streams/newstream/0/forward/20\" rel=\"last\" /> <link href=\"http://127.0.0.1:2113/streams/newstream/1/forward/20\" rel=\"previous\" /> <link href=\"http://127.0.0.1:2113/streams/newstream/metadata\" rel=\"metadata\" /> <entry> <title>0@newstream</title> <id>http://127.0.0.1:2113/streams/newstream/0</id> <updated>2013-06-29T15:12:53.570125Z</updated> <author> <name>EventStore</name> </author> <summary>event-type</summary> <link href=\"http://127.0.0.1:2113/streams/newstream/0\" rel=\"edit\" /> <link href=\"http://127.0.0.1:2113/streams/newstream/0\" rel=\"alternate\" /> </entry> </feed> There are two other modes that are variants of body . PrettyBody which will try to reformat the JSON to make it \"pretty to read\", and TryHarder that will work harder to try to parse and reformat JSON from an event to return it in the feed. These do not include further information, they are focused on how the feed looks."
  },
  "http-api/read-stream-metadata/read-stream-metadata.html": {
    "href": "http-api/read-stream-metadata/read-stream-metadata.html",
    "title": "Reading Stream Metadata | Event Store",
    "keywords": "<!-- TODO: Break up to write / read? And maybe transclude--> Every stream in Event Store has metadata associated with it. Internally, the metadata includes information such as the ACL of the stream and the maximum count and age for the events in the stream. Client code can also add information into stream metadata for use with projections or the client API. A common use case for information you may want to store in metadata is information associated with an event that is not part of the event. Examples of this might be: Which user wrote the event. Which application server were they talking to. What IP address did the request come from? Stream metadata is stored internally as JSON, and you can access it over the HTTP APIs. Reading Stream Metadata To read the metadata, issue a GET request to the attached metadata resource, which is of the form: http://{eventstore}/streams/{stream-name}/metadata You should not access metadata by constructing this URL yourself, as the right to change the resource address is reserved. Instead, you should follow the link for from the stream itself, which will enable your client to tolerate future changes to the addressing structure without breaking. Request Response curl -i http://127.0.0.1:2113/streams/$users --user admin:changeit HTTP/1.1 200 OK Cache-Control: max-age=0, no-cache, must-revalidate Content-Length: 1267 Content-Type: application/vnd.eventstore.atom+json; charset: utf-8 ETag: \"0;-2060438500\" Vary: Accept Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Date: Sun, 16 Jun 2013 15:08:49 GMT { \"title\": \"Event stream '$users'\", \"id\": \"<http://127.0.0.1:2113/streams/%24users\">, \"updated\": \"2013-06-16T15:08:47.5245306Z\", \"author\": { \"name\": \"EventStore\" }, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/%24users\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24users/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24users/0/forward/20\", \"relation\": \"last\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24users/1/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24users/metadata\", \"relation\": \"metadata\" } ], ... Once you have the URI of the metadata stream, a GET request will retrieve the metadata: Request Response curl -i http://127.0.0.1:2113/streams/$users/metadata --user admin:changeit HTTP/1.1 200 OK Cache-Control: max-age=31536000, public Content-Length: 652 Content-Type: application/vnd.eventstore.atom+json; charset: utf-8 Vary: Accept Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: GET, POST, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Date: Sun, 16 Jun 2013 13:18:29 GMT { \"title\": \"0@$$$users\", \"id\": \"<http://127.0.0.1:2113/streams/%24%24%24users/0\">, \"updated\": \"2013-06-16T12:25:13.8428624Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"$metadata\", \"content\": { \"eventStreamId\": \"$$$users\", \"eventNumber\": 0, \"eventType\": \"$metadata\", \"data\": { \"readRole\": \"$all\", \"metaReadRole\": \"$all\" }, \"metadata\": \"\" }, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/%24%24%24users/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24%24%24users/0\", \"relation\": \"alternate\" } ] } Reading metadata may require that you pass credentials if you have security enabled, as in the examples above. If you do not pass credentials and they are required you will receive a '401 Unauthorized' response. Request Response curl -i http://127.0.0.1:2113/streams/$users/metadata HTTP/1.1 401 Unauthorized Content-Length: 0 Content-Type: text/plain; charset: utf-8 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: GET, POST, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * WWW-Authenticate: Basic realm=\"ES\" Date: Sun, 16 Jun 2013 13:20:22 GMT"
  },
  "projections/user-defined-projections/index.html": {
    "href": "projections/user-defined-projections/index.html",
    "title": "User Defined Projections | Event Store",
    "keywords": "User Defined Projections User defined projections are written in JavaScript (ECMASCRIPT 6). Example projection: <!-- TODO: Which does what? Counts number myeventtype in account-1 stream, and the transformBy does something… Map state to something else, setting it to 10 --> options({ //option resultStreamName: \"my_demo_projection_result\", $includeLinks: false, reorderEvents: false, processingLag: 0 }) fromStream('account-1') //selector .when({ //filter $init:function(){ return { count: 0 } }, myEventType: function(state, evnt){ s.count += 1; } }) .transformBy(function(state){ //transformation state.count = 10; }) .outputState() //transformation Projections API Options Name Description Notes resultStreamName Overrides the default resulting stream name for the outputState() transformation, which is $projections-{projection-name}-result. $includeLinks Configures the projection to include/exclude link to events. Default: false processingLag When reorderEvents is turned on, this value is used to compare the total milliseconds between the first and last events in the buffer and if the value is equal or greater, the events in the buffer will be processed. The buffer is an ordered list of events. Default: 500ms Only valid for fromStreams() selector reorderEvents Process events by storing a buffer of events ordered by their prepare position Default: false Only valid for fromStreams() selector Selectors Selector Description Notes fromAll() Selects events from the $all stream. Provides partitionBy when foreachStream outputState fromCategory({category}) Selects events from the $ce-{category} stream. Provides partitionBy when foreachStream outputState fromStream({streamId}) Selects events from the {streamId} stream. Provides partitionBy when outputState fromStreams([]streams) Selects events from the streams supplied. Provides partitionBy when outputState fromStreamsMatching(function filter) Selects events from the $all stream that returns true for the given filter. Provides when Filters/Transformations Filter/Partition Description Notes when(handlers) Allows only the given events of a particular to pass through. Provides $defines_state_transform transformBy filterBy outputTo outputState foreachStream() Partitions the state for each of the streams provided. Provides when outputState() If the projection maintains state, setting this option will produce a stream called $projections-{projection-name}-result with the state as the event body. Provides transformBy filterBy outputTo partitionBy(function(event)) Partitions a projection by the partition returned from the handler. Provides when transformBy(function(state)) Provides the ability to transform the state of a projection by the provided handler. Provides transformBy filterBy outputState outputTo filterBy(function(state)) Causes projection results to be null for any state that returns a falsey value from the given predicate. Provides transformBy filterBy outputState outputTo Handlers Each handler is provided with the current state of the projection as well as the event that triggered the handler. The event provided through the handler contains the following properties. isJson:true/false data:{} body:{} bodyRaw: string sequenceNumber: integer metadataRaw: {} linkMetadataRaw: string partition: string Handler Description Notes {event-type} When using fromAll() and 2 or more event type handlers are specified and the $by_event_type projection is enabled and running, the projection will start as a fromStreams('$et-event-type-foo', $et-event-type-bar ) until the projection has caught up and then will move over to reading from the transaction log (i.e. from $all). $init Provide the initialization for a projection. Commonly used to setup the initial state for a projection. $initShared Provide the initialization for a projection where the projection is possibly partitioned. $any Event type pattern match that will match any event type. Commonly used when the user is interested in any event type from the selector. $deleted Will be called upon the deletion of a stream. Can only be used with foreachStream Functions Name Description Notes emit(streamId, eventName, eventBody, metadata) Writes an event to the designated stream linkTo(streamId, event, metadata) Writes a link to event to the designated stream"
  },
  "projections/system-projections/index.html": {
    "href": "projections/system-projections/index.html",
    "title": "System Projections | Event Store",
    "keywords": "System Projections Event Store ships with 4 built in projections. By Category ( $by_category ) By Event Type ( $by_event_type ) Stream by Category ( $stream_by_category ) Streams ( $streams ) Note When you start Event Store from a fresh database, these projections are present but disabled and querying their statuses you will see that they are Stopped . You can enable a projection by issuing an HTTP request to http://{event-store-ip}:{ext-http-port}/projection/{projection-name}/command/enable . The status of the projection will switch from Stopped to Running . By Category This projection links existing events from streams to a new stream with a $ce- prefix (a category) by splitting a stream id by a configurable separator. <!-- TODO: This is a little confusing, what is it? --> `$by_category\\ first - By default the $by_category projection will link existing events from a stream id with a name such as account-1 to a stream called $ce-account . You can configure the separator as well as where to split the stream id . You can edit the projection and provide your own values if the defaults don't fit your particular scenario. The first parameter specifies how the separator will be used and the possible values for that parameter is first or last. The separator can be any character. For example: If the body of the projection is first and - , for a stream id of account-1 , the resulting stream name from the projection will be $ce-account . If the body of the projection is last and - , for a stream id of shopping-cart-1 , the resulting stream name from the projection will be $ce-shopping-cart . Note This particular projection enables the use of the byCategory selector for user defined projections which we will discuss in the User defined projections section. By Event Type This projection links existing events from streams to a new stream with a stream id in the format $et-{event-type} . $by_event_type You cannot configure this projection. Stream By Category This projection links existing events from streams to a new stream with a $category prefix by splitting a stream id by a configurable separator. <!-- TODO: Again, what is this? --> $stream_by_category first - By default the $stream_by_category projection will link existing events from a stream id with a name such as account-1 to a stream called $category-account . You can configure the separator as well as where to split the stream id . You can edit the projection and provide your own values if the defaults don't fit your particular scenario. The first parameter specifies how the separator will be used and the possible values for that parameter is first or last. The separator can be any character. For example: If the body of the projection is first and -, for a stream id of account-1 , the resulting stream name from the projection will be $category-account . If the body of the projection is last and -, for a stream id of shopping-cart-1 , the resulting stream name from the projection will be $category-shopping-cart . Streams This projection links existing events from streams to a stream named $streams $streams You cannot configure this projection."
  },
  "projections/projections-config/index.html": {
    "href": "projections/projections-config/index.html",
    "title": "Config | Event Store",
    "keywords": "Config Event Store version 4.0.2, included a new set of options for fine-tuning projections. By changing these settings, you can lessen the amount of pressure projections put on an Event Store node or improve projection performance. You can change these settings on a case-by-case basis, and monitor potential improvements. Emit options These options control how projections write events. In busy systems, projections can put a lot of extra pressure on the master node. This is especially true for Event Store servers that also have persistent subscriptions running, which only the master node can process. If you are seeing a lot of commit timeouts and slow writes from your projections and other clients, then these are the settings that you should start with. Emit Enabled This setting determines whether a projection can emit events and any projection that calls emit() or linkTo() requires it. If this option is not set and a projection attempts to emit events, you will see an error message like the following: 'emit' is not allowed by the projection/configuration/mode This setting is disabled by default, and is usually set when you create the projection and if you need the projection to emit events. Track Emitted Streams This setting enables tracking of a projection's emitted streams. It will only have an affect if the projection has EmitEnabled enabled. Tracking emitted streams enables you to delete a projection and all the streams that it has created. You should only use it if you intend to delete a projection and create new ones that project to the same stream. When enabled, an event written records the stream name of each event emitted by the projection. This means that write amplification is a possibility, as each event that the projection emits writes a separate event. As such, this option is not recommended for projections that emit a lot of events, and you should enable it should only in cases where necessary. The stream that tracks emitted stream IDs is named as follows : $projections-{projection_name}-emittedstreams This setting is disabled by default. Note Between Event Store versions 3.8.0 and 4.0.2, this option was enabled by default when a projection was created through the UI. If you have any projections created during this time frame, it's worth checking whether this option is enabled. Max Allowed Writes In Flight This sets the maximum number of writes to allow for a projection. Because a projection can write to multiple different streams, it is possible for the projection to send off multiple writes at the same time. This option sets the number of concurrent writes that a projection can perform. By default, projections will try to perform writes as quickly as they come through. This can add a lot of pressure to a node, especially for projections that emit to many different streams. If you see your projections causing frequent commit timeouts or slow reads, you can try lowering this value to see if there is any improvement. Note Lower values may cause the projection to slow down as the number of writes are throttled, but the trade off for this is cleaner logs and fewer commit timeouts. By default, this is unbounded, allowing a projection to write as fast as it can. Max Write Batch Length This determines the maximum number of events the projection can write in a batch at a time. The default for this option is 500. Checkpoint options Checkpoints store how far along a projection is in the streams it is processing from. There is a bit of performance overhead with writing a checkpoint, as it does more than simply write an event. So writing them too often can slow projections down. However, we recommend you try other methods of improving projections before changing these values, as checkpoints are an important part of running projections. Checkpoint After Ms This prevents a new checkpoint from being written within a certain time frame from the previous one. The aim of this option is to keep a projection from writing too many checkpoints too quickly - something that can happen in a very busy system. By default, this is set to 0 seconds, which means there is no limit to how quickly checkpoints can be written. Checkpoint Handled Threshold This controls the number of events that a projection can handle before attempting to write a checkpoint. An event is only considered handled if it actually passed through the projection's filter. As such, if the projection is set to checkpoint every 4,000 events, but it only reads from the foo stream, the projection will only checkpoint every 4,000 foo events. The default for this option is 4,000 events. Checkpoint Unhandled Bytes Threshold This specifies the number of bytes a projection can process before attempting to write a checkpoint. Unhandled bytes are the events that are not processed by the projection itself. For example, if the projection only reads from the foo stream, but only writes from the bar stream comes through, a checkpoint will be written after this number of bytes have been processed. This is to prevent the projection from having to read through a potentially large number of unrelated events again because none of them actually passed its filter. This option defaults to 10mb. Processing options Pending Events Threshold This determines the number of events that can be pending before the projection is paused. Pausing the projection stops the projection from reading, allowing it to finish with the current events that are waiting to be processed. Once the pending queue has drained to half the threshold, the projection will start reading again. The default is 5000. Setting these config options These options are accesible through the admin UI from the Config tab when editing a projection. Note The config of a projection can only be changed when the projection has been stopped."
  },
  "getting-started/projections/index.html": {
    "href": "getting-started/projections/index.html",
    "title": "Step 3 - Projections | Event Store",
    "keywords": "Step 3 - Projections This getting started guide shows you how to get started with Event Store using the Atom publishing protocol as the primary interface. This second step covers projections, used to give you continuous queries of your data. When running a projection you can choose whether the query should run and give you all results present or whether the query should continue running into the future finding new results as they happen and updating its result set. Note The described is for development and evaluation of Event Store. It does not describe a production setup. Setting up Projections You enable projections with the command line argument --run-projections . For example, the command below enables all projection modes (system and user defined): Tip Read this guide for all the possible parameter values. Windows Linux Docker EventStore.ClusterNode.exe --run-projections=all --start-standard-projections=true To disable them again, run: EventStore.ClusterNode.exe --run-projections=none Add EVENTSTORE_RUN_PROJECTIONS=All and EVENTSTORE_START_STANDARD_PROJECTIONS=true to your environment variables, or a configuration file and start Event Store: sudo systemctl start eventstore To disable them again, change the values to EVENTSTORE_RUN_PROJECTIONS=none . The Event Store Docker image has projections enabled by default, but you need to enable standard projections: docker run --name eventstore-node -it -p 2113:2113 -p 1113:1113 -e EVENTSTORE_RUN_PROJECTIONS=All -e EVENTSTORE_START_STANDARD_PROJECTIONS=true eventstore/eventstore To disable them again: docker run --name eventstore-node -it -p 2113:2113 -p 1113:1113 -e EVENTSTORE_RUN_PROJECTIONS=None eventstore/eventstore You will then see new tabs enabled in the Web admin UI with the 4 system projections in a Stopped state: You can also query the state of all projections using the HTTP API. Request Response curl -i http://localhost:2113/projections/any -H \"accept:application/json\" | grep -E 'effectiveName|status' The response is a list of all known projections and useful information about them. \"effectiveName\": \"$streams\" \"status\": \"Stopped\" \"statusUrl\": \"http://localhost:2113/projection/$streams\" \"effectiveName\": \"$stream_by_category\" \"status\": \"Stopped\" \"statusUrl\": \"http://localhost:2113/projection/$stream_by_category\" \"effectiveName\": \"$by_category\" \"status\": \"Stopped\" \"statusUrl\": \"http://localhost:2113/projection/$by_category\" \"effectiveName\": \"$by_event_type\" \"status\": \"Stopped\" \"statusUrl\": \"http://localhost:2113/projection/$by_event_type\" Add Sample Data Download the following files that contain sample data used throughout this step of the getting started guide. shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1164.json shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1165.json shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1166.json shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1167.json Add the sample data to streams: curl -i -d \"@shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1164.json\" \"http://127.0.0.1:2113/streams/shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1164\" -H \"Content-Type:application/vnd.eventstore.events+json\" curl -i -d \"@shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1165.json\" \"http://127.0.0.1:2113/streams/shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1165\" -H \"Content-Type:application/vnd.eventstore.events+json\" curl -i -d \"@shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1166.json\" \"http://127.0.0.1:2113/streams/shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1166\" -H \"Content-Type:application/vnd.eventstore.events+json\" curl -i -d \"@shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1167.json\" \"http://127.0.0.1:2113/streams/shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1167\" -H \"Content-Type:application/vnd.eventstore.events+json\" Writing your first projection Tip Read this guide to find out more about the user defined projection's API. The projection counts the number of 'XBox One S's that customers added to their shopping carts. A Projection starts with a selector, in this case fromAll() . Another possibility is fromCategory({category} which this guide discusses later, but for now, fromAll should do. The second part of a projection is a set of filters. There is a special filter called $init that sets up initial state. You want to start a counter from 0 and each time an ItemAdded event is observed for an 'Xbox One S', increment the counter. Here is the projection so far: fromAll() .when({ $init: function(){ return { count: 0 } }, ItemAdded: function(s,e){ if(e.body.Description.indexOf(\"Xbox One S\") >= 0){ s.count += 1; } } }) Create the projection by calling the projection API and providing it with the definition of the projection. Here you decide how to run the projection, declaring that you want the projection to start from the beginning and keep running. curl -i --data-binary \"@xbox-one-s-counter.json\" http://localhost:2113/projections/continuous?name=xbox-one-s-counter%26type=js%26enabled=true%26emit=true%26trackemittedstreams=true -u admin:changeit Tip You can find more information about how to interact with projections in the API section . You should have received a '201 Created response' which means that Event Store created the projection successfully. You can confirm that the projection is running by issuing a status request. Request Response curl -i http://localhost:2113/projection/xbox-one-s-counter | grep status \"status\": \"Running\", \"statusUrl\": \"http://localhost:2113/projection/xbox-one-s-counter\", Querying for the state of the projection Now the projection is running, you can query the state of the projection. As this projection has a single state, query it with the following request: Request Response curl -i http://localhost:2113/projection/xbox-one-s-counter/state Which returns the state (JSON by default). { \"count\": 3 } Writing to Streams from Projections The above gives you the correct result, but requires you to poll for the state of a projection. What if you wanted Event Store to notify you about state updates via subscriptions? Output State You need to update the projection to output the state to a stream by calling the outputState() method on the projection which by default produces a $projections-{projection-name}-result stream. Download the following file that contains the update to the projection: getting-started-projections-output-state To update the projection, issue the following request: curl -i -X PUT --data-binary @\"xbox-one-s-counter-outputState.json\" http://localhost:2113/projection/xbox-one-s-counter/query?emit=yes -u admin:changeit Then reset the projection you created above: Request Response curl -i -X POST \"http://localhost:2113/projection/xbox-one-s-counter/command/reset\" -H \"accept:application/json\" -H \"Content-Length:0\" -u admin:changeit { \"msgTypeId\": 293, \"name\": \"xbox-one-s-counter\" } You can now read the events in the result stream by issuing a read request. Request Response curl -i \"http://localhost:2113/streams/\\$projections-xbox-one-s-counter-result?embed=body\" -H \"accept:application/json\" -u admin:changeit | grep data \"data\": \"{\\\"count\\\":3}\", \"data\": \"{\\\"count\\\":2}\", \"data\": \"{\\\"count\\\":1}\", Configure Projection Properties You can configure properties of the projection by updating values of the options object. For example, to change the name of the results stream: <!-- Can not find reference ~/code-snippets/getting-started/update-projections-options.json --> Then send the update to the projection: curl -i -X PUT -d \"@update-projection-options.json\" http://localhost:2113/projection/xbox-one-s-counter/query?emit=yes -u admin:changeit Now you can read the result as above, but use the new stream name: curl -i \"http://localhost:2113/streams/xboxes?embed=body\" -H \"accept:application/json\" -u admin:changeit | grep data Tip You can find all the options available in the user defined projections guide . The Number of items per shopping cart The example so far relied on a global state for the projection, but what if you wanted a count of the number of items in the shopping cart per shopping cart. Event Store has a built in $by_category projection that lets you select events from a particular list of streams. Enable this projection with the following command. curl -i -d{} http://localhost:2113/projection/%24by_category/command/enable -u admin:changeit The projection links events from existing streams to new streams by splitting the stream name by a separator. You can configure the projection to specify the position of where to split the stream id and provide your own separator. By default the category splits the stream id by a dash. The category is the first string. Stream Name Category shoppingCart-54 shoppingCart shoppingCart-v1-54 shoppingCart shoppingCart No category as there is no separator You want to define a projection that produces a count per stream for a category, but the state needs to be per stream. To do so, use $by_category and it's fromCategory API method. Download the following file that contains the projection: getting-started-projections-count-per-stream Create the projection with the following HTTP request: curl -i --data-binary \"@shopping-cart-counter.json\" http://localhost:2113/projections/continuous?name=shopping-cart-item-counter%26type=js%26enabled=true%26emit=true%26trackemittedstreams=true -u admin:changeit Querying for the state of the projection by partition Querying for the state of the projection is different due to the partitioning of the projection. You have to specify the partition and the name of the stream. Request Response curl -i http://localhost:2113/projection/shopping-cart-item-counter/state?partition=shoppingCart-b989fe21-9469-4017-8d71-9820b8dd1164 { \"count\": 2 } Next Step In this third part of our getting started guide you learned about projections. The next, and final part covers which API or SDK to use, and when. Step 4 - Which API or SDK"
  },
  "http-api/update-stream-metadata/update-stream-metadata.html": {
    "href": "http-api/update-stream-metadata/update-stream-metadata.html",
    "title": "Writing Metadata | Event Store",
    "keywords": "Writing Metadata To update the metadata for a stream, issue a POST request to the metadata resource. This will replace the current metadata with the information posted. Inside a file named metadata.txt : [ { \"eventId\": \"7c314750-05e1-439f-b2eb-f5b0e019be72\", \"eventType\": \"$user-updated\", \"data\": { \"readRole\": \"$all\", \"metaReadRole\": \"$all\" } } ] You can also add user-specified metadata here. Some examples of good uses of user-specified metadata: which adapter is responsible for populating a stream. which projection caused a stream to be created. a correlation ID of some business process. This information is then posted to the stream: Request Response curl -i -d @metadata.txt http://127.0.0.1:2113/streams/$users/metadata --user admin:changeit -H \"Content-Type: application/vnd.eventstore.events+json\" HTTP/1.1 201 Created Content-Length: 0 Content-Type: text/plain; charset: utf-8 Location: http://127.0.0.1:2113/streams/%24%24%24users/1 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: GET, POST, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Date: Sun, 16 Jun 2013 14:50:21 GMT If the specified user does not have permissions to write to the stream metadata, you will receive a '401 Unauthorized' response: Request Response curl -i -d @metadata.txt http://127.0.0.1:2113/streams/$users/metadata --user invaliduser:invalidpass -H \"Content-Type: application/vnd.eventstore.events+json\" HTTP/1.1 401 Unauthorized Content-Length: 0 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * WWW-Authenticate: Basic realm=\"ES\" Date: Sun, 16 Jun 2013 14:51:37 GMT"
  },
  "http-api/optional-http-headers/index.html": {
    "href": "http-api/optional-http-headers/index.html",
    "title": "Optional HTTP Headers | Event Store",
    "keywords": "Optional HTTP Headers <!-- TODO: Can Swagger replace this? And sub files --> Event Store supports the use of custom HTTP headers for requests. The headers were previously in the form X-ES-ExpectedVersion but have been changed to ES-ExpectedVersion in compliance with RFC-6648 . The headers supported are: Header Description ES-ExpectedVersion The expected version of the stream (allows optimistic concurrency) ES-ResolveLinkTo Whether or not to resolve linkTos in stream ES-RequiresMaster Whether this operation needs to be run on the master node ES-TrustedAuth Allows a trusted intermediary to handle authentication ES-LongPoll Instructs the server to do a long poll operation on a stream read ES-HardDelete Instructs the server to hard delete the stream when deleting as opposed to the default soft delete ES-EventType Instructs the server the event type associated to a posted body ES-EventId Instructs the server the event id associated to a posted body"
  },
  "http-api/optimistic-concurrency-and-idempotence/index.html": {
    "href": "http-api/optimistic-concurrency-and-idempotence/index.html",
    "title": "Optimistic Concurrency & Idempotence | Event Store",
    "keywords": "Optimistic Concurrency & Idempotence Idempotency All operations on the HTTP interface are idempotent (unless the expected version is ignored). It is the responsibility of the client to retry operations under failure conditions, ensuring that the event IDs of the events posted are the same as the first attempt. Provided the client maintains this Event Store will treat all operations as idempotent. For example: Request Response curl -i -d @event.txt \"http://127.0.0.1:2113/streams/newstream\" HTTP/1.1 201 Created Access-Control-Allow-Origin: * Access-Control-Allow-Methods: POST, GET, PUT, DELETE Location: http://127.0.0.1:2113/streams/newstream444/1 Content-Type: application/json Server: Mono-HTTPAPI/1.0 Date: Thu, 06 Sep 2012 19:49:37 GMT Content-Length: 107 Keep-Alive: timeout=15,max=100 <!-- TODO: What's this? --> Request Response curl -i -d @event.txt \"http://127.0.0.1:2113/streams/newstream444\" HTTP/1.1 201 Created Access-Control-Allow-Origin: * Access-Control-Allow-Methods: POST, GET, PUT, DELETE Location: http://127.0.0.1:2113/streams/newstream444/1 Content-Type: application/json Server: Mono-HTTPAPI/1.0 Date: Thu, 06 Sep 2012 19:49:37 GMT Content-Length: 107 Keep-Alive: timeout=15,max=100 Assuming you were posting to a new stream you would get the event written once (and the stream created). The second event will return as the first but not write again. Note This allows the client rule of “if you get an unknown condition, retry” to work. For example: Request Response curl -i \"http://127.0.0.1:2113/streams/newstream444\" HTTP/1.1 200 OK Access-Control-Allow-Origin: * Access-Control-Allow-Methods: POST, GET, PUT, DELETE Content-Type: application/json Server: Mono-HTTPAPI/1.0 Date: Thu, 06 Sep 2012 19:50:30 GMT Content-Length: 2131 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'newstream444'\", \"id\": \"http://127.0.0.1:2113/streams/newstream444\", \"updated\": \"2012-09-06T16:39:44.695643Z\", \"author\": { \"name\": \"EventStore\" }, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream444\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream444\", \"relation\": \"first\" } ], \"entries\": [ { \"title\": \"newstream444 #1\", \"id\": \"http://127.0.0.1:2113/streams/newstream444/1\", \"updated\": \"2012-09-06T16:39:44.695643Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"Entry #1\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream444/1\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream444/event/1?format=text\", \"type\": \"text/plain\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream444/event/1?format=json\", \"relation\": \"alternate\", \"type\": \"application/json\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream444/event/1?format=xml\", \"relation\": \"alternate\", \"type\": \"text/xml\" } ] }, { \"title\": \"newstream444 #0\", \"id\": \"http://127.0.0.1:2113/streams/newstream444/0\", \"updated\": \"2012-09-06T16:39:44.695631Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"Entry #0\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream444/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream444/event/0?format=text\", \"type\": \"text/plain\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream444/event/0?format=json\", \"relation\": \"alternate\", \"type\": \"application/json\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream444/event/0?format=xml\", \"relation\": \"alternate\", \"type\": \"text/xml\" } ] } ] }"
  },
  "http-api/creating-a-stream/index.html": {
    "href": "http-api/creating-a-stream/index.html",
    "title": "Creating a Stream | Event Store",
    "keywords": "uid: eventstore.org/Event Store HTTP API/4.0.2/createStream Creating a Stream <!-- TODO: So document the API endpoint? --> Note As of Event Store 2.0.0, there is no explicit stream creation operation, as there is no longer a $StreamCreated as the first event in every stream. To set stream metadata (for example, an access control list or a maximum age or count of events), use the operations described in Stream Metadata , and then post to the stream using the operations described in Writing to a Stream) ."
  },
  "server/metadata-and-reserved-names/index.html": {
    "href": "server/metadata-and-reserved-names/index.html",
    "title": "Metadata and Reserved Names | Event Store",
    "keywords": "Metadata and Reserved Names Stream Metadata Every stream in Event Store has metadata associated with it. Event Store allows you to change some values in the metadata, and you can write your own additional data into stream metadata (such as how often to snapshot for your own code). Reserved Names All internal data used by Event Store will is prefixed with a $ character (for example $maxCount on a stream’s metadata), because of this you should not use names with a $ prefix as event names, metadata keys, or stream names, except where detailed below. The supported internal settings are: Name Description $maxAge Sets a sliding window based on dates. When data reaches a certain age it will disappear automatically from the stream and will be considered eligible for scavenging. This value is set as an integer representing the number of seconds. This value must be >=1. $maxCount Sets a sliding window based on the number of items in the stream. When data reaches a certain length it will disappear automatically from the stream and will be considered eligible for scavenging. This value is set as an integer representing the count of items. This value must be >= 1. $cacheControl This controls the cache of the head of a stream. Most URIs in a stream are infinitely cacheable but the head by default will not cache. It may be preferable in some situations to set a small amount of caching on the head to allow intermediaries to handle polls (say 10 seconds). The argument is an integer representing the seconds to cache. This value must be >= 1. Note If you set both $maxAge and $maxCount then events will become eligible for scavanging when either criteria is met. For example, if you set $maxAge to 10 and $maxCount to 50,000, events will be marked as eligible for scavenging after either 10 seconds, or 50,000 events, have passed. Deleted items will only actually be removed once the scavenge process is run. Security access control lists are also included in the $acl section of the stream metadata. Name Description $r The list of users with read permissions $w The list of users with write permissions $d The list of users with delete permissions $mw The list of users with write permissions to stream metadata $mr The list of users with read permissions to stream metadata You can find more details on access control lists can here . Event Metadata <!-- TODO: Is this duplicated or different?--> Every event in Event Store can have metadata associated with it. Event Store supports some values being set in the metadata and you can write your own additional data into event metadata if you wish (such as the source of the event). All names starting with $ are however a reserved space for internal details. The currently supported internal details are: $correlationId The application level correlation ID associated with this message. $causationId The application level causation ID associated with this message. Projections will honor both the correlationId and causationId patterns for any events it produces internally (linkTo/emit/etc)."
  },
  "server/deleting-streams-and-events/index.html": {
    "href": "server/deleting-streams-and-events/index.html",
    "title": "Deleting streams and events | Event Store",
    "keywords": "Deleting streams and events Meta data in Event Store defines whether an event is deleted or not. Stream metadata such as \"Truncate Before\", \"Max Age\" and \"Max Count\" is used to filter out events considered deleted. When reading a stream, the index will check the stream's metadata to determine whether any of its events have been deleted. $all bypasses the index, meaning that it does not check the metadata to determine whether events exist or not. As such, events that have been deleted will still be readable until a scavenge has removed them. There are a number of requirements for a scavenge to successfully remove events, for more information about this, please see the section on Scavenging . Warning The last event in a stream is always kept as a record of the last event number in the stream. Soft delete and Truncate before Truncate before ( $tb ) considers any event with an event number equal to or lower than its value to be deleted. For example, if you had the following events in a stream : 0@test-stream 1@test-stream 2@test-stream 3@test-stream If you set the truncate before value to 2, a read of the stream would result in only the last event being read : 3@test-stream Soft delete makes use of Truncate before. When a stream is deleted, its Truncate before is set to the streams current last event number. When a soft deleted stream is read, the read will return a StreamNotFound or 404 result. After deleting the stream, you are able to write to it again, continuing from where it left off. For example, if you soft deleted the above example stream, the truncate before would be set to 3 (the stream's current event number). If you were to write to the stream again, the next event would be written with event number 4. Only events from event number 4 onwards would be visible when reading this stream. Max count and Max age Max count ( $maxCount ) limits the number of events that can be read from a stream. If you try to read a stream that has a max count of 5, you will only be able to read the last 5 events regardless of how many events are actually in the stream. Max age ( $maxAge ) specifies the number of seconds an event can live for. This age is calculated at the time of the read, so if you read a stream with a Max Age of 3 minutes and one of the events in the stream has existed for 4 minutes at the time of the read, it will not be returned. Hard delete A hard delete writes a tombstone event to the stream, permanently deleting it. The stream cannot be recreated or written to again. Tombstone events are written with the event type $streamDeleted . When a hard deleted stream is read, the read will return a StreamDeleted or 410 result. The events in the deleted stream are liable to be removed in a scavenge, but the tombstone event will remain. Warning A hard delete of a stream is permanent. The stream cannot be written to or recreated. As such, you should generally prefer to soft delete streams unless you have a specific need to permanently delete the stream. Deleted events and projections If you are intending on using projections and deleting streams, there are some things to take into consideration: Due to the nature of $all, projections using fromAll will read any deleted events that have not been scavenged away. They will also receive any tombstone events from hard deletes. Projections that read from a specific stream will also receive that stream's metadata events. These can be filtered out by ignoring events with an event type $metadata ."
  },
  "index.html": {
    "href": "index.html",
    "title": "Welcome to Event Store | Event Store",
    "keywords": "Welcome to Event Store The team and community behind Event Store welcomes you and hopes we can help you find what you're looking for. New to Event Sourcing? If the concept of event sourcing, and why it's useful, is new to you, we recommend you read our guide to event sourcing first. New to Event Store? If you're familiar with event sourcing and want to know how to test Event Store, read the Introduction section. Already using Event Store? If you're already using Event Store, then welcome back and hopefully you can find what you need under the other sub-sections you can see in the left hand navigation menu, you can also try using the search box in the top navigation bar. If there's still something missing, then create an issue on GitHub , use the 'Improve this Doc' link on the top right on any documentation page to make a pull request, or contact docs@eventstore.org ."
  },
  "projections/index.html": {
    "href": "projections/index.html",
    "title": "Overview | Event Store",
    "keywords": "Overview What are projections? Projections is a subsystem in Event Store that provides you with the ability to write new or link existing events to streams in a reactive manner. Note Projections require the event body to be in JSON. When to use projections? Projections are good at solving one specific query type, a category known as 'temporal correlation queries'. This query type happens more often than you may think in business systems and few systems can execute these queries well. Business Cases You are looking for how many users on Twitter said \"happy\" within 5 minutes of the word \"foo coffee shop\" and within 2 minutes of saying \"london\". While a simple example, this is the type of query that projections can solve. Let's try a real business problem. As a medical research doctor you want to find people that were diagnosed with pancreatic cancer within the last year. During their treatment they should not have had any proxies for a heart condition such as aspirin given every morning. Within three weeks of their diagnosis they should have been put on treatment X. Within one month after starting the treatment they should have failed with a lab result that looks like L1. Within another six weeks they should have been been put on treatment Y and within four weeks failed that treatment with a lab result that looks like L2. This is a common type of query that exists in many industries and is the exact case that projections work well as a query model for. You can use projections in nearly all examples of near real-time complex event processing. There are a large number of problems that fit into this category from monitoring of temperature sensors in a data centre to reacting to changes in the stock market. It is important to remember the types of problems that projections are intended to solve. Many problems are not a good fit with the projections library and will be better served by hosting another read model that is populated by a catchup subscription Continuous Querying Projections also support the concept of continuous queries. When running a projection you can choose whether the query should run and give you all results present or whether the query should continue running into the future finding new results as they happen and updating its result set. As an example in the medical example above the doctor could leave the query running and be notified of any new patients that happened to meet the criteria that they was searching for. The output of all queries is a stream, this stream can then be listened to like any other stream. <!-- TODO: Is this supposed to be here? --> Types of Projections There are 2 types of projections, there are built in (system) projections which are written in C# and then there are javascript projections which you can create via the API or the admin UI."
  },
  "http-api/security/index.html": {
    "href": "http-api/security/index.html",
    "title": "Security | Event Store",
    "keywords": "Security Event Store supports security over HTTP. This guide is an introduction to how security is implemented. <!-- TODO: Where can you read more? --> Authentication Event Store supports authentication over basic authentication to internal users. You create these users with the RESTful API or the admin console. Once you have configured users, you can use standard basic authentication on requests. Note You can also use the trusted intermediary header to provide for externalized authentication that can allow you to integrate almost any authentication system with Event Store. Read more about it the trusted intermediary header here . As an example if you were to use the default admin user admin:changeit , you would include this in you request: Request Response curl -i 'http://127.0.0.1:2113/streams/$all' -u admin:changeit HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"28334346;248368668\" Content-Type: application/vnd.eventstore.atom+json; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 04 Jul 2013 00:13:59 GMT Content-Length: 12212 Keep-Alive: timeout=15,max=100 If you were to use the wrong user or no user where one is required, you would receive a 401 Unauthorized response. Request Response curl -i 'http://127.0.0.1:2113/streams/$all' -u admin:password HTTP/1.1 401 Unauthorized Access-Control-Allow-Methods: POST, DELETE, GET, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * WWW-Authenticate: Basic realm=\"ES\" Content-Type: text/plain; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 04 Jul 2013 00:20:34 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 As you pass the username and password as part of the request it's not recommended that you run Event Store over HTTP, you should enable SSL to encrypt the user information. You can find instructions on how to accomplish this in Windows and Linux . If you are running the clustered version you can also setup SSL for the replication protocol <!-- TODO: Does this need further explanation? --> . Access Control Lists Along with authentication, Event Store supports per stream configuration of Access Control Lists (ACL). To configure the ACL of a stream you should go to its head and look for the metadata relationship link to obtain the metadata for the stream. { \"uri\": \"http://127.0.0.1:2113/streams/%24all/metadata\", \"relation\": \"metadata\" } To set access control lists over HTTP you can post to the metadata stream as with setting any other metadata. You can also set Access Control Lists for a stream in the web UI. For more information on the structure of how Access Control Lists work please read Access Control Lists ."
  },
  "http-api/swagger/Subscriptions/Create subscription.html": {
    "href": "http-api/swagger/Subscriptions/Create subscription.html",
    "title": "Create subscription | Event Store",
    "keywords": "Create subscription Create a persistant subscription Before interacting with a subscription group, you need to create one. You will receive an error if you attempt to create a subscription group more than once. This requires admin permissions . | Improve this Doc View Source Create subscription Request PUT /subscriptions/{stream}/{subscription} Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group subscriptionItem Subscription to create Responses Status Code Description Samples 200 Subscription created"
  },
  "http-api/swagger/Subscriptions/Delete subscription.html": {
    "href": "http-api/swagger/Subscriptions/Delete subscription.html",
    "title": "Delete subscription | Event Store",
    "keywords": "Delete subscription Deletes a subscription Deletes a subscription | Improve this Doc View Source Delete subscription Request DELETE /subscriptions/{stream}/{subscription} Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group Responses Status Code Description Samples 200 OK"
  },
  "http-api/swagger/Subscriptions/Get {n} subscription events.html": {
    "href": "http-api/swagger/Subscriptions/Get {n} subscription events.html",
    "title": "Get {n} subscription events | Event Store",
    "keywords": "Get {n} subscription events Reads a stream via a persistent subscription and return a specific number of events Reads a stream via a persistent subscription and return a specific number of events | Improve this Doc View Source Get {n} subscription events Request GET /subscriptions/{stream}/{subscription}/{count}[?embed] Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group *count integer How many events to return for the request. embed string Responses Status Code Description Samples 200 OK"
  },
  "http-api/swagger/Subscriptions/Acknowledge a single message.html": {
    "href": "http-api/swagger/Subscriptions/Acknowledge a single message.html",
    "title": "Acknowledge a single message | Event Store",
    "keywords": "Acknowledge a single message Acknowledge a single message Clients must acknowledge (or not acknowledge) messages in the competing consumer model. If the client fails to respond in the given timeout period, the message will be retried. You should use the rel links in the feed for acknowledgements not bookmark URIs as they are subject to change in future versions. | Improve this Doc View Source Acknowledge a single message Request POST /subscriptions/{stream}/{subscription}/ack/{messageid} Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group *messageid string The id of the message that needs to be acked Responses Status Code Description Samples 200 New persistant subscription 400 bad input parameter"
  },
  "http-api/swagger/Subscriptions/Acknowledge multiple messages.html": {
    "href": "http-api/swagger/Subscriptions/Acknowledge multiple messages.html",
    "title": "Acknowledge multiple messages | Event Store",
    "keywords": "Acknowledge multiple messages Acknowledge multiple messages Clients must acknowledge (or not acknowledge) messages in the competing consumer model. If the client fails to respond in the given timeout period, the message will be retried. You should use the rel links in the feed for acknowledgements not bookmark URIs as they are subject to change in future versions. | Improve this Doc View Source Acknowledge multiple messages Request POST /subscriptions/{stream}/{subscription}/ack[?ids] Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group ids string The ids of the messages that need to be acked separated by commas Responses Status Code Description Samples 200 New persistant subscription 400 bad input parameter"
  },
  "http-api/swagger/Subscriptions/Don't acknowledge a single message.html": {
    "href": "http-api/swagger/Subscriptions/Don't acknowledge a single message.html",
    "title": "Don't acknowledge a single message | Event Store",
    "keywords": "Don't acknowledge a single message Negative acknowledge a single message Clients must acknowledge (or not acknowledge) messages in the competing consumer model. If the client fails to respond in the given timeout period, the message will be retried. You should use the rel links in the feed for acknowledgements not bookmark URIs as they are subject to change in future versions. | Improve this Doc View Source Don't acknowledge a single message Request POST /subscriptions/{stream}/{subscription}/nack/{messageid}[?action] Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group *messageid string The id of the message that needs to be nacked action string Park - Don't retry the message, park it until a request is sent to reply the parked messages Retry - Retry the message Skip - Discard the message Stop - Stop the subscription Responses Status Code Description Samples 200 New persistant subscription 400 bad input parameter"
  },
  "http-api/swagger/Subscriptions/Don't acknowledge multiple messages.html": {
    "href": "http-api/swagger/Subscriptions/Don't acknowledge multiple messages.html",
    "title": "Don't acknowledge multiple messages | Event Store",
    "keywords": "Don't acknowledge multiple messages Negative acknowledge multiple messages Clients must acknowledge (or not acknowledge) messages in the competing consumer model. If the client fails to respond in the given timeout period, the message will be retried. You should use the rel links in the feed for acknowledgements not bookmark URIs as they are subject to change in future versions. | Improve this Doc View Source Don't acknowledge multiple messages Request POST /subscriptions/{stream}/{subscription}/nack[?ids&action] Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group ids string The ids of the messages that need to be nacked separated by commas action string Park - Don't retry the message, park it until a request is sent to reply the parked messages Retry - Retry the message Skip - Discard the message Stop - Stop the subscription Responses Status Code Description Samples 200 New persistant subscription 400 bad input parameter"
  },
  "http-api/swagger/Subscriptions/Replay previously parked messages.html": {
    "href": "http-api/swagger/Subscriptions/Replay previously parked messages.html",
    "title": "Replay previously parked messages | Event Store",
    "keywords": "Replay previously parked messages Replay any previously parked messages in a stream Replay any previously parked messages in a stream that were parked by a negative acknowledgement action. | Improve this Doc View Source Replay previously parked messages Request POST /subscriptions/{stream}/{subscription}/replayParked Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group Responses Status Code Description Samples 200 OK"
  },
  "http-api/swagger/Users/Get all users.html": {
    "href": "http-api/swagger/Users/Get all users.html",
    "title": "Get all users | Event Store",
    "keywords": "Get all users Get all users Returns all users defined in Event Store. | Improve this Doc View Source Get all users Request GET /users Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Users/Create a user.html": {
    "href": "http-api/swagger/Users/Create a user.html",
    "title": "Create a user | Event Store",
    "keywords": "Create a user Create a User Create a new user. | Improve this Doc View Source Create a user Request POST /users Parameters Name Type Value Notes userItem User to create Responses Status Code Description Samples 201 New user created 400 Bad request"
  },
  "http-api/swagger/Users/Get a user.html": {
    "href": "http-api/swagger/Users/Get a user.html",
    "title": "Get a user | Event Store",
    "keywords": "Get a user Get user Returns the user currently authenticated with the API, or the user specified. | Improve this Doc View Source Get a user Request GET /users/{login} Parameters Name Type Value Notes *login string The user passed to the API call. Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Users/Update a user.html": {
    "href": "http-api/swagger/Users/Update a user.html",
    "title": "Update a user | Event Store",
    "keywords": "Update a user Update specified user Update the FullName of Groups of the specified user. | Improve this Doc View Source Update a user Request PUT /users/{login} Parameters Name Type Value Notes *login string The user's name userUpdateItem User to update Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Users/Delete a user.html": {
    "href": "http-api/swagger/Users/Delete a user.html",
    "title": "Delete a user | Event Store",
    "keywords": "Delete a user Deletes a user Delete specified user. | Improve this Doc View Source Delete a user Request DELETE /users/{login} Parameters Name Type Value Notes *login string The user's name Responses Status Code Description Samples 204 User deleted"
  },
  "http-api/swagger/Users/Enable a user.html": {
    "href": "http-api/swagger/Users/Enable a user.html",
    "title": "Enable a user | Event Store",
    "keywords": "Enable a user Enable the specified user Enable the acount of the specified user. | Improve this Doc View Source Enable a user Request PUT /users/{login}/command/enable Parameters Name Type Value Notes *login string The user's name Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Users/Disable a user.html": {
    "href": "http-api/swagger/Users/Disable a user.html",
    "title": "Disable a user | Event Store",
    "keywords": "Disable a user Disable the specified user Disable the acount of the specified user. | Improve this Doc View Source Disable a user Request PUT /users/{login}/command/disable Parameters Name Type Value Notes *login string The user's name Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Users/Reset password.html": {
    "href": "http-api/swagger/Users/Reset password.html",
    "title": "Reset password | Event Store",
    "keywords": "Reset password Reset user password Reset the password of the specified user. | Improve this Doc View Source Reset password Request POST /users/{login}/command/reset-password Parameters Name Type Value Notes *login string The user's name *newPassword The new password for the user Responses Status Code Description Samples 200 OK 400 Bad request"
  },
  "http-api/swagger/Users/Change password.html": {
    "href": "http-api/swagger/Users/Change password.html",
    "title": "Change password | Event Store",
    "keywords": "Change password Change user password Change the password of the specified user. | Improve this Doc View Source Change password Request POST /users/{login}/command/change-password Parameters Name Type Value Notes *login string The user's name *newPassword The new password for the user Responses Status Code Description Samples 200 OK 400 Bad request"
  },
  "http-api/swagger/Projections/Get all projections.html": {
    "href": "http-api/swagger/Projections/Get all projections.html",
    "title": "Get all projections | Event Store",
    "keywords": "Get all projections Get all projections Returns all projections defined in Event Store. | Improve this Doc View Source Get all projections Request GET /projections/any Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Get all non-transient projections.html": {
    "href": "http-api/swagger/Projections/Get all non-transient projections.html",
    "title": "Get all non-transient projections | Event Store",
    "keywords": "Get all non-transient projections Get all non-transient projections Returns all known projections except ad-hoc projections. | Improve this Doc View Source Get all non-transient projections Request GET /projections/all-non-transient Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Get all queries.html": {
    "href": "http-api/swagger/Projections/Get all queries.html",
    "title": "Get all queries | Event Store",
    "keywords": "Get all queries Get all queries Returns all queries defined in Event Store. | Improve this Doc View Source Get all queries Request GET /projections/onetime Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Create a onetime projection.html": {
    "href": "http-api/swagger/Projections/Create a onetime projection.html",
    "title": "Create a onetime projection | Event Store",
    "keywords": "Create a onetime projection Create a onetime projection Create a new onetime projection. | Improve this Doc View Source Create a onetime projection Request POST /projections/onetime[?name&type&enabled&checkpoints&emit&trackemittedstreams] Parameters Name Type Value Notes name string Name of the projection type string The projection type enabled boolean Is the projection enabled checkpoints boolean Are checkpoints enabled emit boolean Is emit enabled trackemittedstreams boolean Should your projection create a separate stream and write any streams it emits to that stream. Responses Status Code Description Samples 201 New projection created 400 Bad request"
  },
  "http-api/swagger/Projections/Get all continious projections.html": {
    "href": "http-api/swagger/Projections/Get all continious projections.html",
    "title": "Get all continious projections | Event Store",
    "keywords": "Get all continious projections Get all continious projections Returns all continually running projections defined in Event Store. | Improve this Doc View Source Get all continious projections Request GET /projections/continuous Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Create a continious projection.html": {
    "href": "http-api/swagger/Projections/Create a continious projection.html",
    "title": "Create a continious projection | Event Store",
    "keywords": "Create a continious projection Create a continious projection Create a new continious projection. | Improve this Doc View Source Create a continious projection Request POST /projections/continuous[?name&enabled&checkpoints&emit&type&trackemittedstreams] Parameters Name Type Value Notes name string Name of the projection enabled boolean Is the projection enabled checkpoints boolean Are checkpoints enabled emit boolean Is emit enabled type string The projection type trackemittedstreams boolean Should your projection create a separate stream and write any streams it emits to that stream. Responses Status Code Description Samples 201 New projection created 400 Bad request"
  },
  "http-api/swagger/Projections/Read projection events based on a query.html": {
    "href": "http-api/swagger/Projections/Read projection events based on a query.html",
    "title": "Read projection events based on a query | Event Store",
    "keywords": "Read projection events based on a query Read events from projection based on a query definition Read events from projection based on a query definition, i.e. fromAll, fromStream, fromStreams | Improve this Doc View Source Read projection events based on a query Request POST /projections/read-events Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Get all transient projections.html": {
    "href": "http-api/swagger/Projections/Get all transient projections.html",
    "title": "Get all transient projections | Event Store",
    "keywords": "Get all transient projections Get all transient projections Returns all transient projections defined in Event Store. | Improve this Doc View Source Get all transient projections Request GET /projections/transient Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Create a transient projection.html": {
    "href": "http-api/swagger/Projections/Create a transient projection.html",
    "title": "Create a transient projection | Event Store",
    "keywords": "Create a transient projection Create a transient projection Create a new transient projection. | Improve this Doc View Source Create a transient projection Request POST /projections/transient[?name&type&enabled] Parameters Name Type Value Notes name string Name of the projection type string The projection type enabled boolean Is the projection enabled Responses Status Code Description Samples 201 New user created 400 Bad request"
  },
  "http-api/swagger/Projections/Get projection definition.html": {
    "href": "http-api/swagger/Projections/Get projection definition.html",
    "title": "Get projection definition | Event Store",
    "keywords": "Get projection definition Get projection definition Returns definition of the specified projection. | Improve this Doc View Source Get projection definition Request GET /projection/{name}/query[?config] Parameters Name Type Value Notes *name string The name of the projection config boolean Wether to return the projection definition config. Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Update projection definition.html": {
    "href": "http-api/swagger/Projections/Update projection definition.html",
    "title": "Update projection definition | Event Store",
    "keywords": "Update projection definition Update projection definition Update the specified projection definition. | Improve this Doc View Source Update projection definition Request PUT /projection/{name}/query[?type&emit] Parameters Name Type Value Notes *name string The name of the projection type string The projection type emit boolean Is emit enabled Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Get the projection state.html": {
    "href": "http-api/swagger/Projections/Get the projection state.html",
    "title": "Get the projection state | Event Store",
    "keywords": "Get the projection state Get the projection state Return the current state of the specified projection. | Improve this Doc View Source Get the projection state Request GET /projection/{name}/state[?partition] Parameters Name Type Value Notes *name string The name of the projection partition string The partition name in state Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Get result of projection.html": {
    "href": "http-api/swagger/Projections/Get result of projection.html",
    "title": "Get result of projection | Event Store",
    "keywords": "Get result of projection Get result of projection Get the final result of a projection. | Improve this Doc View Source Get result of projection Request GET /projection/{name}/result[?partition] Parameters Name Type Value Notes *name string The name of the projection partition string The partition name in state Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Get projection statistics.html": {
    "href": "http-api/swagger/Projections/Get projection statistics.html",
    "title": "Get projection statistics | Event Store",
    "keywords": "Get projection statistics Get projection statistics Returns the statistics for a projection, such as how many events, the status etc. | Improve this Doc View Source Get projection statistics Request GET /projection/{name}/statistics Parameters Name Type Value Notes *name string The name of the projection Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Disable projection.html": {
    "href": "http-api/swagger/Projections/Disable projection.html",
    "title": "Disable projection | Event Store",
    "keywords": "Disable projection Disable projection Disable the specified projection. | Improve this Doc View Source Disable projection Request POST /projection/{name}/command/disable[?enableRunAs] Parameters Name Type Value Notes *name string The name of the projection enableRunAs boolean Run as the user issuing the command. Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "dotnet-api/stream-metadata/index.html": {
    "href": "dotnet-api/stream-metadata/index.html",
    "title": "Stream Metadata | Event Store",
    "keywords": "Stream Metadata Every stream in Event Store has metadata associated with it. Internally, the metadata includes information such as the ACL of the stream and the maximum count and age for the events in the stream. Client code can also put information into stream metadata for use with projections or through the client API. A common use of this information is to store associated details about an event that is not part of the event. Examples of these are: \"which user wrote the event?\" \"Which application server were they talking to?\" \"From what IP address did the request come from?\" This information is not part of the actual event but is metadata associated with the event. Event Store stores stream metadata as JSON, and you can access it over the HTTP APIs. Methods Read Stream Metadata <!-- TODO: What's the difference? --> Task<StreamMetadataResult> GetStreamMetadataAsync(string stream, UserCredentials userCredentials = null) Task<RawStreamMetadataResult> GetStreamMetadataAsRawBytesAsync(string stream, UserCredentials userCredentials = null) Write Stream Metadata <!-- TODO: What's the difference? --> Task<WriteResult> SetStreamMetadataAsync(string stream, long expectedMetastreamVersion, StreamMetadata metadata, UserCredentials userCredentials = null) Task<WriteResult> SetStreamMetadataAsync(string stream, long expectedMetastreamVersion, byte[] metadata, UserCredentials userCredentials = null) Read Stream Metadata To read stream metadata over the .NET API you can use methods found on the EventStoreConnection . You can use the GetStreamMetadata methods in two ways. The first is to return a fluent interface over the stream metadata, and the second is to return you the raw JSON of the stream metadata. Task<StreamMetadataResult> GetStreamMetadataAsync(string stream, UserCredentials userCredentials = null) This returns a StreamMetadataResult . The fields on this result are: Member Description string Stream The name of the stream bool IsStreamDeleted true is the stream is deleted, false otherwise. long MetastreamVersion The version of the metastream format StreamMetadata Metadata A StreamMetadata object representing the metadata JSON You can then access the StreamMetadata via the StreamMetadata object. It contains typed fields for well known stream metadata entries. Member Description long? MaxAge The maximum age of events in the stream. Items older than this will be automatically removed. long? MaxCount The maximum count of events in the stream. When you have more than count the oldest will be removed. long? TruncateBefore When set says that items prior to event 'E' can be truncated and will be removed. TimeSpan? CacheControl The head of a feed in the atom api is not cacheable. This allows you to specify a period of time you want it to be cacheable. Low numbers are best here (say 30-60 seconds) and introducing values here will introduce latency over the atom protocol if caching is occuring. StreamAcl Acl The access control list for this stream. If instead you want to work with raw JSON you can use the raw methods for stream metadata. Task<RawStreamMetadataResult> GetStreamMetadataAsRawBytesAsync(string stream, UserCredentials userCredentials = null) This returns a RawStreamMetadataResult . The fields on this result are: Member Description string Stream The name of the stream bool IsStreamDeleted True is the stream is deleted, false otherwise. long MetastreamVersion The version of the metastream (see Expected Version ) byte[] Metadata The raw data of the metadata JSON <!-- TODO: what does security mean? --> Note If you have security enabled, reading metadata may require that you pass credentials. By default it is only allowed for admins though you can change this via default ACLs. If you do not pass credentials and they are required you will receive an AccessedDeniedException . Writing Metadata You can write metadata in both a typed and a raw mechanism. When writing it is generally easier to use the typed mechanism. Both writing mechanisms support an expectedVersion which works the same as on any stream and you can use to control concurrency, read Expected Version for further details. Task<WriteResult> SetStreamMetadataAsync(string stream, long expectedMetastreamVersion, StreamMetadata metadata, UserCredentials userCredentials = null) The StreamMetadata passed above has a builder that you can access via the StreamMetadata.Create() method. The options available on the builder are: Method Description SetMaxCount(long count) Sets the maximum count of events in the stream. SetMaxAge(TimeSpan age) Sets the maximum age of events in the stream. SetTruncateBefore(long seq) Sets the event number from which previous events can be scavenged.< SetCacheControl(TimeSpan cacheControl) The amount of time the stream head is cachable. SetReadRoles(string[] roles) Sets the roles allowed to read the underlying stream. SetWriteRoles(string[] roles) Sets the roles allowed to write to the underlying stream. SetDeleteRoles(string[] roles) Sets the roles allowed to delete the underlying stream. SetMetadataReadRoles(string[] roles) Sets the roles allowed to read the metadata stream. SetMetadataWriteRoles(string[] roles) Sets the roles allowed to write the metadata stream. Be careful with this privilege as it gives all the privileges for a stream as that use can assign themselves any other privilege. SetCustomMetadata(string key, string value) The SetCustomMetadata method and overloads allow the setting of arbitrary custom fields into the stream metadata. You can add user-specified metadata via the SetCustomMetadata overloads. Some examples of good uses of user-specified metadata are: which adapter is responsible for populating a stream. which projection caused a stream to be created. a correlation ID of some business process. Task<WriteResult> SetStreamMetadataAsync(string stream, long expectedMetastreamVersion, byte[] metadata, UserCredentials userCredentials = null) This method will put the data that is in metadata as the stream metadata. Metadata in this case can be anything in a vector of bytes. The server only understands JSON. Read Access Control Lists for more information on the format in JSON for access control lists. Note Writing metadata may require that you pass credentials if you have security enabled by default it is only allowed for admins though you can change this via default ACLs. If you do not pass credentials and they are required you will receive an AccessedDeniedException ."
  },
  "server/command-line-arguments/index.html": {
    "href": "server/command-line-arguments/index.html",
    "title": "Command Line Arguments | Event Store",
    "keywords": "Command Line Arguments Event Store supports many configuration options. There are three distinct ways to set any parameter, all with their own advantages and disadvantages. Via command line Environment variables YAML files. Command Line To pass a configuration value over the command line you add the configuration to the line executing Event Store e.g. EventStore.ClusterNode.exe --log ~/logs While command line arguments tend to be useful in development scenarios, they are often not the preferred way to handle configuration in a production system. Environment Variables You can set all arguments can also as environment variables. This mechanism is often used in UNIX based systems. <!-- TODO: Example? --> YAML Files The last way you can set arguments is to put them into one or more configuration files. To tell Event Store to use a configuration file you pass the files on the command line with --config=filename . The basic format of the YAML configuration file is as follows: --- Log: \"/home/Ouro/logs\" IntHttpPort: 2111 --- Note You need to use the three dashes and spacing in your YAML file. Files can be better for large installations as you can centrally distribute and manage them, or generate them by a configuration management system such as Puppet. The order of precedence between the multiple configuration sources is also important as you could feasibly set them in multiple ways. The command line is the highest priority followed by environment variables. Files are the lowest precedence and are processed in the order given on the command line. When starting Event Store the major parameters are listed in the log (including what set them). ES VERSION: 1.0.3 (dev/5488fa34228e283bad985966b700e1fc48a0780a, Tue, 2 Jul 2013 12:39:02 +0300) OS: Linux (Unix 3.2.0.27) RUNTIME: 3.0.10 (master/1166156 Tue Apr 23 14:56:42 EEST 2013) (EventStore build) (64-bit) GC: 2 GENERATIONS LOGS: /home/greg/foo SHOW HELP: False (<DEFAULT>) SHOW VERSION: False (<DEFAULT>) LOGS DIR: /home/greg/foo (--logsdir from command line) CONFIGS: <empty> (<DEFAULT>) DEFINES: <empty> (<DEFAULT>) IP: 127.0.0.1 (<DEFAULT>) TCP PORT: 1113 (<DEFAULT>) SECURE TCP PORT: 0 (<DEFAULT>) HTTP PORT: 2113 (<DEFAULT>) STATS PERIOD SEC: 30 (<DEFAULT>) CACHED CHUNKS: -1 (<DEFAULT>) CHUNKS CACHE SIZE: 536871424 (<DEFAULT>) DB PATH: /tmp/foo (--db from command line) SKIP DB VERIFY: False (<DEFAULT>) RUN PROJECTIONS: True (<DEFAULT>) PROJECTION THREADS: 3 (<DEFAULT>) WORKER THREADS: 5 (<DEFAULT>) HTTP PREFIXES: <empty> (<DEFAULT>) ENABLE TRUSTED AUTH: False (<DEFAULT>) CERTIFICATE STORE: <empty> (<DEFAULT>) CERTIFICATE NAME: <empty> (<DEFAULT>) CERTIFICATE FILE: <empty> (<DEFAULT>) CERTIFICATE PASSWORD: <empty> (<DEFAULT>) PREPARE TIMEOUT MS: 2000 (<DEFAULT>) COMMIT TIMEOUT MS: 2000 (<DEFAULT>) Note User projections are not enabled by default, but the projections engine is used internally for account management. If you want to run user projections, you have to start Event Store using the --run-projections=all command line parameter. Event Store supports the following parameters: Application Options Parameter Environment (all prefixed with EVENTSTORE\\ )_ Yaml Description -Help --help=VALUE HELP Help Show help. (Default: False) -Version --version=VALUE VERSION Version Show version. (Default: False) -Log --log=VALUE LOG Log Path where to keep log files. (Default: See default directories -Config --config=VALUE CONFIG Config Configuration files. -Defines --defines=VALUE DEFINES Defines Run-time conditionals. (Default: n/a) -WhatIf --what-if=VALUE WHAT_IF WhatIf Print effective configuration to console and then exit. (Default: False) -StartStandardProjections --start-standard-projections=VALUE START_STANDARD_PROJECTIONS StartStandardProjections Start the built in system projections. (Default: False) -DisableHTTPCaching --disable-http-caching=VALUE DISABLE_HTTP_CACHING DisableHTTPCaching Disable HTTP caching. (Default: False) -MonoMinThreadpoolSize --mono-min-threadpool-size=VALUE MONO_MIN_THREADPOOL_SIZE MonoMinThreadpoolSize Minimum number of worker threads when running under mono. Set to 0 to leave machine defaults. (Default: 10) -Force --force=VALUE FORCE Force Force the Event Store to run in possibly harmful environments such as with Boehm GC. (Default: False) -StatsPeriodSec --stats-period-sec=VALUE STATS_PERIOD_SEC StatsPeriodSec The number of seconds between statistics gathers. (Default: 30) -WorkerThreads --worker-threads=VALUE WORKER_THREADS WorkerThreads The number of threads to use for pool of worker services. (Default: 5) -EnableHistograms --enable-histograms=VALUE ENABLE_HISTOGRAMS EnableHistograms Enables the tracking of various histograms in the backend, typically only used for debugging etc (Default: False) -LogHttpRequests --log-http-requests=VALUE LOG_HTTP_REQUESTS LogHttpRequests Log Http Requests and Responses before processing them. (Default: False) Authentication Options Parameter Environment (all prefixed with EVENTSTORE\\ )_ Yaml Description -AuthenticationType --authentication-type=VALUE AUTHENTICATION_TYPE AuthenticationType The type of authentication to use. (Default: internal) -AuthenticationConfig --authentication-config=VALUE AUTHENTICATION_CONFIG AuthenticationConfig Path to the configuration file for authentication configuration (if applicable). Certificate Options Parameter Environment (all prefixed with EVENTSTORE\\ )_ Yaml Description -CertificateStoreLocation --certificate-store-location=VALUE CERTIFICATE_STORE_LOCATION CertificateStoreLocation The certificate store location name. -CertificateStoreName --certificate-store-name=VALUE CERTIFICATE_STORE_NAME CertificateStoreName The certificate store name. -CertificateSubjectName --certificate-subject-name=VALUE CERTIFICATE_SUBJECT_NAME CertificateSubjectName The certificate subject name. -CertificateThumbprint --certificate-thumbprint=VALUE CERTIFICATE_THUMBPRINT CertificateThumbprint The certificate fingerprint/thumbprint. -CertificateFile --certificate-file=VALUE CERTIFICATE_FILE CertificateFile The path to certificate file. -CertificatePassword --certificate-password=VALUE CERTIFICATE_PASSWORD CertificatePassword The password to certificate in file. Cluster Options Parameter Environment (all prefixed with EVENTSTORE\\ )_ Yaml Description -ClusterSize --cluster-size=VALUE CLUSTER_SIZE ClusterSize The number of nodes in the cluster. (Default: 1) -NodePriority --node-priority=VALUE NODE_PRIORITY NodePriority The node priority used during master election (Default: 0) -CommitCount --commit-count=VALUE COMMIT_COUNT CommitCount The number of nodes which must acknowledge commits before acknowledging to a client. (Default: -1) -PrepareCount --prepare-count=VALUE PREPARE_COUNT PrepareCount The number of nodes which must acknowledge prepares. (Default: -1) -DiscoverViaDns --discover-via-dns=VALUE DISCOVER_VIA_DNS DiscoverViaDns Whether to use DNS lookup to discover other cluster nodes. (Default: True) -ClusterDns --cluster-dns=VALUE CLUSTER_DNS ClusterDns DNS name from which other nodes can be discovered. (Default: fake.dns) -ClusterGossipPort --cluster-gossip-port=VALUE CLUSTER_GOSSIP_PORT ClusterGossipPort The port on which cluster nodes' managers are running. (Default: 30777) -GossipSeed --gossip-seed=VALUE GOSSIP_SEED GossipSeed Endpoints for other cluster nodes from which to seed gossip (Default: n/a) -GossipIntervalMs --gossip-interval-ms=VALUE GOSSIP_INTERVAL_MS GossipIntervalMs The interval, in ms, nodes should try to gossip with each other. (Default: 1000) -GossipAllowedDifferenceMs --gossip-allowed-difference-ms=VALUE GOSSIP_ALLOWED_DIFFERENCE_MS GossipAllowedDifferenceMs The amount of drift, in ms, between clocks on nodes allowed before gossip is rejected. (Default: 60000) -GossipTimeoutMs --gossip-timeout-ms=VALUE GOSSIP_TIMEOUT_MS GossipTimeoutMs The timeout, in ms, on gossip to another node. (Default: 500) Database Options Parameter Environment (all prefixed with EVENTSTORE\\ )_ Yaml Description -MinFlushDelayMs --min-flush-delay-ms=VALUE MIN_FLUSH_DELAY_MS MinFlushDelayMs The minimum flush delay in milliseconds. (Default: 2) -DisableScavengeMerging --disable-scavenge-merging=VALUE DISABLE_SCAVENGE_MERGING DisableScavengeMerging Disables the merging of chunks when scavenge is running (Default: False) -ScavengeHistoryMaxAge --scavenge-history-max-age=VALUE SCAVENGE_HISTORY_MAX_AGE ScavengeHistoryMaxAge The number of days to keep scavenge history (Default: 30) -CachedChunks --cached-chunks=VALUE CACHED_CHUNKS CachedChunks The number of chunks to cache in unmanaged memory. (Default: -1) -ReaderThreadsCount --reader-threads-count=VALUE READER_THREADS_COUNT ReaderThreadsCount The number of reader threads to use for processing reads. (Default: 4) -ChunksCacheSize --chunks-cache-size=VALUE CHUNKS_CACHE_SIZE ChunksCacheSize The amount of unmanaged memory to use for caching chunks. (Default: 536871424) -MaxMemTableSize --max-mem-table-size=VALUE MAX_MEM_TABLE_SIZE MaxMemTableSize Adjusts the maximum size of a mem table. (Default: 1000000) -HashCollisionReadLimit --hash-collision-read-limit=VALUE HASH_COLLISION_READ_LIMIT HashCollisionReadLimit The number of events to read per candidate in the case of a hash collision (Default: 100) -Db --db=VALUE DB Db The path the db should be loaded/saved to. (Default: See default directories ) -Index --index=VALUE INDEX Index The path the index should be loaded/saved to. -MemDb --mem-db=VALUE MEM_DB MemDb Keep everything in memory, no directories or files are created. (Default: False) -SkipDbVerify --skip-db-verify=VALUE SKIP_DB_VERIFY SkipDbVerify Bypasses the checking of file hashes of database during startup (allows for faster startup). (Default: False) -WriteThrough --write-through=VALUE WRITE_THROUGH WriteThrough Enables Write Through when writing to the file system, this bypasses filesystem caches. (Default: False) -Unbuffered --unbuffered=VALUE UNBUFFERED Unbuffered Enables Unbuffered/DirectIO when writing to the file system, this bypasses filesystem caches. (Default: False) -PrepareTimeoutMs --prepare-timeout-ms=VALUE PREPARE_TIMEOUT_MS PrepareTimeoutMs Prepare timeout (in milliseconds). (Default: 2000) -CommitTimeoutMs --commit-timeout-ms=VALUE COMMIT_TIMEOUT_MS CommitTimeoutMs Commit timeout (in milliseconds). (Default: 2000) -UnsafeDisableFlushToDisk --unsafe-disable-flush-to-disk=VALUE UNSAFE_DISABLE_FLUSH_TO_DISK UnsafeDisableFlushToDisk Disable flushing to disk. (UNSAFE: on power off) (Default: False) -BetterOrdering --better-ordering=VALUE BETTER_ORDERING BetterOrdering Enable Queue affinity on reads during write process to try to get better ordering. (Default: False) -UnsafeIgnoreHardDelete --unsafe-ignore-hard-delete=VALUE UNSAFE_IGNORE_HARD_DELETE UnsafeIgnoreHardDelete Disables Hard Deletes (UNSAFE: use to remove hard deletes) (Default: False) -IndexCacheDepth --index-cache-depth=VALUE INDEX_CACHE_DEPTH IndexCacheDepth Sets the depth to cache for the mid point cache in index. (Default: 16) -AlwaysKeepScavenged --always-keep-scavenged=VALUE ALWAYS_KEEP_SCAVENGED AlwaysKeepScavenged Always keeps the newer chunks from a scavenge operation. (Default: False) Interface Options Parameter Environment (all prefixed with EVENTSTORE\\ )_ Yaml Description -IntIp --int-ip=VALUE INT_IP IntIp Internal IP Address. (Default: 127.0.0.1) -ExtIp --ext-ip=VALUE EXT_IP ExtIp External IP Address. (Default: 127.0.0.1) -IntHttpPort --int-http-port=VALUE INT_HTTP_PORT IntHttpPort Internal HTTP Port. (Default: 2112) -ExtHttpPort --ext-http-port=VALUE EXT_HTTP_PORT ExtHttpPort External HTTP Port. (Default: 2113) -IntTcpPort --int-tcp-port=VALUE INT_TCP_PORT IntTcpPort Internal TCP Port. (Default: 1112) -IntSecureTcpPort --int-secure-tcp-port=VALUE INT_SECURE_TCP_PORT IntSecureTcpPort Internal Secure TCP Port. (Default: 0) -ExtTcpPort --ext-tcp-port=VALUE EXT_TCP_PORT ExtTcpPort External TCP Port. (Default: 1113) -ExtSecureTcpPortAdvertiseAs --ext-secure-tcp-port-advertise-as=VALUE EXT_SECURE_TCP_PORT_ADVERTISE_AS ExtSecureTcpPortAdvertiseAs Advertise Secure External Tcp Port As. (Default: 0) -ExtSecureTcpPort --ext-secure-tcp-port=VALUE EXT_SECURE_TCP_PORT ExtSecureTcpPort External Secure TCP Port. (Default: 0) -ExtIpAdvertiseAs --ext-ip-advertise-as=VALUE EXT_IP_ADVERTISE_AS ExtIpAdvertiseAs Advertise External Tcp Address As. -ExtTcpPortAdvertiseAs --ext-tcp-port-advertise-as=VALUE EXT_TCP_PORT_ADVERTISE_AS ExtTcpPortAdvertiseAs Advertise External Tcp Port As. (Default: 0) -ExtHttpPortAdvertiseAs --ext-http-port-advertise-as=VALUE EXT_HTTP_PORT_ADVERTISE_AS ExtHttpPortAdvertiseAs Advertise External Http Port As. (Default: 0) -IntIpAdvertiseAs --int-ip-advertise-as=VALUE INT_IP_ADVERTISE_AS IntIpAdvertiseAs Advertise Internal Tcp Address As. -IntSecureTcpPortAdvertiseAs --int-secure-tcp-port-advertise-as=VALUE INT_SECURE_TCP_PORT_ADVERTISE_AS IntSecureTcpPortAdvertiseAs Advertise Secure Internal Tcp Port As. (Default: 0) -IntTcpPortAdvertiseAs --int-tcp-port-advertise-as=VALUE INT_TCP_PORT_ADVERTISE_AS IntTcpPortAdvertiseAs Advertise Internal Tcp Port As. (Default: 0) -IntHttpPortAdvertiseAs --int-http-port-advertise-as=VALUE INT_HTTP_PORT_ADVERTISE_AS IntHttpPortAdvertiseAs Advertise Internal Http Port As. (Default: 0) -IntTcpHeartbeatTimeout --int-tcp-heartbeat-timeout=VALUE INT_TCP_HEARTBEAT_TIMEOUT IntTcpHeartbeatTimeout Heartbeat timeout for internal TCP sockets (Default: 700) -ExtTcpHeartbeatTimeout --ext-tcp-heartbeat-timeout=VALUE EXT_TCP_HEARTBEAT_TIMEOUT ExtTcpHeartbeatTimeout Heartbeat timeout for external TCP sockets (Default: 1000) -IntTcpHeartbeatInterval --int-tcp-heartbeat-interval=VALUE INT_TCP_HEARTBEAT_INTERVAL IntTcpHeartbeatInterval Heartbeat interval for internal TCP sockets (Default: 700) -ExtTcpHeartbeatInterval --ext-tcp-heartbeat-interval=VALUE EXT_TCP_HEARTBEAT_INTERVAL ExtTcpHeartbeatInterval Heartbeat interval for external TCP sockets (Default: 2000) -GossipOnSingleNode --gossip-on-single-node=VALUE GOSSIP_ON_SINGLE_NODE GossipOnSingleNode When enabled tells a single node to run gossip as if it is a cluster (Default: False) -AdminOnExt --admin-on-ext=VALUE ADMIN_ON_EXT AdminOnExt Whether or not to run the admin ui on the external HTTP endpoint (Default: True) -StatsOnExt --stats-on-ext=VALUE STATS_ON_EXT StatsOnExt Whether or not to accept statistics requests on the external HTTP endpoint, needed if you use admin ui (Default: True) -GossipOnExt --gossip-on-ext=VALUE GOSSIP_ON_EXT GossipOnExt Whether or not to accept gossip requests on the external HTTP endpoint (Default: True) -IntHttpPrefixes --int-http-prefixes=VALUE INT_HTTP_PREFIXES IntHttpPrefixes The prefixes that the internal HTTP server should respond to. (Default: n/a) -ExtHttpPrefixes --ext-http-prefixes=VALUE EXT_HTTP_PREFIXES ExtHttpPrefixes The prefixes that the external HTTP server should respond to. (Default: n/a) -EnableTrustedAuth --enable-trusted-auth=VALUE ENABLE_TRUSTED_AUTH EnableTrustedAuth Enables trusted authentication by an intermediary in the HTTP (Default: False) -AddInterfacePrefixes --add-interface-prefixes=VALUE ADD_INTERFACE_PREFIXES AddInterfacePrefixes Add interface prefixes (Default: True) -UseInternalSsl --use-internal-ssl=VALUE USE_INTERNAL_SSL UseInternalSsl Whether to use secure internal communication. (Default: False) -DisableInsecureTCP --disable-insecure-tcp=VALUE DISABLE_INSECURE_TCP DisableInsecureTCP Whether to disable insecure TCP communication (Default: False) -SslTargetHost --ssl-target-host=VALUE SSL_TARGET_HOST SslTargetHost Target host of server's SSL certificate. (Default: n/a) -SslValidateServer --ssl-validate-server=VALUE SSL_VALIDATE_SERVER SslValidateServer Whether to validate that server's certificate is trusted. (Default: True) -ConnectionPendingSendBytesThreshold --connection-pending-send-bytes-threshold=VALUE CONNECTION_PENDING_SEND_BYTES_THRESHOLD ConnectionPendingSendBytesThreshold The maximum number of pending send bytes allowed before a connection is closed. (Default: 10485760) Projections Options Parameter Environment (all prefixed with EVENTSTORE\\ )_ Yaml Description -RunProjections --run-projections=VALUE RUN_PROJECTIONS RunProjections Enables the running of projections. System runs built-in projections, All runs user projections. (Default: None) Possible Values:None,System,All -ProjectionThreads --projection-threads=VALUE PROJECTION_THREADS ProjectionThreads The number of threads to use for projections. (Default: 3)"
  },
  "projections/api/index.html": {
    "href": "projections/api/index.html",
    "title": "API | Event Store",
    "keywords": "API URI Description HTTP Verb /projections/any Returns all known projections. GET /projections/all-non-transient Returns all known non ad-hoc projections. GET /projections/transient Returns all known ad-hoc projections. GET /projections/onetime Returns all known one-time projections. GET /projections/continuous Returns all known continuous projections. GET /projections/transient?name={name}&type={type}&enabled={enabled} Create an ad-hoc projection. This type of projection runs until completion and automatically deleted afterwards. POST Parameters name: Name of the projection type: JS or Native. (JavaScript or native. At this time, Event Store only supports JavaScript) enabled: Enable the projection (true/false) /projections/onetime?name={name}&type={type}&enabled={enabled}&checkpoints={checkpoints}&emit={emit}&trackemittedstreams={trackemittedstreams} Create a one-time projection. This type of projection runs until completion and then stops. POST Parameters name: Name of the projection type: JS or Native. (JavaScript or native. At this time, Event Store only supports JavaScript) enabled: Enable the projection (true/false) checkpoints: Enable checkpoints (true/false) emit: Enable the ability for the projection to write to streams (true/false) trackemittedstreams: Write the name of the streams the projection is managing to a separate stream. $projections-{projection-name}-emittedstreams (true/false) /projections/continuous?name={name}&type={type}&enabled={enabled}&emit={emit}&trackemittedstreams={trackemittedstreams} Create a continuous projection. This type of projection will, if enabled will continuously run unless disabled or an unrecoverable error is encountered. POST Parameters name: Name of the projection type: JS or Native. (JavaScript or native. At this time, Event Store only supports JavaScript) enabled: Enable the projection (true/false) emit: Allow the projection to write to streams (true/false) trackemittedstreams: Write the name of the streams the projection is managing to a separate stream. $projections-{projection-name}-emittedstreams (true/false) /projection/{name}/query?config={config} Returns the definition query and if config is set to true, will return the configuration. GET Parameters name: Name of the projection config: Return the definition of the projection (true/false) /projection/{name}/query?type={type}&emit={emit} Update a projection's query. PUT Parameters name: Name of the projection type: JS or Native. (JavaScript or native. At this time, Event Store only supports JavaScript) emit: Allow the projection to write to streams (true/false) trackemittedstreams: Write the name of the streams the projection is managing to a separate stream. $projections-{projection-name}-emittedstreams (true/false) /projection/{name} Returns information for a projection. GET /projection/{name}?deleteStateStream={deleteStateStream}&deleteCheckpointStream={deleteCheckpointStream}&deleteEmittedStreams={deleteEmittedStreams} Delete a projection, optionally delete the streams that were created as part of the projection. DELETE Parameters name: Name of the projection deleteStateStream: Delete the state stream (true/false) deleteCheckpointStream: Delete the checkpoint stream (true/false) deleteEmittedStreams: Delete the emitted streams stream (true/false) /projection/{name}/statistics Returns detailed information for a projection. GET Parameters name: Name of the projection /projection/{name}/state?partition={partition} Query for the state of a projection. GET Parameters name: Name of the projection partition: The partition /projection/{name}/result?partition={partition} Query for the result of a projection. GET Parameters name: Name of the projection partition: The partition /projection/{name}/command/disable?enableRunAs={enableRunAs} Disable a projection. POST Parameters name: Name of the projection enableRunAs: Enables the projection to run as the user who issued the request. /projection/{name}/command/enable?enableRunAs={enableRunAs} Enable a projection. POST Parameters name: Name of the projection enableRunAs: Enables the projection to run as the user who issued the request. /projection/{name}/command/reset?enableRunAs={enableRunAs} Reset a projection. (This will re-emit events, streams that are written to from the projection will also be soft deleted). POST Parameters name: Name of the projection enableRunAs: Enables the projection to run as the user who issued the request. /projection/{name}/command/abort?enableRunAs={enableRunAs} Abort a projection. POST Parameters name: Name of the projection enableRunAs: Enables the projection to run as the user who issued the request."
  },
  "getting-started/which-api-sdk/index.html": {
    "href": "getting-started/which-api-sdk/index.html",
    "title": "Step 5 - Which API or SDK to use | Event Store",
    "keywords": "Step 5 - Which API or SDK to use This getting started guide shows you how to get started with Event Store using the Atom publishing protocol as the primary interface. This final step covers the different APIs and client SDKs Event Store has available with the aim of helping you choose which one suits your use case. TCP Event Store offers a low level protocol in the form of an asynchronous TCP protocol that exchanges protobuf objects. At present this protocol has adapters for .NET and the JVM. Event Store Supported Clients .net Client JVM Client Community Developed Clients Node.js Node.js Node.js Haskell Erlang F# PHP Elixir Python Java 8 Maven plugin HTTP Event Store also offers an HTTP-based interface, specifically based on the AtomPub protocol . As it operates over HTTP this is less efficient, but nearly every environment supports it. Event Store Supported Clients HTTP API Community Developed Clients Ruby Go If you have your own client to add, click the 'Improve this Doc' link on the top right of the page to submit a pull request. Which to use? There are many factors that go into the choice of which of the protocols (TCP vs HTTP) to use. Both have their strengths and weaknesses. TCP is faster This especially applies to subscribers as events pushed to the subscriber, whereas with Atom the subscribers poll the head of the atom feed to check if new events are available. The difference can be as high as 2–3 times higher (sub 10ms for TCP, vs seconds for Atom). Also, the number of writes per second supported is often dramatically higher when using TCP. At the time of writing, standard Event Store appliances can service around 2000 writes/second over HTTP compared to 15,000-20,000/second over TCP. This might be a deciding factor if you are in a high-performance environment. AtomPub is more scalable for large numbers of subscribers This is due to the ability to use intermediary caching with Atom feeds. Most URIs returned by Event Store point to immutable data and are infinitely cachable. Therefore on a replay of a projection much of the data required is likely available on a local or intermediary cache. This can also lead to lower network traffic. Atom tend to operate better in a large heterogeneous environment where you have callers from different platforms. This is especially true if you have to integrate with different teams or external vendors. Atom is an industry standard and well documented protocol whereas the TCP protocol is a custom protocol they would need to understand. Most platforms have good existing tooling for Atom including feed readers. None of this tooling exists for analyzing traffic with the TCP protocol. Note Our recommendation would be to use AtomPub as your primary protocol unless you have low subscriber SLAs or need higher throughput on reads and writes than Atom can offer. This is due to the open nature and ease of use of the Atom protocol. Often in integration scenarios these are more important than raw performance. Next Step Congratulations! You've reached the end of our getting started guide, what's next? TBD TBD"
  },
  "http-api/optional-http-headers/resolve-linkto/index.html": {
    "href": "http-api/optional-http-headers/resolve-linkto/index.html",
    "title": "Optional HTTP Headers: Resolve LinkTo | Event Store",
    "keywords": "Optional HTTP Headers: Resolve LinkTo When using projections you can have links placed into another stream. By default Event Store will always resolve linkTo s for you returning the event that the link points to. You can use the ES-ResolveLinkTos: false HTTP header to tell Event Store to return you the actual link and to not resolve it. You can see the differences in behavior in the following cURL commands. Request Response curl -i -u admin:changeit http://127.0.0.:2113/streams/testing2/7 -H \"ES-ResolveLinkTos: true\" HTTP/1.1 200 OK Access-Control-Allow-Methods: GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Cache-Control: max-age=31536000, public Vary: Accept Content-Type: application/vnd.eventstore.atom+json; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 27 Jun 2013 14:17:42 GMT Content-Length: 462 Keep-Alive: timeout=15,max=100 { \"title\": \"4@$projections-$all\", \"id\": \"http://127.0.0.1:2113/streams/%24projections-%24all/4\", \"updated\": \"2013-06-27T10:55:32.408301Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"$ProjectionCreated\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/%24projections-%24all/4\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/%24projections-%24all/4\", \"relation\": \"alternate\" } ] } Note The content links are pointing to the original $projections-$all stream (the linked events are being resolved back to where they point). With the header set the links (or embedded content) will instead point back to the actual linkTo events. <!-- TODO: Describe. Before and after --> Request Response curl -i -u admin:changeit http://127.0.0.:2113/streams/testing2/7 -H \"ES-ResolveLinkTos: false\" HTTP/1.1 200 OK Access-Control-Allow-Methods: GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Cache-Control: max-age=31536000, public Vary: Accept Content-Type: application/vnd.eventstore.atom+json; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 27 Jun 2013 14:16:37 GMT Content-Length: 673 Keep-Alive: timeout=15,max=100 { \"title\": \"7@testing2\", \"id\": \"http://127.0.0.1:2113/streams/testing2/7\", \"updated\": \"2013-06-27T11:16:04.171969Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"$>\", \"content\": { \"eventStreamId\": \"testing2\", \"eventNumber\": 7, \"eventType\": \"$>\", \"data\": \"4@$projections-$all\", \"metadata\": { \"$v\": \"7:-1:0:2\", \"$c\": 2046, \"$p\": 1538, \"$causedBy\": \"8ac4f769-7bfb-49aa-bd87-c591cc116697\" } }, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/testing2/7\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/testing2/7\", \"relation\": \"alternate\" } ] }"
  },
  "http-api/optional-http-headers/requires-master/index.html": {
    "href": "http-api/optional-http-headers/requires-master/index.html",
    "title": "Optional HTTP Headers: Requires Master | Event Store",
    "keywords": "Optional HTTP Headers: Requires Master When running in a clustered environment there are times when you only want an operation to happen on the current leader node. A client could get information in an eventually consistent fashion by communicating with the servers (the TCP client included with the multi-node version does this). Over HTTP the RequiresMaster header tells the node that it is not allowed to serve in the case of a read or forward the request in the case of a write. If the node is the master everything will work as normal, if it is not it will respond with a 307 temporary redirect to the master. Run the below on the master: Request Response curl -i \"http://127.0.0.1:32004/streams/stream\" -H \"ES-RequireMaster: True\" HTTP/1.1 200 OK Cache-Control: max-age=0, no-cache, must-revalidate Content-Length: 1296 Content-Type: application/vnd.eventstore.atom+json; charset: utf-8 ETag: \"0;-2060438500\" Vary: Accept Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Date: Thu, 27 Jun 2013 14:48:37 GMT { \"title\": \"Event stream 'stream'\", \"id\": \"http://127.0.0.1:32004/streams/stream\", \"updated\": \"2013-06-27T14:48:15.2596358Z\", \"streamId\": \"stream\", \"author\": { \"name\": \"EventStore\" }, \"links\": [ { \"uri\": \"http://127.0.0.1:32004/streams/stream\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:32004/streams/stream/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:32004/streams/stream/0/forward/20\", \"relation\": \"last\" }, { \"uri\": \"http://127.0.0.1:32004/streams/stream/1/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:32004/streams/stream/metadata\", \"relation\": \"metadata\" } ], \"entries\": [ { \"title\": \"0@stream\", \"id\": \"http://127.0.0.1:32004/streams/stream/0\", \"updated\": \"2013-06-27T14:48:15.2596358Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"TakeSomeSpaceEvent\", \"links\": [ { \"uri\": \"http://127.0.0.1:32004/streams/stream/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:32004/streams/stream/0\", \"relation\": \"alternate\" } ] } ] } Run the following on any other node: Request Response curl -i \"http://127.0.0.1:31004/streams/stream\" -H \"ES-RequireMaster: True\" HTTP/1.1 307 Temporary Redirect Content-Length: 0 Content-Type: text/plain; charset: utf-8 Location: http://127.0.0.1:32004/streams/stream Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Date: Thu, 27 Jun 2013 14:48:28 GMT"
  },
  "dotnet-api/users/index.html": {
    "href": "dotnet-api/users/index.html",
    "title": "User Management | Event Store",
    "keywords": "User Management The Event Store Client API includes helper methods that use the HTTP API to allow for the management of users. This document describes the methods found in the UsersManager class. All methods in this class are asynchronous. Methods Create a User Creates a user, the credentials for this operation must be a member of the $admins group. public Task CreateUserAsync(string login, string fullName, string[] groups, string password, UserCredentials userCredentials = null) Disable a User Disables a user, the credentials for this operation must be a member of the $admins group. public Task DisableAsync(string login, UserCredentials userCredentials = null) Enable a User Enables a user, the credentials for this operation must be a member of the $admins group. public Task EnableAsync(string login, UserCredentials userCredentials = null) Delete a User Deletes (non-recoverable) a user, the credentials for this operation must be a member of the $admins group. If you prefer this action to be recoverable, disable the user as opposed to deleting the user. public Task DeleteUserAsync(string login, UserCredentials userCredentials = null) List all Users Lists all users. public Task<List<UserDetails>> ListAllAsync(UserCredentials userCredentials = null) Get Details of User Return the details of the user supplied in user credentials (e.g. the user making the request). public Task<UserDetails> GetCurrentUserAsync(UserCredentials userCredentials) Get Details of Logged in User public Task<UserDetails> GetUserAsync(string login, UserCredentials userCredentials) Update User Details public Task UpdateUserAsync(string login, string fullName, string[] groups, UserCredentials userCredentials = null) Reset User Password Resets the password of a user. The credentials doing this operation must be part of the $admins group. public Task ResetPasswordAsync(string login, string newPassword, UserCredentials userCredentials = null)"
  },
  "dotnet-api/optimistic-concurrency-and-idempotence/index.html": {
    "href": "dotnet-api/optimistic-concurrency-and-idempotence/index.html",
    "title": "Optimistic Concurrency & Idempotence | Event Store",
    "keywords": "Optimistic Concurrency & Idempotence Writing supports an optimistic concurrency check on the version of the stream to which events are written. The method of specifying what the expected version is depends whether you are making writes using the HTTP or .NET API. The .NET API has constants which you can use to represent certain conditions: Parameter Description ExpectedVersion.Any Disables the optimistic concurrency check. ExpectedVersion.NoStream Specifies the expectation that target stream does not yet exist. ExpectedVersion.EmptyStream Specifies the expectation that the target stream has been explicitly created, but does not yet have any user events written in it. ExpectedVersion.StreamExists Specifies the expectation that the target stream or its metadata stream has been created, but does not expect the stream to be at a specific event number. Any other integer value The event number that you expect the stream to currently be at. If the optimistic concurrency check fails during writing, a WrongExpectedVersionException is thrown. <!-- TODO: Is this a real word?--> Idempotence If identical write operations occur, Event Store treats them in a way which makes it idempotent. If a write is treated in this manner, Event Store acknowledges it as successful, but duplicate events are not written. The idempotence check is based on the EventId and stream . It is possible to reuse an EventId across streams whilst maintaining idempotence. The level of idempotence guarantee depends on whether the optimistic concurrency check is not disabled during writing (by passing ExpectedVersion.Any as the expectedVersion for the write). If you specify an expected version The specified expectedVersion is compared to the currentVersion of the stream. This will yield one of three results: expectedVersion > currentVersion - a WrongExpectedVersionException is thrown. expectedVersion == currentVersion - events are written and acknowledged. expectedVersion < currentVersion - the EventId of each event in the stream starting from expectedVersion are compared to those in the write operation. This can yield one of three further results: All events have been committed already - the write is acknowledged as successful, but no duplicate events written. None of the events were previously committed - a WrongExpectedVersionException is thrown. Some of the events were previously committed - this is considered a bad request. If the write contains the same events as a previous request, either all or none of the events should have been previously committed. This surfaces as a WrongExpectedVersionException . If you specify ExpectedVersion.Any Note Idempotence is not guaranteed if you use ExpectedVersion.Any . The chance of a duplicate event is small, but is possible. The idempotence check will yield one of three results: All events have been committed already - the write is acknowledged as successful, but no duplicate events written. None of the events were previously committed - the events are written and the write acknowledged. Some of the events were previously committed - this is considered a bad request. If the write contains the same events as a previous request, either all or none of the events should have been previously committed. This currently surfaces as a WrongExpectedVersionException ."
  },
  "dotnet-api/embedded-client/index.html": {
    "href": "dotnet-api/embedded-client/index.html",
    "title": "Embedded Client | Event Store",
    "keywords": "Embedded Client EmbeddedVNodeBuilder The EmbeddedVNodeBuilder class sets up and builds an Event Store node. You can configure your node through methods provided by the EmbeddedVNodeBuilder class. Note The builder used for the EmbeddedVNodeBuilder is the same Event Store uses internally to create the ClusterNode , see EventStore.ClusterNode.Program.cs for more examples on how to use it. Building a node You have two options when you start creating a node, EmbeddedVNodeBuilder.AsSingleNode() or EmbeddedVNodeBuilder.AsClusterMember(clusterSize) , which will create a single node or a cluster node respectively. After creating the builder, you can configure the node through the methods provided by the EmbeddedVNodeBuilder . These are listed below. Once you have configured the node, build it with EmbeddedVNodeBuilder.Build() which returns the configured ClusterVNode . Start the node with ClusterVNode.StartAndWaitUntilReady() or ClusterVNode.Start() . ClusterVNode.StartAndWaitUntilReady() returns a task that completes once the node has started and all subsystems have finished loading. For example, to build a single node with default options : var nodeBuilder = EmbeddedVNodeBuilder.AsSingleNode() .OnDefaultEndpoints() .RunInMemory(); var node = nodeBuilder.Build(); node.StartAndWaitUntilReady().Wait(); To build a node to be part of a cluster with custom endpoints and gossip seeds: var nodeBuilder = EmbeddedVNodeBuilder.AsClusterMember(3) .RunOnDisk(\"node1db\") .WithInternalHttpOn(new IPEndPoint(IPAddress.Loopback, 1112)) .WithExternalHttpOn(new IPEndPoint(IPAddress.Loopback, 1113)) .WithExternalTcpOn(new IPEndPoint(IPAddress.Loopback, 1114)) .WithInternalTcpOn(new IPEndPoint(IPAddress.Loopback, 1115)) .DisableDnsDiscovery() .WithGossipSeeds(new IPEndPoint[] { new IPEndPoint(IPAddress.Loopback, 2112), new IPEndPoint(IPAddress.Loopback, 3112) }); var node = nodeBuilder.Build(); node.Start(); Warning When running an embedded cluster, the task returned by StartAndWaitUntilReady() only completes on the master node. Connecting to an embedded node You can connect to an embedded Event Store node with the EmbeddedEventStoreConnection class. Calling EmbeddedEventStoreConnection.Create(ClusterVNode) returns an IEventStoreConnection configured to connect to your embedded node. From there you can use the connection as normal in the .NET Client. using(var embeddedConn = EmbeddedEventStoreConnection.Create(node)) { embeddedConn.ConnectAsync().Wait(); embeddedConn.AppendToStreamAsync(\"testStream\", ExpectedVersion.Any, new EventData(Guid.NewGuid(), \"eventType\", true, Encoding.UTF8.GetBytes(\"{\\\"Foo\\\":\\\"Bar\\\"}\"), null)).Wait(); } Logging with an embedded node To enable logging for an embedded node, you need to initialize the LogManager and ensure that you configure the logger with a log.config file in your configuration directory. To initialize the LogManager , call this before building the nodes: LogManager.Init(logComponentName, logDirectory, logConfigurationDirectory); EmbeddedVNodeBuilder options The following options are available when building an Embedded Node. Application Options Method Description AsSingleNode() Returns a builder set to construct options for a single node instance AsClusterMember(int clusterSize) Returns a builder set to construct options for a cluster node instance with a cluster size DisableHTTPCaching() Disable HTTP Caching WithWorkerThreads(int count) Sets the number of worker threads to use in shared threadpool WithStatsPeriod(TimeSpan statsPeriod) Sets the period between statistics gathers EnableLoggingOfHttpRequests() Enable logging of HTTP requests and responses before they are processed EnableHistograms() Enable the tracking of histograms, typically used for debugging EnableTrustedAuth() Enable trusted authentication by an intermediary in the HTTP Certificate options Method Description WithServerCertificateFromFile(string path, string password) Sets the Server SSL Certificate loaded from a file WithServerCertificate(X509Certificate2 sslCertificate) Sets the Server SSL Certificate WithServerCertificateFromStore(StoreLocation storeLocation, StoreName storeName, string certificateSubjectName, string certificateThumbprint) Sets the Server SSL Certificate loaded from a certificate store WithServerCertificateFromStore(StoreName storeName, string certificateSubjectName, string certificateThumbprint) Sets the Server SSL Certificate loaded from a certificate store Cluster options Method Description WithClusterGossipPort(int port) Sets the internal gossip port (used when using cluster DNS, this should point to a known port gossip will be running on) WithGossipSeeds(params IPEndPoint[] endpoints) Sets the gossip seeds this node should talk to WithClusterDnsName(string name) Sets the DNS name used for the discovery of other cluster nodes DisableDnsDiscovery() Disable DNS discovery for the cluster WithGossipInterval(TimeSpan gossipInterval) Sets the gossip interval WithGossipAllowedTimeDifference(TimeSpan gossipAllowedDifference) Sets the allowed gossip time difference WithGossipTimeout(TimeSpan gossipTimeout) Sets the gossip timeout WithPrepareTimeout(TimeSpan prepareTimeout) Sets the prepare timeout WithCommitTimeout(TimeSpan commitTimeout) Sets the commit timeout WithPrepareCount(int prepareCount) Sets the number of nodes which must acknowledge prepares. WithCommitCount(int commitCount) Sets the number of nodes which must acknowledge commits before acknowledging to a client. WithNodePriority(int nodePriority) Sets the node priority used during master election Database options Method Description RunInMemory() Sets the builder to run in memory RunOnDisk(string path) Sets the builder to write database files to the specified path MaximumMemoryTableSizeOf(int size) Sets the maximum size a memtable is allowed to reach (in count) before moved to be a ptable DoNotVerifyDbHashes() Marks that the existing database files should not be checked for checksums on startup. VerifyDbHashes() Marks that the existing database files should be checked for checksums on startup. WithMinFlushDelay(TimeSpan minFlushDelay) Sets the minimum flush delay DisableScavengeMerging() Disables the merging of chunks when scavenge is running WithScavengeHistoryMaxAge(int scavengeHistoryMaxAge) The number of days to keep scavenge history (Default: 30) WithIndexPath(string indexPath) Sets the path the index should be loaded or saved to WithIndexCacheDepth(int indexCacheDepth) Sets the depth to cache for the mid point cache in index WithUnsafeIgnoreHardDelete() Disables Hard Deletes (UNSAFE: use to remove hard deletes) WithUnsafeDisableFlushToDisk() Disables Hard Deletes (UNSAFE: use to remove hard deletes) WithBetterOrdering() Enable queue affinity on reads during write process to try to get better ordering. WithTfChunkSize(int chunkSize) Sets the transaction file chunk size. Default is WithTfChunksCacheSize(long chunksCacheSize) Sets the transaction file chunk cache size. Default is WithTfCachedChunks(int cachedChunks) The number of chunks to cache in unmanaged memory. Interface options Method Description OnDefaultEndpoints() Sets the default endpoints on localhost (1113 tcp, 2113 http) AdvertiseInternalIPAs(IPAddress intIpAdvertiseAs) Sets up the Internal IP to advertise AdvertiseExternalIPAs(IPAddress extIpAdvertiseAs) Sets up the External IP to advertise AdvertiseInternalHttpPortAs(int intHttpPortAdvertiseAs) Sets up the Internal HTTP port to advertise AdvertiseExternalHttpPortAs(int extHttpPortAdvertiseAs) Sets up the External HTTP port to advertise AdvertiseInternalSecureTCPPortAs(int intSecureTcpPortAdvertiseAs) Sets up the Internal Secure TCP port to advertise AdvertiseExternalSecureTCPPortAs(int extSecureTcpPortAdvertiseAs) Sets up the External Secure TCP port to advertise AdvertiseInternalTCPPortAs(int intTcpPortAdvertiseAs) Sets up the Internal TCP port to advertise AdvertiseExternalTCPPortAs(int extTcpPortAdvertiseAs) Sets up the External TCP port to advertise WithInternalHttpOn(IPEndPoint endpoint) Sets the internal HTTP endpoint to the specified value WithExternalHttpOn(IPEndPoint endpoint) Sets the external HTTP endpoint to the specified value WithInternalTcpOn(IPEndPoint endpoint) Sets the internal TCP endpoint to the specified value WithInternalSecureTcpOn(IPEndPoint endpoint) Sets the internal secure TCP endpoint to the specified value WithExternalTcpOn(IPEndPoint endpoint) Sets the external TCP endpoint to the specified value WithExternalSecureTcpOn(IPEndPoint endpoint) Sets the external secure TCP endpoint to the specified value EnableSsl() Sets that SSL should be used on connections WithSslTargetHost(string targetHost) Sets the target host of the server's SSL certificate. ValidateSslServer() Sets whether to validate that the server's certificate is trusted. NoGossipOnPublicInterface() Disables gossip on the public (client) interface NoAdminOnPublicInterface() Disables the admin interface on the public (client) interface NoStatsOnPublicInterface() Disables statistics screens on the public (client) interface AddInternalHttpPrefix(string prefix) Adds a HTTP prefix for the internal HTTP endpoint AddExternalHttpPrefix(string prefix) Adds a HTTP prefix for the external HTTP endpoint DontAddInterfacePrefixes() Don't add the interface prefixes (e.g. If the External IP is set to the Loopback address, add http://localhost:2113/ as a prefix) WithInternalHeartbeatInterval(TimeSpan heartbeatInterval) Sets the heartbeat interval for the internal network interface. WithExternalHeartbeatInterval(TimeSpan heartbeatInterval) Sets the heartbeat interval for the external network interface. WithInternalHeartbeatTimeout(TimeSpan heartbeatTimeout) Sets the heartbeat timeout for the internal network interface. WithExternalHeartbeatTimeout(TimeSpan heartbeatTimeout) Sets the heartbeat timeout for the external network interface. Projections options Method Description StartStandardProjections() Start standard projections. RunProjections(ProjectionType projectionType, int numberOfThreads = Opts.ProjectionThreadsDefault) Sets the mode and the number of threads on which to run projections. RunProjections(ClientAPI.Embedded.ProjectionsMode projectionsMode, int numberOfThreads = Opts.ProjectionThreadsDefault) Sets the mode and the number of threads on which to run projections. EmbeddedEventStoreConnection The following methods are available on EmbeddedEventStoreConnection for connecting to an embedded node. Method Description Create(ClusterVNode eventStore, string connectionName = null) Creates a new embedded IEventStoreConnection to a single node with default connection settings Create(ClusterVNode eventStore, ConnectionSettings connectionSettings, string connectionName = null) Creates a new embedded IEventStoreConnection to a single node using specific ConnectionSettings"
  },
  "dotnet-api/competing-consumers/index.html": {
    "href": "dotnet-api/competing-consumers/index.html",
    "title": "Competing Consumers | Event Store",
    "keywords": "Competing Consumers This document explains how to use .NET API for setting up and consuming competing consumer subscription groups. For an overview of competing consumers and how they relate to other subscription types, please see the overview document . Note The Administration UI includes a Competing Consumers section where a user can create, update, delete and view subscriptions and their statuses. Methods Creating a Persistent Subscription Before interacting with a subscription group, you need to create one. You will receive an error if you attempt to create a subscription group more than once. This requires admin permissions . Task<PersistentSubscriptionCreateResult> CreatePersistentSubscriptionAsync(string stream, string groupName, PersistentSubscriptionSettings settings, UserCredentials credentials); Updating a Persistent Subscription You can edit the settings of an existing subscription while it is running. This action drops the current subscribers and resets the subscription internally. This requires admin permissions . Task<PersistentSubscriptionUpdateResult> UpdatePersistentSubscriptionAsync(string stream, string groupName, PersistentSubscriptionSettings settings, UserCredentials credentials); Deleting a Persistent Subscription <!-- TODO: Explanation? --> Task<PersistentSubscriptionDeleteResult> DeletePersistentSubscriptionAsync(string stream, string groupName, UserCredentials userCredentials = null); Connecting to a Persistent Subscription <!-- TODO: Explanation? --> EventStorePersistentSubscription ConnectToPersistentSubscription( string groupName, string stream, Func<EventStorePersistentSubscription, ResolvedEvent, Task> eventAppeared, Action<EventStorePersistentSubscription, SubscriptionDropReason, Exception> subscriptionDropped = null, UserCredentials userCredentials = null, int bufferSize = 10, bool autoAck = true); Persistent Subscription Settings Both the Create and Update methods take a PersistentSubscriptionSettings object as a parameter. The methods use this object to provide the settings for the persistent subscription. A fluent builder is available for these options that you can locate using the Create() method. The following table shows the options you can set on a persistent subscription. Member Description ResolveLinkTos Tells the subscription to resolve link events. DoNotResolveLinkTos Tells the subscription to not resolve link events. PreferRoundRobin If possible preference a round robin between the connections with messages (if not possible uses next available). PreferDispatchToSingle If possible preference dispatching to a single connection (if not possible will use next available). StartFromBeginning Start the subscription from the first event in the stream. StartFrom(int position) Start the subscription from the position-th event in the stream. StartFromCurrent Start the subscription from the current position. WithMessageTimeoutOf(TimeSpan timeout) Sets the timeout for a client before retrying the message. CheckPointAfter(TimeSpan time) The amount of time the system should try to checkpoint after. MinimumCheckPointCountOf(int count) The minimum number of messages to write a checkpoint for. MaximumCheckPointCountOf(int count) The maximum number of messages not checkpointed before forcing a checkpoint. WithMaxRetriesOf(int count) Sets the number of times to retry a message should before considering it a bad message. WithLiveBufferSizeOf(int count) The size of the live buffer (in memory) before resorting to paging. WithReadBatchOf(int count) The size of the read batch when in paging mode. WithBufferSizeOf(int count) The number of messages to buffer when in paging mode. WithExtraStatistics Tells the backend to measure timings on the clients so statistics contain histograms of them. Creating a Subscription Group The first step of dealing with a subscription group is to create one. You will receive an error if you attempt to create a subscription group multiple times. You must have admin permissions to create a persistent subscription group. Note Normally you wouldn't create the subscription group in your general executable code. Instead, you create it as a step during an install or as an admin task when setting up Event Store. You should assume the subscription exists in your code. PersistentSubscriptionSettings settings = PersistentSubscriptionSettings.Create() .DoNotResolveLinkTos() .StartFromCurrent(); _result = _conn.CreatePersistentSubscriptionAsync(_stream, \"agroup\", settings, MyCredentials).Result; Parameter Description string stream The stream to the persistent subscription is on. string groupName The name of the subscription group to create. PersistentSubscriptionSettings settings The settings to use when creating this subscription. UserCredentials credentials The user credentials to use for this operation. Updating a Subscription Group You can edit the settings of an existing subscription group while it is running, you don't need to delete and recreate it to change settings. When you update the subscription group, it resets itself internally dropping the connections and having them reconnect. You must have admin permissions to update a persistent subscription group. PersistentSubscriptionSettings settings = PersistentSubscriptionSettings.Create() .DoNotResolveLinkTos() .StartFromCurrent(); _result = _conn.UpdatePersistentSubscriptionAsync(_stream, \"agroup\", settings, MyCredentials).Result; Note If you change settings such as startFromBeginning , this doesn't reset the group's checkpoint. If you want to change the current position in an update, you must delete and recreate the subscription group. Parameter Description string stream The stream to the persistent subscription is on. string groupName The name of the subscription group to update. PersistentSubscriptionSettings settings The settings to use when updating this subscription. UserCredentials credentials The user credentials to use for this operation. Deleting a Subscription Group Remove a subscription group with the delete operation. Like the creation of groups, you rarely do this in your runtime code and is undertaken by an administrator running a script. var result = _conn.DeletePersistentSubscriptionAsync(stream, \"groupname\", DefaultData.AdminCredentials).Result; Parameter Description string stream The stream to the persistent subscription is on. string groupName The name of the subscription group to update. UserCredentials credentials The user credentials to use for this operation. Connecting to a Subscription Group Once you have created a subscription group, clients can connect to that subscription group. A subscription in your application should only have the connection in your code, you should assume that the subscription was created via the client API, the restful API, or manually in the UI. The most important parameter to pass when connecting is the buffer size. This parameter represents how many outstanding messages the server should allow this client. If this number is too small, your subscription will spend much of its time idle as it waits for an acknowledgment to come back from the client. If it's too big, you waste resources and can start causing time out messages depending on the speed of your processing. var subscription = _conn.ConnectToPersistentSubscription(\"foo\", \"nonexisting2\", (sub, e) => Console.Write(\"appeared\"), (sub, reason, ex) =>{}); Parameter Description string stream The stream to the persistent subscription is on. string groupName The name of the subscription group to connect to. Action eventAppeared The action to call when an event arrives over the subscription. Action subscriptionDropped The action to call if the subscription is dropped. UserCredentials credentials The user credentials to use for this operation. int bufferSize The number of in-flight messages this client is allowed. bool autoAck Whether to automatically acknowledge messages after eventAppeared returns. Acknowledgements Clients must acknowledge (or not acknowledge) messages in the competing consumer model. If you enable auto-ack the subscription will automatically acknowledge messages once your handler completes them. If you throw an exception, it will shut down your subscription with a message and the uncaught exception. You can choose to not auto-ack messages. This can be useful when you have multi-threaded processing of messages in your subscriber and need to pass control to something else. There are methods on the subscription object that you can call Acknowledge, and NotAcknowledge both take a ResolvedEvent (the one you processed) both also have overloads for passing and IEnumerable<ResolvedEvent> . Consumer Strategies When creating a persistent subscription, the settings allow for different consumer strategies via the WithNamedConsumerStrategy method. Built-in strategies are defined in the enum SystemConsumerStrategies . Note HTTP clients bypass the consumer strategy which means it ignores any ordering or pinning. Strategy Name Description RoundRobin (default) Distributes events to all clients evenly. If the client bufferSize is reached the client is ignored until events are acknowledged/not acknowledged. DispatchToSingle Distributes events to a single client until the bufferSize is reached. After which the next client is selected in a round robin style, and the process is repeated. Pinned For use with an indexing projection such as the system $by_category projection. Event Store inspects event for its source stream id, hashing the id to one of 1024 buckets assigned to individual clients. When a client disconnects it's buckets are assigned to other clients. When a client connects, it is assigned some of the existing buckets. This naively attempts to maintain a balanced workload. The main aim of this strategy is to decrease the likelihood of concurrency and ordering issues while maintaining load balancing. This is not a guarantee , and you should handle the usual ordering and concurrency issues."
  },
  "getting-started/reading-subscribing-events/index.html": {
    "href": "getting-started/reading-subscribing-events/index.html",
    "title": "Step 2 - Read events from a stream and subscribe to changes | Event Store",
    "keywords": "Step 2 - Read events from a stream and subscribe to changes This getting started guide shows you how to get started with Event Store using the Atom publishing protocol as the primary interface. This second step covers reading events from a stream and subscribing to changes to events in a stream. Note The described is for development and evaluation of Event Store. It does not describe a production setup. Event Store exposes all streams as atom feeds , and you can read data from the stream by navigating to the head URI of the stream http://127.0.0.1:2113/streams/<STREAM_ID > with cURL. Request Response curl -i -H \"Accept:application/vnd.eventstore.atom+json\" \"http://127.0.0.1:2113/streams/newstream\" curl -i -H \"Accept:application/vnd.eventstore.atom+json\" \"http://127.0.0.1:2113/streams/newstream\" HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-Forwarded-Host, X-Forwarded-Prefix, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTos Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position, ES-CurrentVersion Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"0;-2060438500\" Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 15 Dec 2017 12:23:23 GMT Content-Length: 1262 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'newstream'\", \"id\": \"http://127.0.0.1:2113/streams/newstream\", \"updated\": \"2017-12-15T12:19:32.021776Z\", \"streamId\": \"newstream\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": true, \"selfUrl\": \"http://127.0.0.1:2113/streams/newstream\", \"eTag\": \"0;-2060438500\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/1/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/metadata\", \"relation\": \"metadata\" } ], \"entries\": [ { \"title\": \"0@newstream\", \"id\": \"http://127.0.0.1:2113/streams/newstream/0\", \"updated\": \"2017-12-15T12:19:32.021776Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"event-type\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/newstream/0\", \"relation\": \"alternate\" } ] } ] } # TODO: Use current dates in build somehow Note This example uses cURL, but you can read Atom feeds with a wide variety of applications and languages. Note This command asked Event Store to return the feed in JSON format, you can also use Accept:application/atom+xml if you prefer XML. The feed has a single item inside of it, the one you posted in part 1 . You can then get the event by issuing a GET to the alternate URI value from the response above. Request Response curl -i http://127.0.0.1:2113/streams/newstream/0 -H \"Accept: application/json\" curl -i http://127.0.0.1:2113/streams/newstream/0 -H \"Accept: application/json\" HTTP/1.1 200 OK Access-Control-Allow-Methods: GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Cache-Control: max-age=31536000, public Vary: Accept Content-Type: application/json; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Wed, 03 Jul 2013 11:09:12 GMT Content-Length: 14 Keep-Alive: timeout=15,max=100 { \"a\": \"1\" } Note You can also use Accept: text/xml if you prefer XML. To read a single page of events, you request the stream and then iterate through the event links by executing GET requests. This may feel inefficient at first but remember the event URIs and most of the page URIs are infinitely cachable. You can also GET the events in the feed itself if by using ?embed=body in the request. You can find further information on this here . Sometimes your feed may span more than one atom page, and you need to paginate through the feed. You do this by following the relation links in the feed. To read a feed from the beginning to the end you send a GET request to the last link and then continue to GET the previous page links. Reverse the order of events by sending a GET request to the first link and continuing to GET the next page links. <!-- TODO: Add an example --> Subscribing to Receive Stream Updates A common operation is to subscribe to a stream and receive notifications for changes. This works the same way as paging through an Atom feed. As new events arrive, Event Store creates new previous links and you continue following them. There are three types of subscription are available, useful in different situations. Volatile Subscriptions This subscription calls a given function for events written after establishing the subscription. They are useful when you need notification of new events with minimal latency, but where it's not necessary to process every event. <!-- TODO: Create an example pop-out? See DFM docs, comments --> Note For example, if a stream has 100 events in it when a subscriber connects, the subscriber can expect to see event number 101 onwards until the time the subscription is closed or dropped. Catch-Up Subscriptions This subscription specifies a starting point, in the form of an event number or transaction file position. The given function is called for events from the starting point until the end of the stream, and then for subsequently written events. <!-- TODO: Example needed --> Note For example, if you specify a starting point of 50 when a stream has 100 events, the subscriber can expect to see events 51 through 100, and then any events subsequently written until you drop or close the subscription. Persistent Subscriptions <!-- TODO: Create a version pop-out --> Note Persistent subscriptions exist in version 3.2.0 and above of Event Store. This subscriptions supports the \" competing consumers \" messaging pattern. Event Store saves the subscription state server-side and allows for at-least-once delivery guarantees across multiple consumers on the same stream. It is possible to have many groups of consumers compete on the same stream, with each group getting an at-least-once guarantee. <!-- TODO: Example needed --> Note These subscriptions are useful when you need to distribute messages to many workers. Next Step In this second part of our getting started guide you learned how to read events from a stream and subscribe to changes. The next part covers projections, used to give you continuous queries of your data. Step 3 - Projections"
  },
  "http-api/optional-http-headers/longpoll/index.html": {
    "href": "http-api/optional-http-headers/longpoll/index.html",
    "title": "Optional HTTP Headers: LongPoll | Event Store",
    "keywords": "Optional HTTP Headers: LongPoll You use the ES-LongPoll header to instruct the server that when on the head link of a stream and no data is available the server should wait a period of time to see if data becomes available. Warning Version 2.1 and higher support the ES-LongPoll header. Lower versions will ignore the header. <!-- TODO: version 2.1 of what? --> You can use this to provide lower latency for Atom clients instead of client initiated polling. Instead of the client polling every say 5 seconds to get data from the feed the client would send up a request with ES-LongPoll: 15 . This instructs the backend to wait for up to 15 seconds before returning with no result. The latency is therefore lowered from the poll interval to about 10ms from the time an event is written until the time the HTTP connection is notified. You can see the use of the ES-LongPoll header in the following cURL command. First go to the head of the stream (in this case we are using the default chat of the chat sample). Request Response curl -i http://127.0.0.1:2113/streams/chat-GeneralChat -H \"Accept: application/json\" HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"0;-43840953\" Content-Type: application/json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 08 May 2014 10:12:27 GMT Content-Length: 1348 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'chat-GeneralChat'\", \"id\": \"http://127.0.0.1:2113/streams/chat-GeneralChat\", \"updated\": \"2014-05-08T07:10:23.666463Z\", \"streamId\": \"chat-GeneralChat\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": true, \"selfUrl\": \"http://127.0.0.1:2113/streams/chat-GeneralChat\", \"eTag\": \"0;248368668\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/1/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/metadata\", \"relation\": \"metadata\" } ], \"entries\": [ { \"title\": \"0@chat-GeneralChat\", \"id\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/0\", \"updated\": \"2014-05-08T07:10:23.666463Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"UserJoinedChat\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/0\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/0\", \"relation\": \"alternate\" } ] } ] } Then grab the previous rel link http://127.0.0.1:2113/streams/chat-GeneralChat/1/forward/20 and try it. It returns an empty feed. Request Response curl -i http://127.0.0.1:2113/streams/chat-GeneralChat/1/forward/20 -H \"Accept: application/json\" HTTP/1.1 200 OK Access-Control-Allow-Methods: GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"0;-43840953\" Content-Type: application/json; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 08 May 2014 10:13:58 GMT Content-Length: 843 Keep-Alive: timeout=15,max=100 { \"title\": \"Event stream 'chat-GeneralChat'\", \"id\": \"http://127.0.0.1:2113/streams/chat-GeneralChat\", \"updated\": \"0001-01-01T00:00:00Z\", \"streamId\": \"chat-GeneralChat\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": false, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/0/forward/20\", \"relation\": \"last\" }, { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/0/backward/20\", \"relation\": \"next\" }, { \"uri\": \"http://127.0.0.1:2113/streams/chat-GeneralChat/metadata\", \"relation\": \"metadata\" } ], \"entries\": [] } The entries section is empty (there is no further data that can be provided). Now try that URI with a long poll header. curl -i http://127.0.0.1:2113/streams/chat-GeneralChat/1/forward/20 -H \"Accept: application/json\" -H \"ES-LongPoll: 10\" If you do not insert any events into the stream while this is running it will take 10 seconds for the HTTP request to finish. If you append an event to the stream while its running you will see the result for that request when the event is appended."
  },
  "http-api/writing-to-a-stream/index.html": {
    "href": "http-api/writing-to-a-stream/index.html",
    "title": "Writing to a Stream | Event Store",
    "keywords": "Writing to a Stream You write to a stream over HTTP using a POST request to the resource of the stream. If the stream does not exist then the stream will be implicitly created. Single Event Note Writing a single event has changed from Event Store 2.X.X to Event Store 3.X.X. In Event Store 3.X.X a post of application/json assumes the post data to be the actual event. When you need to write multiple events, use application/vnd.eventstore.events+json instead. Issuing a POST request with the data below to a stream, with the correct content type set, will result in writing the event to the stream, and a 201 response from the server, giving you the location of the event. In a file named myevent.txt [ { \"eventId\": \"fbf4a1a1-b4a3-4dfe-a01f-ec52c34e16e4\", \"eventType\": \"event-type\", \"data\": { \"a\": \"1\" } } ] POST the following request: Request Response curl -i -d @myevent.txt \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -H \"ES-EventType: SomeEvent\" HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream2/0 Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Thu, 29 Jan 2015 14:28:05 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 The event will be available in the stream. Some clients may not be able to generate a GUID (or may not want to generate a GUID) for the ID. You need this ID for idempotence purposes but the server can generate it for you. If we were to leave off the ES-EventId header you would see different behavior: Request Response curl -i -d @myevent.txt \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -H \"ES-EventType: SomeEvent\" HTTP/1.1 301 FOUND Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream/incoming/ad1c1288-0d61-4995-88b2-06c57a42495b Content-Type: ; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:01:29 GMT Content-Length: 28 Keep-Alive: timeout=15,max=100 In this case Event Store has responded with a 301 redirect . The location points to another URI that you can post the event to. This new URI will be idempotent for posting to even without an event ID. Request Response curl -i -d @myevent.txt \"http://127.0.0.1:2113/streams/newstream/incoming/ad1c1288-0d61-4995-88b2-06c57a42495b\" -H \"Content-Type: application/json\" -H \"ES-EventType: SomeEvent\" HTTP/1.1 201 Created Access-Control-Allow-Methods: GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream/0 Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:15:33 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 It is generally recommended to include an event ID if possible as it will result in fewer round trips between the client and the server. When posting to either the stream or to the returned redirect clients must include the EventType header. If you forget to include the header you will receive an error. Request Response curl -i -d @myevent.txt \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" HTTP/1.1 400 Must include an event type with the request either in body or as ES-EventType header. Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Content-Type: Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:05:45 GMT Content-Length: 0 Connection: close Event Store Events Media Type Event store supports a custom media type for posting events application/vnd.eventstore.events (+json/+xml) . This format allows for extra functionality that posting events as above does not. For example it allows you to post multiple events in a single batch. In a file named simple-event.txt : [ { \"eventId\": \"fbf4a1a1-b4a3-4dfe-a01f-ec52c34e16e4\", \"eventType\": \"event-type\", \"data\": { \"a\": \"1\" } } ] The data is represented by the following jschema ( eventId must be a UUID). [ { \"eventId\" : \"string\", \"eventType\" : \"string\", \"data\" : \"object\", \"metadata\" : \"object\" } ] Create a Stream To create a stream, issue a POST request to the /streams/newstream resource: Request Response curl -i -d @event.txt \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/vnd.eventstore.events+json\" HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Location: http://127.0.0.1:2113/streams/newstream/0 Content-Type: text/plain; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 28 Jun 2013 12:17:59 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 Appending Events To append events, issue a POST request to the same resource again and edit the message ID GUID: <!-- TODO: This doesn't show that --> Request Response curl -i -d @event.txt \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/vnd.eventstore.events+json\" HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Location: http://127.0.0.1:2113/streams/newstream/1 Content-Type: text/plain; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 28 Jun 2013 12:32:18 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 Data only events Version 3.7.0 of Event Store added support for the application/octet-stream content type to support data only events. When creating these events, you will need to provide the ES-EventType and ES-EventId headers and cannot have metadata associated with the Event. In the example below SGVsbG8gV29ybGQ= is the data you POST to the stream: Request Response curl -i -d \"SGVsbG8gV29ybGQ=\" \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/octet-stream\" -H \"ES-EventType:rawDataType\" -H \"ES-EventId:eeccf3ce-4f54-409d-8870-b35dd836cca6\" HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-Forwarded-Host, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream/0 Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 27 Jun 2016 13:15:27 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 Expected Version The expected version header is a number representing the version of the stream you read from. For example if you read from the stream and it was at version 5 then you expect it to be at version 5. This can allow for optimistic locking when multiple applications are reading/writing to streams. If your expected version is not the current version you will receive a HTTP status code of 400. Note See the idempotency section below, if you post the same event twice it will be idempotent and will not give a version error. Request Response curl -i -d @event.txt \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -H \"ES-ExpectedVersion: 3\" HTTP/1.1 400 Wrong expected EventNumber Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Content-Type: text/plain; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Fri, 28 Jun 2013 12:33:30 GMT Content-Length: 0 Connection: close There are some special values you can put into the expected version header. -2 states that this write should never conflict and should always succeed. -1 states that the stream should not exist at the time of the writing (this write will create it). 0 states that the stream should exist but should be empty. Batch Writes You can include more than one write in a single post by placing multiple events inside of the array representing the events, you can include metadata. The following post body inserts two events: [ { \"eventId\": \"fbf4b1a1-b4a3-4dfe-a01f-ec52c34e16e4\", \"eventType\": \"event-type\", \"data\": { \"a\": \"1\" } }, { \"eventId\": \"0f9fad5b-d9cb-469f-a165-70867728951e\", \"eventType\": \"event-type\", \"data\": { \"a\": \"1\" } } ] When you write multiple events in a single post, Event Store treats them transactionally, it writes all events together or all will fail. Idempotency Appends to streams are idempotent based upon the EventId assigned in your post. If you were to re-run the last cURL command it will return the same value again. Request Response curl -i -d @event.txt \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" HTTP/1.1 201 Created Access-Control-Allow-Methods: DELETE, GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER Access-Control-Allow-Origin: * Location: http://127.0.0.1:2113/streams/newstream/2 Content-Type: ; charset: utf-8 Server: Mono-HTTPAPI/1.0 Date: Wed, 03 Apr 2013 15:21:53 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 This is important behaviour as this is how you implement error handling. If you receive a timeout, broken connection, no answer, etc from your HTTP POST then it's your responsibility is to retry the post. You must also keep the same uuid that you assigned to the event in the first POST . If you are using the expected version parameter with your message, then Event Store is 100% idempotent. If you use Any as your expected version value, Event Store will do its best to keep events idempotent but cannot assure that everything is fully idempotent and you will end up in 'at-least-once' messaging. Read this guide for more details on idempotency. This idempotency also applies to the URIs generated by the server if you post a body as an event without the ES-EventId header associated with the request: Request Response curl -i -d @event.txt \"http://127.0.0.1:2113/streams/newstream\" -H \"Content-Type:application/json\" -H \"ES-EventType: SomeEvent\" HTTP/1.1 301 FOUND Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream/incoming/c7248fc1-3db4-42c1-96aa-a071c92649d1 Content-Type: ; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:11:59 GMT Content-Length: 28 Keep-Alive: timeout=15,max=100 You can then post multiple times to the generated redirect URI and Event Store will make the requests will idempotent for you: Request Response curl -i -d @event.txt \"http://127.0.0.1:2113/streams/newstream/incoming/c7248fc1-3db4-42c1-96aa-a071c92649d1\" -H \"Content-Type: application/json\" -H \"ES-EventType: SomeEvent\" HTTP/1.1 201 Created Access-Control-Allow-Methods: GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream/0 Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:14:28 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 If you retry the post: Request Response curl -i -d @event.txt \"http://127.0.0.1:2113/streams/newstream/incoming/c7248fc1-3db4-42c1-96aa-a071c92649d1\" -H \"Content-Type: application/json\" -H \"ES-EventType: SomeEvent\" HTTP/1.1 201 Created Access-Control-Allow-Methods: GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/newstream/0 Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 21 Apr 2014 21:15:33 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100"
  },
  "dotnet-api/projections/index.html": {
    "href": "dotnet-api/projections/index.html",
    "title": "Projections Management | Event Store",
    "keywords": "Projections Management The Event Store Client API includes helper methods that use the HTTP API to allow you to manage projections. This document describes the methods found in the ProjectionsManager class. All methods in this class are asynchronous. Methods Enable a Projection Enables an existing projection by name. You must have access to a projection to enable it. public Task EnableAsync(string name, UserCredentials userCredentials = null) Disable a Projection Disables an existing projection by name. You must have access to a projection to disable it. public Task DisableAsync(string name, UserCredentials userCredentials = null) Abort a Projection Aborts an existing projection by name. You must have access to a projection to abort it. public Task AbortAsync(string name, UserCredentials userCredentials = null) Create a One-Time Projection Creates a projection that runs until the end of the log and then stops. The query parameter contains the JavaScript you want created as a one time projection. public Task CreateOneTimeAsync(string query, UserCredentials userCredentials = null) Create a Continuous Projection Creates a projection that runs until the end of the log and then continues running. The query parameter contains the JavaScript you want created as a one time projection. Continuous projections have explicit names and you can enable or disable them via this name. public Task CreateContinuousAsync(string name, string query, UserCredentials userCredentials = null) List all Projections Returns a list of all projections. public Task<List<ProjectionDetails>> ListAllAsync(UserCredentials userCredentials = null) List One-Time Projections Returns a list of all One-Time Projections. public Task<List<ProjectionDetails>> ListOneTimeAsync(UserCredentials userCredentials = null) Get Statistics on a Projection Returns the statistics associated with a named projection. public Task<string> GetStatisticsAsync(string name, UserCredentials userCredentials = null) Delete Projection Deletes a named projection. You must have access to a projection to delete it. public Task DeleteAsync(string name, UserCredentials userCredentials = null) Get State Retrieves the state of a projection. public Task<string> GetState(string name, UserCredentials userCredentials = null) Get Partition State Retrieves the state of the projection via the given partition. public Task<string> GetPartitionStateAsync(string name, string partition, UserCredentials userCredentials = null) Get Result Retrieves the result of the projection. public Task<string> GetResult(string name, UserCredentials userCredentials = null) Get Partition Result Retrieves the result of the projection via the given partition. public Task<string> GetPartitionResultAsync(string name, string partition, UserCredentials userCredentials = null)"
  },
  "server/users-and-access-control-lists/index.html": {
    "href": "server/users-and-access-control-lists/index.html",
    "title": "Users and Access Control Lists | Event Store",
    "keywords": "Users and Access Control Lists Default Users Event Store provides two default users, $ops and $admin . $admin has full access to everything in Event Store. It can read and write to protected streams, which is any stream that starts with $, such as $projections-master . Protected streams are usually system streams, for example $projections-master manages some of the projections' states. The $admin user can also run operational commands, such as scavenges and shutdowns on Event Store. $ops has the ability to run operational commands like the $admin user, but cannot read protected streams. New Users New users created in Event Store are standard users. They can't read protected streams or perform operations. If you add a user into the $ops or $admins group, they will have the same level of access as those users. By default, any user can read any non-protected stream unless their is an ACL preventing that. Stream ACLs Event Store keeps the ACL of a stream in the streams metadata as JSON with the below definition. { \"$acl\" : { \"$w\" : \"$admins\", \"$r\" : \"$all\", \"$d\" : \"$admins\", \"$mw\" : \"$admins\", \"$mr\" : \"$admins\" } } These fields represent the following: $w The permission to write to this stream. $r The permission to read from this stream. $d The permission to delete this stream. $mw The permission to write the metadata associated with this stream. $mr The permission to read the metadata associated with this stream. You can update these fields with either a single string or an array of strings representing users or groups ( $admins , $all , or custom groups). It is possible to put an empty array into one of these fields, this has the effect of removing all users from that permission. Note It is not recommended to give people access to $mw as then they can then change the ACL. Example The ACL below would give greg read and write permission on the stream, while john would have read permission on the stream. Only users in the $admins group would be able to delete the stream, or read and write the metadata. { \"$acl\" : { \"$w\" : \"greg\", \"$r\" : [\"greg\", \"john\"], \"$d\" : \"$admins\", \"$mw\" : \"$admins\", \"$mr\" : \"$admins\" } } Default ACL There is a special ACL in the $settings that is used as the default ACL. This stream controls the default ACL for streams without an ACL and also controls who can create streams in the system, the default state of these is shown below. { \"$userStreamAcl\" : { \"$r\" : \"$all\", \"$w\" : \"$all\", \"$d\" : \"$all\", \"$mr\" : \"$all\", \"$mw\" : \"$all\" }, \"$systemStreamAcl\" : { \"$r\" : \"$admins\", \"$w\" : \"$admins\", \"$d\" : \"$admins\", \"$mr\" : \"$admins\", \"$mw\" : \"$admins\" } } The $userStreamAcl controls the default ACL for user streams, while the $systemStreamAcl is used as the default for all system streams. Note $w in the $userStreamAcl also applies to the ability to create a stream. Members of $admins always have access to everything, this permission cannot be removed. When a permission is set on a stream in your system it will override the default, however it is not necessary to specify all permissions on a stream. It is only necessary to specify those which differ from the default. Warning All these examples assume you have a user named ouro that has been created on your system. The examples also assume the password is ouroboros . Example { \"$userStreamAcl\" : { \"$r\" : \"$all\", \"$w\" : \"ouro\", \"$d\" : \"ouro\", \"$mr\" : \"ouro\", \"$mw\" : \"ouro\" }, \"$systemStreamAcl\" : { \"$r\" : \"$admins\", \"$w\" : \"$admins\", \"$d\" : \"$admins\", \"$mr\" : \"$admins\", \"$mw\" : \"$admins\" } } This default ACL would give ouro and $admins create and write permissions on all streams, while everyone else can read from them. To do this you could use either the HTTP API or a client API to write the above data to the stream (requires admin privileges by default for obvious reasons. Be careful allowing default access to system streams to non-admins as they would also have access to $settings unless you specifically overrode it). { \"$userStreamAcl\" : { \"$r\" : \"$all\", \"$w\" : \"ouro\", \"$d\" : \"ouro\", \"$mr\" : \"ouro\", \"$mw\" : \"ouro\" }, \"$systemStreamAcl\" : { \"$r\" : \"$admins\", \"$w\" : \"$admins\", \"$d\" : \"$admins\", \"$mr\" : \"$admins\", \"$mw\" : \"$admins\" } } Request Response curl -i -d@ ~/settings.js \"http://127.0.0.1:2113/streams/%24settings\" -H \"Content-Type:application/json\" -H \"ES-EventType: settings\" -H \"ES-EventId: C322E299-CB73-4B47-97C5-5054F920746E\" -u \"admin:changeit\" Warning You should not copy/paste the UUID in the command line above but generate a new one or not provide one (you will be redirected to a URI with one as described in writing events in the HTTP API). HTTP/1.1 201 Created Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Location: http://127.0.0.1:2113/streams/%24settings/0 Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 02 Mar 2015 14:56:13 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 If you try to access the $settings stream as an unauthorized user it will 401. Request Response curl -i http://127.0.0.1:2113/streams/%24settings -u ouro:ouroboros HTTP/1.1 401 Unauthorized Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position WWW-Authenticate: Basic realm=\"ES\" Content-Type: text/plain; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 02 Mar 2015 15:21:27 GMT Content-Length: 0 Keep-Alive: timeout=15,max=100 If you wanted to give ouro access by default to system streams I would post: { \"$userStreamAcl\" : { \"$r\" : \"$all\", \"$w\" : \"ouro\", \"$d\" : \"ouro\", \"$mr\" : \"ouro\", \"$mw\" : \"ouro\" }, \"$systemStreamAcl\" : { \"$r\" : [\"$admins\",\"ouro\"], \"$w\" : \"$admins\", \"$d\" : \"$admins\", \"$mr\" : \"$admins\", \"$mw\" : \"$admins\" } } At which point ouro can read system streams by default: Request Response curl -i http://127.0.0.1:2113/streams/%24settings -u ouro:ouroboros HTTP/1.1 200 OK Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization, ES-LongPoll, ES-ExpectedVersion, ES-EventId, ES-EventType, ES-RequiresMaster, ES-HardDelete, ES-ResolveLinkTo, ES-ExpectedVersion Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location, ES-Position Cache-Control: max-age=0, no-cache, must-revalidate Vary: Accept ETag: \"1;-1296467268\" Content-Type: application/atom+xml; charset=utf-8 Server: Mono-HTTPAPI/1.0 Date: Mon, 02 Mar 2015 15:25:17 GMT Content-Length: 1286 Keep-Alive: timeout=15,max=100 You can also then limit ACLs on particular streams which are merged with the default ACLs. { \"$acl\" : { \"$r\" : [\"greg\", \"john\"], } } If you added the above to a stream's ACL, then it would override the read permission on that stream to allow greg and john to read streams, but not ouro , resulting in the effective ACL below. { \"$acl\" : { \"$r\" : [\"greg\", \"john\"], \"$w\" : \"ouro\", \"$d\" : \"ouro\", \"$mr\" : \"ouro\", \"$mw\" : \"ouro\" } } Warning Caching will be allowed on a stream if you have enabled it to be visible to $all . This is done as a performance optimization to avoid having to set cache=private on all data. If people are bookmarking your URIs and they have been cached by an intermediary then they may still be accessible after you change the permissions from $all . While clients should not be bookmarking URIs in this way it is an important consideration."
  },
  "server/setting-up-varnish-in-linux/index.html": {
    "href": "server/setting-up-varnish-in-linux/index.html",
    "title": "Setting up Varnish in Linux | Event Store",
    "keywords": "Setting up Varnish in Linux This document provides a brief guide on how to install Event Store with the varnish reverse proxy in a Linux environment. For more information on how to properly configure varnish for your requirements, read the Varnish documentation . You use a reverse proxy to limit access to Event Store without breaking HTTP caching (authenticate to the proxy not to Event Store itself). Since Event Store runs HTTP only on the loopback adapter, users must enter through the reverse proxy to reach Event Store. Ben Clark’s Gist contains a more configured varnish configuration that includes basic authentication as well as some other niceties such as adding headers for hits/misses). The first thing that we will need to do is to install varnish sudo curl http://repo.varnish-cache.org/debian/GPG-key.txt | sudo apt-key add - echo \"deb http://repo.varnish-cache.org/ubuntu/ precise varnish-3.0\" | sudo tee -a /etc/apt/sources.list sudo apt-get update sudo apt-get install varnish Next configure varnish. sudo vi /etc/default/varnish Edit the section that looks like: DAEMON_OPTS=\"-a :80 \\ -T localhost:6082 \\ -f /etc/varnish/default.vcl \\ -S /etc/varnish/secret \\ -s malloc,256m\" Replace the port with the port you want to run on: sudo vi /etc/varnish/default.vcl Set it to: backend default { .host = \"127.0.0.1\"; .port = \"2114\"; } Finally use sudo service varnish restart to restart varnish and Event Store should berunning with a reverse proxy. If you want to check out the status of varnish you can check with varnishstat from the command line."
  },
  "server/scavenging/index.html": {
    "href": "server/scavenging/index.html",
    "title": "Scavenging events | Event Store",
    "keywords": "Scavenging events When you delete events or streams in Event Store, they aren't removed immediately. To permenantly delete these events you will need to run a 'scavenge' on your database. A scavenge reclaims disk space by rewriting your database chunks, minus the events to delete, and then deleting the old chunks. Once a scavenge has run, you cannot recover any deleted events. Note Scavenges only affect completed chunks, so deleted events in the current chunk will still be there after you run a scavenge. Starting a scavenge Scavenges are not run automatically by Event Store. Our recommendation is that you set up a scheduled task, for example using cron or Windows Scheduler, to trigger a scavenge as often as you need. You can start a scavenge by issuing an empty POST to the HTTP API with the credentials of an admin or ops user : curl -i -d {} -X POST http://localhost:2113/admin/scavenge -u \"admin:changeit\" You can also start scavenges from the Admin page in the Admin UI. Tip Each node in a cluster has its own independent database. As such, when you run a scavenge, you will need to issue a scavenge request to each node. How often should you run a scavenge This depends on the following: How often you delete streams. Depending on how you set $maxAge , $maxCount or $tb metadata on your streams. Scavenging while Event Store is online It is safe to run a scavenge while Event Store is running and processing events, it is designed to be an online operation. Keep in mind that scavenging will increase the number of reads/writes made to your disk, and it is not recommended to run it when your system is under heavy load."
  },
  "server/cluster-with-manager-nodes/index.html": {
    "href": "server/cluster-with-manager-nodes/index.html",
    "title": "Setting up a Cluster with Manager Nodes | Event Store",
    "keywords": "Setting up a Cluster with Manager Nodes High availability Event Store allows you to run more than one node as a cluster. There are two modes available for clustering: With database nodes only (open source and commercial) With manager nodes and database nodes (commercial only) This document covers setting up Event Store with manager nodes and database nodes. Manager Nodes Each physical (or virtual) machine in an Event Store cluster typically runs one manager node and one database node. It is also possible to have multiple database nodes per physical machine, running under one manager node. Manager nodes have a number of responsibilities: They are responsible for starting the database nodes and supervising them to ensure they are restarted in case of a crash or termination owing to abnormal circumstances. This is known as the watchdog service. They communicate with other manager nodes to determine cluster state, and relay that information to the database nodes under management. They provide a known endpoint for clients to connect to, to discover cluster information. When running on Windows, manager nodes run as Windows services. Configuring Nodes Each database or manager node can have a variety of configuration sources. Each source has a priority and determines running configuration by evaluating each source and applying the option from the source with the highest priority. From lowest to highest priority, the sources of configuration are: Default settings. Settings specified in a configuration file written using YAML. Settings specified in environment variables. Settings specified as command line options to the node. You can determine the configuration of a node by passing the -WhatIf flag to the process. Typical Deployment Topologies Event Store clusters follow a \"shared nothing\" philosophy, meaning that clustering requires no shared disks for clustering to work. Instead, several database nodes store your data to ensure it isn't lost in case of a drive failure or a node crashing. Event Store uses a quorum-based replication model, in which a majority of nodes in the cluster must acknowledge that a write has been committed to disk before acknowledging the write to the client. This means that to be able to tolerate the failure of n nodes, the cluster must be of size (2n + 1) . A three-database-node cluster can continue to accept writes in the case that one node is unavailable, a five-database-node cluster can continue to accept writes in the case that two nodes are unavailable, and so forth. A typical deployment topology consists of three physical machines, each running one manager node and one database node, as shown in figure 1. Each of the physical machines may have two network interfaces, one for communicating with other cluster members, and one for serving clients. Although it may be preferable in some situations to run over two separate networks, it is also possible to use different TCP ports on one interface. <!-- TODO: There is no figure 1? --> Cluster gossip Event Store uses a quorum-based replication model. When working normally, a cluster will have one database node which is known as a master , and the remaining nodes will be slaves . The master node is responsible for coordinating writes while it is the master. Database nodes use a consensus algorithm to determine which database node should be master and which should be slaves. Event Store bases the decision as to which node should be the master on a number of factors (some of which are configurable). For database nodes to have this information available to them, the nodes gossip with other nodes in the cluster. Gossip runs over the internal (and optionally the external) HTTP interfaces of database nodes, and over both internal and external interfaces of manager nodes. Discovering cluster members Manager and database nodes need to know about one another to gossip. To start this process, you provide gossip seeds or the addresses where it can find other nodes, to each node. When running with manager nodes, it normally uses the following approach: On each physical machine, configure the database node(s) with a gossip seed of the internal HTTP interface of the manager running on the same physical machine. Configure the managers to discover other managers in one of two ways: via a DNS entry and a well-known gossip port. via a list of other managers’ addresses. The preferred method is via a DNS entry. To set this up, create a DNS entry for the cluster with an A record pointing to each member of the cluster. Each manager will then look up other nodes in the cluster during the startup process based on the DNS name. Since DNS only provides information about addresses, you need to use a consistent TCP port across the cluster for gossip. Example 1 - A Three-Machine Cluster This example shows the configuration for a three node cluster, running in the typical setup of one manager node and one database node per physical machine, with cluster discovery via DNS. Each machine has one network interface, therefore uses different ports for the internal and external traffic. All nodes, in this case, are running Windows, so the manager nodes will run as Windows services. Figure 2 below shows the setup. The important points for writing configuration files are: <!-- TODO: Again, no figure? --> Node IP Addresses: 192.168.1.11, 192.168.1.12 and 192.168.13 TCP ports: (defaults): Manager Nodes: Internal HTTP: 30777 External HTTP: 30778 Database Nodes: Internal TCP: 1112 External TCP: 1113 Internal HTTP: 2112 External HTTP: 2113 DNS Entry Name: cluster1.eventstore.local To configure the cluster correctly, there are a number of steps to follow: Set up a DNS entry named cluster1.eventstore.local with an A record for each node. Write the database node configuration file for each machine. Write the manager node configuration file for each machine. Write the watchdog configuration file for each machine. Deploy Event Store and the configuration files to each machine. ( Windows-specific ) Add HTTP URL ACL entries to allow starting HTTP servers on the required HTTP ports. ( Windows-specific ) Install the manager as a service and start the service. ( Linux-specific ) Configure the manager as a daemon. DNS entry HIt depends on which DNS server is in use, but the eventual lookup should read: nslookup cluster1.eventstore.local Server: 192.168.1.2 Address: 192.168.1.2#53 Name: cluster.eventstore.local Address: 192.168.1.11 Name: cluster.eventstore.local Address: 192.168.1.12 Name: cluster.eventstore.local Address: 192.168.1.13 Database Node Configuration All three nodes will be similar in configuration. The important configuration points are IP Addresses for internal and external interfaces, the ports for each endpoint, the location of the database file, the size of the cluster and the endpoints from which to seed gossip (in this case the local manager). We assume in this case that Event Store stores data on the _D:_ drive. The configuration files are written in YAML, and reads as follows for the first node: Filename : database.yaml Db: d:\\es-data IntIp: 192.168.1.11 ExtIp: 192.168.1.11 IntTcpPort: 1112 IntHttpPort: 2112 ExtTcpPort: 1113 ExtHttpPort: 2113 DiscoverViaDns: false GossipSeed: ['192.168.1.11:30777'] ClusterSize: 3 For each subsequent node, the IP Addresses change, as does the gossip seed (since it is the manager running on the same physical machine as each node). Manager Configuration Again, all three nodes will be similar in configuration. The important configuration points are the IP addresses for the internal and external interfaces, the ports for the HTTP endpoints, the log location, and the DNS information about other nodes. Another important piece of configuration is which database nodes the manager is responsible for starting. This is defined in a separate file (the watchdog configuration), the path to which is specified as WatchdogConfig in the manager configuration. The configuration files are written in YAML, and for the first node reads as follows: Filename: manager.yaml IntIp: 192.168.1.11 ExtIp: 192.168.1.11 IntHttpPort: 30777 ExtHttpPort: 30778 DiscoverViaDns: true ClusterDns: cluster1.eventstore.local ClusterGossipPort: 30777 EnableWatchdog: true WatchdogConfig: c:\\EventStore-Config\\watchdog.esconfig Log: d:\\manager-log Watchdog Configuration The watchdog configuration file details which database nodes the manager is responsible for starting and supervising. Unlike the other configuration files, the manager configuration uses a custom format instead of YAML. Each node for which the manager is responsible has one line in the file, which starts with a # symbol and then details the command line options given to the database node when it is started. Under normal circumstances, this will be the path to the database node’s configuration file. For the first node in the example cluster, the watchdog configuration file reads as follows: # --config c:\\EventStore-Config\\database.yaml Deploying Event Store software and configuration Having written configuration files for each node, you can now deploy Event Store and configuration. Although it is possible to use relative paths when writing configuration files, it is preferable to use absolute paths to reduce the potential for confusion. In this case, Event Store is deployed on each node in _c:\\EventStore-HA-v3.0.1_, and the configuration files for that node are deployed into _C:\\EventStore-Config_. No installation process is necessary, the packaged distribution can be unzipped, assuming they have been unblocked following download. Adding HTTP ACL entries for HTTP servers (Windows-Specific) To allow for non-elevated users to run HTTP servers on Windows, you must add entries to the access control list using netsh . By default, the manager node runs as NT AUTHORITY\\Local Service , so this is the user who must have permission to run the HTTP server. The commands used to add these entries on node one are as follows (Run as an elevated user): # Database Node Internal HTTP Interface netsh http add urlacl url=http://192.168.1.11:2112/ user=\"NT AUTHORITY\\LOCAL SERVICE\" # Database Node External HTTP Interface netsh http add urlacl url=http://192.168.1.11:2113/ user=\"NT AUTHORITY\\LOCAL SERVICE\" # Manager Node Internal HTTP Interface netsh http add urlacl url=http://192.168.1.11:30777/ user=\"NT AUTHORITY\\LOCAL SERVICE\" # Manager Node External HTTP Interface netsh http add urlacl url=http://192.168.1.11:30778/ user=\"NT AUTHORITY\\LOCAL SERVICE\" Configure the Manager Node as a service (Windows-Specific) You can install manager nodes as a Windows service, so they can start on boot rather than running in interactive mode. Each manager service is given an instance name, which becomes the name of the service (and part of the description for easy identification). The service is installed by default with a startup type of \"Automatic (Delayed Start)\". Installing the service To install the manager node on machine 1, use the following command is: C:\\EventStore-HA-v3.0.1\\> EventStore.WindowsManager.exe install -InstanceName es-cluster1 -ManagerConfig C:\\EventStore-Config\\manager.yaml The service will then be visible in the services list, with a description of \"Event Store Manager (es-cluster1)\". Uninstalling the service To uninstall the manager node service in future, use the following command (where the instance name matches the name used during installation). C:\\EventStore-HA-v3.0.1\\> EventStore.WindowsManager.exe uninstall -InstanceName es-cluster1 Manually starting and stopping the service To start the manager node use the net start es-cluster1 command. To stop the manager node use the net stop es-cluster1 command."
  },
  "server/architecture-setup/index.html": {
    "href": "server/architecture-setup/index.html",
    "title": "HTTP Architecture Setup | Event Store",
    "keywords": "HTTP Architecture Setup <!-- TODO: Is the title descrptive? --> Authentication Options There are two main options for authentication with Event Store. You secure Event Store itself, or you can use per-stream Access Control Lists to give more fine-grained control on which users can access which data. You can also take a hybrid approach that mixes the two. Secure Event Store To secure Event Store, you bind the server to the localhost (127.0.0.1) interface and then install a reverse proxy such as nginx or Varnish on the public IP. You can find an example of setting up Event Store with Varnish here . The reverse proxy will be your public interface. Internally it will handle the authentication and route requests to Event Store. Event Store is only accessible through the localhost adapter and is not exposed publicly. The locally running reverse proxy will be allowed to cache responses, and because of this, reverse proxies will be more performant than calling Event Store directly. Secure Streams with ACLs Event Store supports internal authentication, you can expose Event Store directly on a port, and Event Store handles all authentication. As Event Store is handling all security requests it will have all information about users. Event Store uses this information to check the Access Control Lists of streams and allows for fine-grained control of security. This will cause more internal requests served by Event Store and thus will be less performant. Note Per-stream access control lists require setting caching to private to ensure data is not cached in a shared cache, read this article for more information Hybrid Option Even if you use a reverse proxy as above, you can support external authentication from Event Store itself. You do this by enabling the trusted intermediary option in your configuration. This allows the intermediary to write a header with the user information that Event Store will use. You can find how to do this in the HTTP headers section . Security with SSL Windows Setting up SSL in Windows is the same as setting up any httplistener in Windows for SSL. You can find many examples of this can online, and we recommend this guide from Damir Dobric Linux Setting up SSL in Linux is the same as setting up any mono httplistener in Linux for SSL. You can find many examples of this can online, and we recommend this guide from Joshua Perina . This method will likely work for other systems such as OpenBSD as well."
  },
  "http-api/optional-http-headers/trusted-intermediary/index.html": {
    "href": "http-api/optional-http-headers/trusted-intermediary/index.html",
    "title": "Optional HTTP Headers: Trusted Intermediary | Event Store",
    "keywords": "Optional HTTP Headers: Trusted Intermediary The trusted intermediary header relates to supporting a common security architecture. There are thousands of possible methods for handling authentication and it is impossible for us to support them all. The general idea behind the header is that you can configure a trusted intermediary to handle the authentication instead of Event Store. A sample configuration of this might be to enable OAuth2. First configure Event Store to run on the local loopback. Next configure nginx to handle OAuth2 authentication. After authenticating the user, nginx would rewrite the request and forward it to the loopback to the Event Store that would actually serve the request. ES-TrustedAuth: \"root; admin, other\" The header has the form of \"{user}; group, group1\". This information will then be used with the internal Event Store ACLs to handle security. It is important to note that this feature is DISABLED by default. You must specifically opt into this feature by running the Event Store with the Enable Trusted Intermediary command/config option."
  },
  "http-api/optional-http-headers/harddelete/index.html": {
    "href": "http-api/optional-http-headers/harddelete/index.html",
    "title": "Optional HTTP Headers: HardDelete | Event Store",
    "keywords": "Optional HTTP Headers: HardDelete The ES-HardDelete header controls deleting a stream. By default Event Store will soft delete a stream allowing you to later reuse that stream. If you set the ES-HardDelete header the stream will be permanently deleted. Request Response curl -v -X DELETE http://127.0.0.1:2113/streams/foo -H \"ES-HardDelete:true\" HTTP/1.1 204 Stream deleted Content-Length: 0 Content-Type: text/plain; charset=utf-8 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 20:56:55 GMT This changes the general behavior from returning a '404' and recreatable to having the stream now return '410 GONE' forever. Request Response curl -i http://127.0.0.1:2113/streams/foo2 HTTP/1.1 410 Deleted Content-Length: 0 Content-Type: text/plain; charset=utf-8 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 20:57:01 GMT"
  },
  "http-api/deleting-a-stream/index.html": {
    "href": "http-api/deleting-a-stream/index.html",
    "title": "Deleting a Stream | Event Store",
    "keywords": "uid: eventstore.org/Event Store HTTP API/4.0.2/deleteStream Deleting a Stream To delete a stream over the Atom interface, issue a DELETE request to the resource. Note The documentation here applies to versions after 2.0.1. Prior to 2.0.1 only hard deletes were available and the system uses that behavior. By default when you delete a stream it is soft deleted. This means you can recreate it later if you want to by setting the $tb metadata section as the client API does <!-- TODO: Link? --> . If you try to GET a soft deleted stream you will receive a 404 response: Request Response curl -i http://127.0.0.1:2113/streams/foo HTTP/1.1 404 Not Found Content-Length: 0 Content-Type: text/plain; charset=utf-8 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 20:47:18 GMT If desired, you can recreate the stream by appending new events to it (like creating a new stream): Request Response curl -i -d @event.txt http://127.0.0.1:2113/streams/foo -H \"Content-Type:application/json\" HTTP/1.1 201 Created Content-Length: 0 Content-Type: text/plain; charset=utf-8 Location: http://127.0.0.1:2113/streams/foo/1 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 20:49:30 GMT If you GET a stream that has been soft deleted and then recreated you will notice that the version numbers do not start at zero but at where you soft deleted the stream from: Request Response curl -i http://127.0.0.1:2113/streams/foo HTTP/1.1 200 OK Cache-Control: max-age=0, no-cache, must-revalidate Content-Length: 1215 Content-Type: application/vnd.eventstore.atom+json; charset=utf-8 ETag: \"1;-2060438500\" Vary: Accept Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 20:49:34 GMT { \"title\": \"Event stream 'foo'\", \"id\": \"<http://127.0.0.1:2113/streams/foo\">, \"updated\": \"2014-03-13T20:49:30.3821623Z\", \"streamId\": \"foo\", \"author\": { \"name\": \"EventStore\" }, \"headOfStream\": true, \"selfUrl\": \"<http://127.0.0.1:2113/streams/foo\">, \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/foo\", \"relation\": \"self\" }, { \"uri\": \"http://127.0.0.1:2113/streams/foo/head/backward/20\", \"relation\": \"first\" }, { \"uri\": \"http://127.0.0.1:2113/streams/foo/2/forward/20\", \"relation\": \"previous\" }, { \"uri\": \"http://127.0.0.1:2113/streams/foo/metadata\", \"relation\": \"metadata\" } ], \"entries\": \\[ { \"title\": \"1@foo\", \"id\": \"<http://127.0.0.1:2113/streams/foo/1\">, \"updated\": \"2014-03-13T20:49:30.3821623Z\", \"author\": { \"name\": \"EventStore\" }, \"summary\": \"chatMessage\", \"links\": [ { \"uri\": \"http://127.0.0.1:2113/streams/foo/1\", \"relation\": \"edit\" }, { \"uri\": \"http://127.0.0.1:2113/streams/foo/1\", \"relation\": \"alternate\" } ] } ] } So far we have looked at soft deletes. You can also execute hard deletes on a stream. To issue a permanent delete use the ES-HardDelete header. Warning A hard delete is permanent and the stream is not removed during a scavenge. If you hard delete a stream, the stream can never be recreated. Create a stream with the following request: Request Response curl -i -d @event.txt http://127.0.0.1:2113/streams/foo2 -H \"Content-Type:application/json\" HTTP/1.1 201 Created Content-Length: 0 Content-Type: text/plain; charset=utf-8 Location: http://127.0.0.1:2113/streams/foo2/1 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 20:54:24 GMT Then issue the DELETE as before but with the permanent delete header: Request Response curl -v -X DELETE http://127.0.0.1:2113/streams/foo2 -H \"ES-HardDelete:true\" HTTP/1.1 204 Stream deleted Content-Length: 0 Content-Type: text/plain; charset=utf-8 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 20:56:55 GMT This stream is now permanently deleted, and unlike before where you received a '404' response, the response will now be a '410 GONE'. Request Response curl -i http://127.0.0.1:2113/streams/foo2 HTTP/1.1 410 Deleted Content-Length: 0 Content-Type: text/plain; charset=utf-8 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 20:57:01 GMT If you try to recreate the stream as in the above example you will also receive a '410 GONE' response. Request Response curl -i -d @event.txt http://127.0.0.1:2113/streams/foo2 -H \"Content-Type:application/json\" HTTP/1.1 410 Stream deleted Content-Length: 0 Content-Type: text/plain; charset=utf-8 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 21:00:00 GMT The same applies if you try to delete an already deleted stream. You will receive a '410 GONE' response. Request Response curl -i -X DELETE http://127.0.0.1:2113/streams/foo2 -H \"ES-HardDelete: true\" HTTP/1.1 410 Stream deleted Content-Length: 0 Content-Type: text/plain; charset=utf-8 Server: Microsoft-HTTPAPI/2.0 Access-Control-Allow-Methods: POST, DELETE, GET, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Requested-With, X-PINGOTHER, Authorization Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Location Date: Thu, 13 Mar 2014 21:19:33 GMT"
  },
  "event-sourcing-basics/rolling-snapshots/index.html": {
    "href": "event-sourcing-basics/rolling-snapshots/index.html",
    "title": "Rolling Snapshots | Event Store",
    "keywords": "Rolling Snapshots A rolling snapshot is a projection of the current state of an aggregate at a given point. It represents the state when all events to that point have been replayed. You use rolling snapshots as a heuristic to prevent the need to load all events for the entire history of an aggregate. The figure below shows a typical Event Stream. One way of processing events in the event stream is to replay the events from the beginning of time until the end of the event stream. <!-- ![An event stream][4] --> The problem is that there may be a large number of events between the beginning of time and the current point. You can imagine that an event stream with a million or more events would be inefficient to load. The solution is to use a rolling snapshot to place a denormalization of the state at a given point. It is then be possible to play the events from that point forward to load the aggregate. <!-- ![An event stream with embedded snapshot][5] --> The figure above shows an event stream with a rolling snapshot placed within it. The process for rebuilding an aggregate changes when using rolling snapshots. Instead of reading forward from the beginning of time it is read backwards, putting the events on to a stack until there are no more events left or a snapshot is found. The snapshot is then applied and the events would popped off the stack and applied until the stack was empty. Note Although this is an easy way to conceptualize how rolling snapshots work, this is a less than ideal solution in a production system for various reasons. It is better to store them out of band with events. <!-- TODO: Why? Revisit --> The snapshot itself is nothing more than a serialized form of the graph at that given point. By having the state of that graph at that point replaying all the events prior to that snapshot can be avoided. Snapshots can be taken asynchronously by a process monitoring the Event Store. Introducing Rolling Snapshots allows control of the worst case when loading from events. The maximum number of events that would be processed can be tuned to optimize performance for the system in question. With the introduction of Rolling Snapshots it is a relatively trivial process to achieve one to two orders of magnitude of performance gain on the two operations that the Event Storage supports. It is important though to remember that Rolling Snapshots are just a heuristic and that conceptually the event stream is still viewed in its entirety."
  },
  "event-sourcing-basics/business-value-of-the-event-log/index.html": {
    "href": "event-sourcing-basics/business-value-of-the-event-log/index.html",
    "title": "Business Value of the Event Log | Event Store",
    "keywords": "Business Value of the Event Log Note The value of an event log is directly correlated with use cases that you would want to use Domain Driven Design in the first place. You should use Domain Driven Design in cases where the business derives competitive advantage. Domain Driven Design itself is difficult and expensive to apply, but a company will receive high ROI on the effort if the domain is complex and if they derive competitive advantage from it. Using an Event Log similarly will have high ROI when dealing with an area of competitive advantage but may have negative ROI in other places. Storing only current state limits organizations to asking certain kinds of questions about the data. For example consider orders in the stock market. They can change for a few reasons. An order can change the volume to buy/sell, the trading system can automatically adjust the volume of an order, or a trade could occur lowering the volume available on the current order. If posed with a question regarding current liquidity, such as the price for a given number of shares in the market, it does not matter which of these changes occurred. It does not matter how the data got the way it was. It matters what it is at a given point in time. A vast majority of queries even in the business world are focused on the what labels to send customers mails, how much was sold in April, how many widgets are in the warehouse. There are however other types of queries that are becoming more and more popular in business. They focus on the how. Examples can commonly be seen in the buzzword “Business Intelligence”. Perhaps there is a correlation between people having done an action and their likelihood of purchasing some product? These types of questions generally focus on how something came into being as opposed to what it came out to be. It is best to go through an example. There is a development team at a large online retailer. In an iteration planning meeting a domain expert comes up with an idea. He believes that there is a correlation between people having added then removed an item from their cart and their likelihood of responding to suggestions of that product by purchasing it at a later point. The feature is added to the following iteration. The first hypothetical team is utilizing a stereotypical current state based mechanism for storing state. They plan that in this iteration they will add tracking of items via a fact table that are removed from carts. They plan for the next iteration that they will then build a report. The business will receive after the second iteration a report that can show them information back to the previous iteration when the team released the functionality that began tracking items being removed from carts. This is a very stereotypical process. At some organizations the report and the tracking may be released simultaneously but this is a relatively small detail in the handling. From a business perspective the domain experts are happy. They made a request of the team and the team was able to quickly fulfill the request. New functionality has been added in a quick and relatively painless way. The second team will however have quite a different result. The second team has been storing events; they represent their current state by building up off of a series of events. They just like the first team go through and add tracking of items removed from carts via a fact table but they also run this handler from the beginning of the event log to back populate all of the data from the time that the business started. They release the report in the same iteration and the report has data that dates back for years. The second team can do this because they have managed to store what the system actually did as opposed to what the current state of data is. It is possible to go back and look and interpret the old data in new and interesting ways. It was never considered to track what items were removed from carts or perhaps the number of times a user removes and items from their cart was considered important. These are both examples of new and interesting ways of looking at data. As the events represent every action the system has undertaken any possible model describing the system can be built from the events. Businesses regularly come up with new and interesting ways of looking at data. It is not possible with any level of confidence to predict how a business will want to look at today’s data in five years. The ability for the business to look at the data in the way that it wants in five years is of an unknown but possibly extremely high value; it has already been stated that this should be done in areas where the business derives its competitive advantage so it is relatively easy to reason that the ability to look at today’s data in an unexpected way could be a competitive advantage for the business. How do you value the possible success or failure of a company based upon an architectural decision now? How do software teams justify looking at their Magic 8 Ball to predict what the business will need in five or even ten years? Many try to use YAGNI (You Ain’t Gonna Need It) but YAGNI only applies when you actually know that you won’t need it. How can the dynamic world of business and how they may want to look at data in five or ten years be predicted? Is it more expensive to actually model every behavior in the system? Yes. Is it more expensive in terms of disk cost and thought process to store every event in the system? Yes. Are these costs worth the ROI when the business derives a competitive advantage from the data?"
  },
  "dotnet-api/connecting-to-a-server/index.html": {
    "href": "dotnet-api/connecting-to-a-server/index.html",
    "title": "Connecting to a Server | Event Store",
    "keywords": "Connecting to a Server EventStoreConnection The EventStoreConnection class maintains a full-duplex connection between the client and the Event Store server. EventStoreConnection is thread-safe and we recommend that you create one instance per application. All operations are fully asynchronous and return either a Task or a Task<T> . If you need to execute synchronously, call .Wait() , or Result on the asynchronous version. To get maximum performance from the connection we recommend that you use it asynchronously. Note The Create methods have changed since version 3.0.2 as connection strings are now supported. The old mechanisms will still work but are marked obsolete and will be removed in the future. Creating a Connection The EventStoreConnection classes uses the static Create methods to create a new connection. All method overloads allow you to optionally specify a name for the connection, which the connection returns when it raises events (see Connection Events ). Method Description Create(Uri uri) Connects to Event Store (see URIs below) with default settings Create(ConnectionSettings connectionSettings, Uri uri) Connects to Event Store (see URIs below) with specified settings Create(string connectionString) Connects to Event Store (see URIs below) with settings from connection string (obsolete) Create(IPEndPoint tcpEndPoint) Connects to a single node with default settings (obsolete) Create(ConnectionSettings settings, IPEndPoint tcpEndPoint) Connects to a single node with custom settings (see Customising Connection Settings ) (obsolete) Create(ConnectionSettings connectionSettings, ClusterSettings clusterSettings) Connects to an Event Store HA cluster with custom settings (see Cluster Settings ) Note The connection returned by these methods is inactive. Use the ConnectAsync() method to establish a connection with the server. URIs The create methods support passing of a URI to the connection as opposed to passing IPEndPoints . This URI should be in the format of: Single Node : tcp://user:password@myserver:11234 Cluster : discover://user:password@myserver:1234 Where the port number points to the TCP port of the Event Store instance (1113 by default) or points to the manager gossip port for discovery purposes. With the URI based mechanism you can pass a domain name and the client will resolve it. Note The client performs a blocking DNS call for single node. If you are worried about blocking DNS due to network issues etc., you should resolve the DNS yourself and pass in an IP address. Customising Connection Settings Connection String Many of the overloads accept a connection string that you can use to control settings of the connection. A benefit to having these as a connection string instead of using the fluent API is that you can change them between environments without recompiling (i.e. a single node in dev and a cluster in production ). The connection string format should look familiar to those who have used connection strings in the past. It consists of a series of key/value pairs separated by semicolons. You can set the following values using the connection string. <!-- TODO: Moved, to check and what about ConnectTo? --> Name Format Description VerboseLogging True/false Enables verbose logging MaxQueueSize Integer Maximum number of outstanding operations MaxConcurrentItems Integer Maximum number of concurrent async operations MaxRetries Integer Maximum number of retry attempts MaxReconnections Integer The maximum number of times to try reconnecting RequireMaster True/false If set the server will only process if it is master ReconnectionDelay Integer (milliseconds) The delay before attempting to reconnect OperationTimeout Integer (milliseconds) The time before considering an operation timed out OperationTimeoutCheckPeriod Integer (milliseconds) The frequency in which to check timeouts DefaultUserCredentials String in format username:password The default credentials for the connection UseSslConnection True/false whether to use SSL for this connection TargetHost String The hostname expected on the certificate ValidateServer True/false Whether to validate the remote server FailOnNoServerResponse True/False Whether to fail on no server response HeartbeatInterval Integer (milliseconds) The interval at which to send the server a heartbeat HeartbeatTimeout Integer (milliseconds) The amount of time to receive a heartbeat response before timing out ClusterDns string The DNS name of the cluster for discovery MaxDiscoverAttempts Integer The maximum number of attempts to try to discover the cluster ExternalGossipPort Integer The port to try to gossip on GossipTimeout Integer (milliseconds) The amount of time before timing out a gossip response GossipSeeds Comma separated list of ip:port A list of seeds to try to discover from ConnectTo A URI in format described above to connect to The URI to connect to [!INFO] You can also use spacing instead of camel casing in your connection string. var connectionString = \"ConnectTo=tcp://admin:changeit@localhost:1113; HeartBeatTimeout=500\" Sets the connection string to connect to localhost on the default port and sets the heartbeat timeout to 500ms. var connectionString = \"Connect To = tcp://admin:changeit@localhost:1113; Gossip Timeout = 500\" Using spaces: var connectionString = \"ConnectTo=discover://admin:changeit@mycluster:3114; HeartBeatTimeout=500\" Tells the connection to try gossiping to a manager node found under the DNS 'mycluster' at port '3114' to connect to the cluster. var connectionString = \"GossipSeeds=192.168.0.2:1111,192.168.0.3:1111; HeartBeatTimeout=500\" Tells the connection to try gossiping to the gossip seeds 192.168.0.2 or 192.168.0.3 on port '1111' to discover information about the cluster. Note See the fluent API below for defaults of values. Note You can also use the ConnectionString class to return a ConnectionSettings object. Fluent API Settings used for modifying the behavior of an EventStoreConnection are encapsulated into an object of type ConnectionSettings passed as a parameter to the Create methods listed above. Instances of ConnectionSettings are created using a fluent builder class: ConnectionSettings settings = ConnectionSettings.Create(); This creates an instance of ConnectionSettings with default options. You can override these by chaining the additional builder methods described below. Logging The .NET API can log information to different destinations. By default logging is disabled. <!-- TODO: Moved, to check. Actually missing options. --> Builder Method Description UseConsoleLogger() Output log messages using Console.WriteLine UseDebugLogger() Output log messages using Debug.WriteLine UseCustomLogger() Output log messages to the specified instance of ILogger (You should implement this interface in order to log using another library such as NLog or log4net). EnableVerboseLogging() Turns on verbose logging. By default information about connection, disconnection and errors are logged, however it can be useful to have more information about specific operations as they are occuring. User Credentials Event Store supports Access Control Lists that restrict permissions for a stream based on users and groups. EventStoreConnection allows you to supply credentials for each operation, however it is often more convenient to set default credentials for all operations on the connection. Builder Method Description SetDefaultUserCredentials(UserCredentials credentials) Sets the default UserCredentials to use for this connection. If you don't supply any credentials, the operation will use these. You create a UserCredentials object as follows: UserCredentials credentials = new UserCredentials(\"username\",\"password\"); Security The .NET API and Event Store can communicate either over SSL or an unencrypted channel (by default). To configure the client-side of the SSL connection, use the builder method below. For more information on setting up the server end of the Event Store for SSL, see SSL Setup . UseSslConnection(string targetHost, bool validateServer) Uses an SSL-encrypted connection where targetHost is the name specified on the SSL certificate installed on the server, and validateServer controls whether the connection validates the server certificate. Warning In production systems where credentials are sent from the client to Event Store, you should always use SSL encryption and you should set validateServer to true . Node Preference When connecting to an Event Store HA cluster you can specify that operations are performed on any node, or only on the node that is the master. Builder Method Description PerformOnMasterOnly() Require the master to serve all write and read requests (Default). PerformOnAnyNode() Allow for writes to be forwarded and read requests to be served locally if the current node is not master. Handling Failures The following methods on the ConnectionSettingsBuilder allow you to change the way the connection handles operation failures and connection issues. Reconnections Builder Method Description WithConnectionTimeoutOf (TimeSpan timeout) Sets the timeout to connect to a server before aborting and attempting a reconnect (Default: 1 second). LimitReconnectionsTo (int limit) Limits the number of reconnections this connection can try to make (Default: 10). KeepReconnecting() Allows infinite reconnection attempts. SetReconnectionDelayTo (TimeSpan reconnectionDelay) Sets the delay between reconnection attempts (Default: 100ms). SetHeartbeatInterval (TimeSpan interval) Sets how often the connection should expect heartbeats (lower values detect broken sockets faster) (Default: 750ms). SetHeartbeatTimeout (TimeSpan timeout) Sets how long to wait without heartbeats before determining a connection to be dead (must be longer than the heatrbeat interval) (Default: 1500ms). Operations Builder Method Description SetOperationTimeout (TimeSpan timeout) Sets the operation timeout duration (Default: 7 seconds). SetTimeoutCheckPeriodTo (TimeSpan timeoutCheckPeriod) Sets how often to check for timeouts (Default: 1 second). LimitAttemptsForOperationTo (int limit) Limits the number of operation attempts (Default: 11). LimitRetriesForOperationTo (int limit) Limits the number of operation retries (Default: 10). KeepRetrying() Allows infinite operation retries. LimitOperationsQueueTo (int limit) Sets the limit for number of outstanding operations (Default: 5000). FailOnNoServerResponse() Marks that no response from server should cause an error on the request. Cluster Settings When connecting to an Event Store HA cluster you must pass an instance of ClusterSettings as well as the usual ConnectionSettings . Primarily yu use this to tell the EventStoreConnection how to discover all the nodes in the cluster. A connection to a cluster will automatically handle reconnecting to a new node if the current connection fails. Using DNS Discovery DNS discovery uses a single DNS entry with several records listing all node IP addresses. The EventStoreConnection will then use a well known port to gossip with the nodes. Use ClusterSettings.Create().DiscoverClusterViaDns() followed by: <!-- TODO: Moved, to check. --> Builder Method Description SetClusterDns(string clusterDns) Sets the DNS name under which to list cluster nodes. SetClusterGossipPort(int clusterGossipPort) Sets the well-known port on which the cluster gossip is taking place. SetMaxDiscoverAttempts(int maxDiscoverAttempts) Sets the maximum number of attempts for discovery (Default: 10). SetGossipTimeout(TimeSpan timeout) Sets the period after which gossip times out if none is received (Default: 1 second). Note If you are using the commercial edition of Event Store HA with Manager nodes in place, the gossip port should be the port number of the external HTTP port on which the managers are running. If you are using the open source edition of Event Store HA the gossip port should be the External HTTP port that the nodes are running on. If you cannot use a well-known port for this across all nodes you can instead use gossip seed discovery and set the IPEndPoint of some seed nodes instead. Connecting Using Gossip Seeds The second supported method for node discovery uses a hardcoded set of IPEndPoint s as gossip seeds. Use ClusterSettings.Create().DiscoverClusterViaGossipSeeds() followed by: <!-- TODO: Moved, to check. --> Builder Method Description SetGossipSeedEndPoints(params IPEndPoint[] gossipSeeds) Sets gossip seed endpoints for the client. SetGossipSeedEndPoints(params GossipSeed[] gossipSeeds) Same as above, but allows a specific Host header to be sent with all HTTP requests. SetMaxDiscoverAttempts(int maxDiscoverAttempts) Sets the maximum number of attempts for discovery (Default: 10). SetGossipTimeout(TimeSpan timeout) Sets the period after which gossip times out if none is received (Default: 1 second). Connection Events EventStoreConnection exposes events that your application can use to be notifed of changes to the status of the connection. <!-- TODO: Not moved. --> Event Description EventHandler<ClientConnectionEventArgs> Connected Fired when an IEventStoreConnection connects to an Event Store server. EventHandler<ClientConnectionEventArgs> Disconnected Fired when an IEventStoreConnection disconnects from an Event Store server by some means other than by calling the Close method. EventHandler<ClientReconnectingEventArgs> Reconnecting Fired when an IEventStoreConnection is attempting to reconnect to an Event Store server following a disconnection. EventHandler<ClientClosedEventArgs> Closed Fired when an IEventStoreConnection is closed either using the Close method or when reconnection limits are reached without a successful connection being established. EventHandler<ClientErrorEventArgs> ErrorOccurred Fired when an error is thrown on an IEventStoreConnection . EventHandler<ClientAuthenticationFailedEventArgs> AuthenticationFailed Fired when a client fails to authenticate to an Event Store server."
  },
  "server/setting_up_ssl/index.html": {
    "href": "server/setting_up_ssl/index.html",
    "title": "Setting up SSL on Windows | Event Store",
    "keywords": "Setting up SSL on Windows The steps to set up SSL on Windows are as follows. First, create a certificate using powershell, and copy the thumbprint from the output New-SelfSignedCertificate -DnsName eventstore.org, localhost -CertStoreLocation cert:\\CurrentUser\\My To trust the new certificate, the certificate you have to import the certificat into the Trusted Root Certification Authorities: <!-- TODO: Images maybe? --> Press WindowsKey + R , and enter 'certmgr.msc'. Navigate to Certificates -> Current User -> Personal -> Certificates . Locate the certificate 'eventstore.org'. Right click on the certificate and click on All Tasks -> Export . Follow the prompts. Navigate to Certificates -> Current User -> Trusted Root Certification Authorities -> Certificates . Right click on the Certificates folder menu item and click All Tasks -> Import . Follow the prompts. Start Event Store with the following configuration: <!-- TODO: Again, what does this mean? --> CertificateStoreLocation: CurrentUser CertificateStoreName: My CertificateThumbPrint: {Insert Thumb Print from Step 1} CertificateSubjectName: CN=eventstore.org ExtSecureTcpPort: 1115 Connect to Event Store using the Event Store .NET Client. var settings = ConnectionSettings.Create().UseSslConnection(\"eventstore.org\", true); using (var conn = EventStoreConnection.Create(settings, new IPEndPoint(IPAddress.Loopback, 1115))) { conn.ConnectAsync(); }"
  },
  "http-api/swagger.html": {
    "href": "http-api/swagger.html",
    "title": "HTTP API | Event Store",
    "keywords": "HTTP API"
  },
  "http-api/swagger/Streams.html": {
    "href": "http-api/swagger/Streams.html",
    "title": "Streams | Event Store",
    "keywords": "Streams Endpoints for Stream operations"
  },
  "http-api/swagger/Subscriptions.html": {
    "href": "http-api/swagger/Subscriptions.html",
    "title": "Subscriptions | Event Store",
    "keywords": "Subscriptions Endpoints for Subscription operations"
  },
  "http-api/swagger/Users.html": {
    "href": "http-api/swagger/Users.html",
    "title": "Users | Event Store",
    "keywords": "Users Endpoints for User operations"
  },
  "http-api/swagger/Projections/Enable projection.html": {
    "href": "http-api/swagger/Projections/Enable projection.html",
    "title": "Enable projection | Event Store",
    "keywords": "Enable projection Enable projection Enable the specified projection. | Improve this Doc View Source Enable projection Request POST /projection/{name}/command/enable[?enableRunAs] Parameters Name Type Value Notes *name string The name of the projection enableRunAs boolean Run as the user issuing the command. Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Reset projection.html": {
    "href": "http-api/swagger/Projections/Reset projection.html",
    "title": "Reset projection | Event Store",
    "keywords": "Reset projection Reset projection Reset the specified projection. | Improve this Doc View Source Reset projection Request POST /projection/{name}/command/reset[?enableRunAs] Parameters Name Type Value Notes *name string The name of the projection enableRunAs boolean Run as the user issuing the command. Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Abort projection.html": {
    "href": "http-api/swagger/Projections/Abort projection.html",
    "title": "Abort projection | Event Store",
    "keywords": "Abort projection Abort projection Abort the specified projection. | Improve this Doc View Source Abort projection Request POST /projection/{name}/command/abort[?enableRunAs] Parameters Name Type Value Notes *name string The name of the projection enableRunAs boolean Run as the user issuing the command. Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Get projection config.html": {
    "href": "http-api/swagger/Projections/Get projection config.html",
    "title": "Get projection config | Event Store",
    "keywords": "Get projection config Get the config of a projection Returns the performance configuration of the specified projection. | Improve this Doc View Source Get projection config Request GET /projection/{name}/config Parameters Name Type Value Notes *name string The name of the projection Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Update projection config.html": {
    "href": "http-api/swagger/Projections/Update projection config.html",
    "title": "Update projection config | Event Store",
    "keywords": "Update projection config Update the config of a projection Update the performance configuration of the specified projection. | Improve this Doc View Source Update projection config Request PUT /projection/{name}/config Parameters Name Type Value Notes *name string The name of the projection Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Get a projection.html": {
    "href": "http-api/swagger/Projections/Get a projection.html",
    "title": "Get a projection | Event Store",
    "keywords": "Get a projection Get a projection Returns a specific projection. | Improve this Doc View Source Get a projection Request GET /projection/{name} Parameters Name Type Value Notes *name string The name of the projection Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Projections/Delete a projection.html": {
    "href": "http-api/swagger/Projections/Delete a projection.html",
    "title": "Delete a projection | Event Store",
    "keywords": "Delete a projection Deletes a projection Deletes a projection | Improve this Doc View Source Delete a projection Request DELETE /projection/{name}[?deleteStateStream&deleteCheckpointStream&deleteEmittedStreams] Parameters Name Type Value Notes *name string The projection to delete deleteStateStream boolean TBD deleteCheckpointStream boolean TBD deleteEmittedStreams boolean TBD Responses Status Code Description Samples 204 Projection deleted"
  },
  "http-api/swagger/Stats/Get all stats.html": {
    "href": "http-api/swagger/Stats/Get all stats.html",
    "title": "Get all stats | Event Store",
    "keywords": "Get all stats Get all stats Returns all stats enabled for Event Store. | Improve this Doc View Source Get all stats Request GET /stats Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Stats/Get replication stats.html": {
    "href": "http-api/swagger/Stats/Get replication stats.html",
    "title": "Get replication stats | Event Store",
    "keywords": "Get replication stats Get replication stats Returns all replication stats for Event Store between the master and slave nodes. | Improve this Doc View Source Get replication stats Request GET /stats/replication Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Stats/Get TCP connection stats.html": {
    "href": "http-api/swagger/Stats/Get TCP connection stats.html",
    "title": "Get TCP connection stats | Event Store",
    "keywords": "Get TCP connection stats Get TCP connection stats Returns all TCP connection stats for Event Store. | Improve this Doc View Source Get TCP connection stats Request GET /stats/tcp Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Stats/Get specified stat.html": {
    "href": "http-api/swagger/Stats/Get specified stat.html",
    "title": "Get specified stat | Event Store",
    "keywords": "Get specified stat Get stats sub path Returns the sub path of the Event Store statistics available. | Improve this Doc View Source Get specified stat Request GET /stats/{statPath} Parameters Name Type Value Notes *statPath string The stats sub path Responses Status Code Description Samples 200 OK 404 Not found"
  },
  "http-api/swagger/Subscriptions/Get subscription information.html": {
    "href": "http-api/swagger/Subscriptions/Get subscription information.html",
    "title": "Get subscription information | Event Store",
    "keywords": "Get subscription information Reads stream information via a persistent subscription Needed | Improve this Doc View Source Get subscription information Request GET /subscriptions/{stream}/{subscription}/info Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group Responses Status Code Description Samples 200 OK"
  },
  "http-api/swagger/Subscriptions/Get a stream.html": {
    "href": "http-api/swagger/Subscriptions/Get a stream.html",
    "title": "Get a stream | Event Store",
    "keywords": "Get a stream Read a stream Read a specified stream by a persistent subscription. | Improve this Doc View Source Get a stream Request GET /subscriptions/{stream}/{subscription}[?embed] Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group embed string Needed Responses Status Code Description Samples 200 OK"
  },
  "http-api/swagger/Subscriptions/Update subscription.html": {
    "href": "http-api/swagger/Subscriptions/Update subscription.html",
    "title": "Update subscription | Event Store",
    "keywords": "Update subscription Update a persistant subscription You can edit the settings of an existing subscription while it is running. This will drop the current subscribers and will reset the subscription internally. | Improve this Doc View Source Update subscription Request POST /subscriptions/{stream}/{subscription} Parameters Name Type Value Notes *stream string The stream the persistent subscription is on *subscription string The name of the subscription group subscriptionItem Subscription to create Responses Status Code Description Samples 200 Subscription updated"
  }
}